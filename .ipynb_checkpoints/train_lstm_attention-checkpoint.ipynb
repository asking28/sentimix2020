{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEf-55pMQVDy"
   },
   "outputs": [],
   "source": [
    "#https://www.aclweb.org/anthology/C18-1247.pdf how emotional are you\n",
    "#https://arxiv.org/pdf/1905.12516.pdf multiple datasets \n",
    "#In recent years there has been exponential growth in usage of social media web sites such as Twitter, Facebook, and Reddit. It has led to an urgent need to tackle hate speech and offensive language on these platforms. Such language may induce differences between different groups, which may further cause an imbalance in society. In this paper, we propose a Deep Learning-based model using the Attention mechanism and Bi-directional LSTM(Long Short Term Memory) to classify among hate, offensive, and neutral speech. We conducted various experiments on data preprocessing and tried on different models to achieve new state-of-the-art results (as per our knowledge). We have trained and tested our models on annotated data by Davidson. Et al., 2017. We have evaluated our best model on various metrics such as F1-score, precision, recall, and accuracy. With our proposed model, we achieved an F1-score of 0.96, recall of 0.97, the precision of 0.97, and accuracy of 0.97, which is a weighted average of three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "XrRaD0F2xbxm",
    "outputId": "1e1139d9-f6c6-455f-a5d0-713e30e6806c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D, Conv2D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
    "from keras.layers import average\n",
    "import tensorflow_hub as hub\n",
    "from keras.layers import Average\n",
    "from keras.layers import Concatenate\n",
    "nltk.download('punkt')\n",
    "from numpy import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "#plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "J97kjnISyIhf",
    "outputId": "20a87a41-3d3d-4a4c-a964-8d04b65c6627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-ibFLJujKJ5"
   },
   "outputs": [],
   "source": [
    "root_path='/content/drive/My Drive/Sentimix/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXlXZTxEx4SJ"
   },
   "outputs": [],
   "source": [
    "labels_train_raw=pd.read_csv('/content/drive/My Drive/Sentimix/Train_data/labels_train.csv')\n",
    "labels_dev_raw=pd.read_csv('/content/drive/My Drive/Sentimix/Train_data/labels_dev.csv')\n",
    "# train_data=pd.read_csv('/content/drive/My Drive/Sentimix/train_clean_with_cusswords.csv')\n",
    "# dev_data=pd.read_csv('/content/drive/My Drive/Sentimix/dev_clean_with_cusswords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nkf_mi4DWHB"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('/content/drive/My Drive/Sentimix/pure_hinglish_with_hindi_cuss_train.csv')\n",
    "dev_data=pd.read_csv('/content/drive/My Drive/Sentimix/pure_hinglish_with_hindi_cuss_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "RevbZJRWytz4",
    "outputId": "0c417007-6c71-4dc4-fccf-98fe0dcaf41c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le=LabelEncoder()\n",
    "le.fit(labels_train_raw)\n",
    "labels_train_le=le.transform(labels_train_raw)\n",
    "labels_dev_le=le.transform(labels_dev_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpBF8hRBGC2g"
   },
   "outputs": [],
   "source": [
    "ohc=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "JBFt7JAjGLN6",
    "outputId": "15a0d9c4-f3c6-41cd-d2aa-da640a9c5422"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ohc=OneHotEncoder()\n",
    "labels_train=ohc.fit_transform(labels_train_le.reshape(-1,1))\n",
    "labels_dev=ohc.transform(labels_dev_le.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGxqCh7FgROR"
   },
   "outputs": [],
   "source": [
    "#emb_aaai=np.load('/content/drive/My Drive/Sentimix/Abhishek Folder/embedding_matrix_iiid.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b2Uu7q-_zB6p",
    "outputId": "5b2bd40e-ac64-4968-a9cd-2ea9731adb77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "i208B53c7RB4",
    "outputId": "2e44f5cb-11b8-4259-93e1-81d20a33e997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
      "\r",
      "\u001b[K     |███████▌                        | 10kB 23.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 20kB 4.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 30kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 40kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 3.0MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=113e007045d58cf1cb37d1a55e56b43a7ffede8a17a319bad70bef54d731f1c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.5.4\n"
     ]
    }
   ],
   "source": [
    "def remove_pattern(input_txt, pattern,with_space=False):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    if with_space==False:\n",
    "      for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    else:\n",
    "      for i in r:\n",
    "        input_txt = re.sub(i, ' ', input_txt)\n",
    "    return input_txt \n",
    "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
    "\n",
    "    return input_txt \n",
    "!pip install emoji\n",
    "import emoji\n",
    "import pickle\n",
    "import re\n",
    "with open(root_path+'helper_data/contractions.pkl','rb')as f:\n",
    "  contractions=pickle.load(f)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "contractions=Counter(contractions)\n",
    "with open(root_path+'helper_data/acronyms.pkl','rb')as f:\n",
    "  acronyms=pickle.load(f)\n",
    "acronyms=Counter(acronyms)\n",
    "def acronym(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if acronyms[word]!=0:\n",
    "        w_l.append(acronyms[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "with open(root_path+'hinglish_to_english.pickle','rb')as f:\n",
    "  hing_to_eng=pickle.load(f)\n",
    "hing_to_eng=Counter(hing_to_eng)\n",
    "def hindi_se_english(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    w_l=[]\n",
    "    sent=str(df[column][i])\n",
    "    for word in sent.split():\n",
    "      if hing_to_eng[word]!=0:\n",
    "        w_l.append(hing_to_eng[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
    "    cuss_dict=pickle.load(handle)\n",
    "cuss_dict=Counter(cuss_dict)\n",
    "cuss_dict['bsdk']='abuse'\n",
    "cuss_dict['bhosadike']='abuse'\n",
    "def replace_cuss(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if cuss_dict[word]!=0:\n",
    "        #w_l.append('abuse')\n",
    "        w_l.append(cuss_dict[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "def remove_contraction(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if contractions[word]!=0:\n",
    "        w_l.append(contractions[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "def cleaning(data_f,cleaning_col,new_col):\n",
    "  for i in range(data_f.shape[0]):\n",
    "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
    " # data_f[new_col]=replace_cuss(data_f,cleaning_col)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],\"_\",with_space=True)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@ [\\w]*\",\"<USR>\")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"[0-9]+\",\"<NUM>\")\n",
    " # data_f[new_col]=hindi_se_english(data_f,new_col)\n",
    " # data_f[new_col]=remove_contraction(data_f,new_col)\n",
    " # data_f[new_col]=acronym(data_f,new_col)\n",
    "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
    "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
    "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
    "  data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "  return data_f\n",
    "import numpy as np\n",
    "a=cleaning(train_data,'sent','hindi_clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ur504TzuhEmN",
    "outputId": "1e6254cb-7926-448a-faac-445ffa04da50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'who”'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hing_to_eng['kisake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WnA7MhAFH8f"
   },
   "outputs": [],
   "source": [
    "b=cleaning(dev_data,'sent','hindi_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQPNNhjf4Tjo"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUimeWjPzXTB"
   },
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(a['hindi_clean'].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "d01ddBp04Dj3",
    "outputId": "5823e200-8ee0-4e97-c178-4b577933dc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_metrics in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: Keras>=2.1.5 in /usr/local/lib/python3.6/dist-packages (from keras_metrics) (2.2.5)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.3.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.17.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.0.8)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.1.5->keras_metrics) (1.12.0)\n",
      "Requirement already satisfied: keras-self-attention in /usr/local/lib/python3.6/dist-packages (0.42.0)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.17.4)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (2.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.3.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (3.13)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-self-attention) (1.0.8)\n",
      "Requirement already satisfied: extra-keras-metrics in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from extra-keras-metrics) (4.4.1)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from extra-keras-metrics) (2.2.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (1.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (1.3.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->extra-keras-metrics) (1.17.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_metrics\n",
    "!pip install keras-self-attention\n",
    "!pip install extra-keras-metrics\n",
    "import keras_metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46w8CxEuo0OS"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
    "#     cuss_dict=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XavnQp7-RInZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZa1gXS_P1ra"
   },
   "outputs": [],
   "source": [
    "# add_cuss_list=['mulla','mullah','mullahs','mulle','mulleabhi','mulley','mulli','mullo','mullio','mullon','katuaraj','katue','kathue','katmullo','kathmullo','kathmulllo','katwe','katve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BFClrZMQjF7"
   },
   "outputs": [],
   "source": [
    "# for word in add_cuss_list:\n",
    "#   cuss_dict[word]='abuse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ROe6sEpQrxq"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'wb') as handle:\n",
    "#     pickle.dump(cuss_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JS5TeLmPzf1G"
   },
   "outputs": [],
   "source": [
    "sequences_train = tok.texts_to_sequences(a['hindi_clean'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ovf7C-iX1T-S"
   },
   "outputs": [],
   "source": [
    "sequences_dev = tok.texts_to_sequences(b['hindi_clean'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YtrwEi0hGdEv"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def custom_gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ns59EE9a4cm"
   },
   "source": [
    "## abuse feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEyWVVJua9RG"
   },
   "outputs": [],
   "source": [
    "abuse_f=np.zeros((train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqTkqE2WbLxo"
   },
   "outputs": [],
   "source": [
    "for i in range(train_data.shape[0]):\n",
    "  sent=str(a['hindi_clean'][i])\n",
    "  for word in sent.split():\n",
    "    if cuss_dict[word]!=0:\n",
    "      abuse_f[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QMFqbby_a8_I",
    "outputId": "4398c0bb-9fbb-479a-dfb0-1e50801ac61a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abuse_f[:10]\n",
    "labels_train_raw['labels'][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elHgHogVbu6g"
   },
   "outputs": [],
   "source": [
    "abuse_fd=np.zeros((dev_data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elHGVB0Tb25H"
   },
   "outputs": [],
   "source": [
    "for i in range(dev_data.shape[0]):\n",
    "  sent=str(b['hindi_clean'][i])\n",
    "  for word in sent.split():\n",
    "    if cuss_dict[word]!=0:\n",
    "      abuse_fd[i]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgIw3Ac7QlIs"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3NhuXrxQnnd"
   },
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(vocab_size,100,input_length=max_len)(inputs)\n",
    "    x = Conv1D(256, 3, activation='relu')(layer)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "\n",
    "    x = Conv1D(128, 4, activation='relu')(x)\n",
    "    x = LSTM(100,recurrent_dropout=0.2)(x)\n",
    "    layer = Dense(200,name='FC1')(x)\n",
    "    layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(300,name='FC2')(layer)\n",
    "    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(3,name='out_layer')(layer)\n",
    "    layer = BatchNormalization(name = 'BN4')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "XiKUapNZRPJ6",
    "outputId": "6ea5221c-d6cd-4c76-877a-7763ab0cb793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model=cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mO3wV1XmRsXf",
    "outputId": "48d82c70-5e82-4772-f34b-276eea65eab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "3L3oTMaIRvPt",
    "outputId": "47a6b78e-3fb9-4766-cf54-b28dbf846213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 20, 100)           3572900   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 18, 256)           77056     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3, 128)            131200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 3)                 903       \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 3,956,171\n",
      "Trainable params: 3,955,165\n",
      "Non-trainable params: 1,006\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "UmlQVM41Rz9c",
    "outputId": "91144b2f-1eca-4a51-ee90-605d52d82b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 15131 samples, validate on 1869 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "15131/15131 [==============================] - 23s 2ms/step - loss: 0.9627 - acc: 0.5173 - val_loss: 1.0149 - val_acc: 0.5270\n",
      "Epoch 2/5\n",
      "15131/15131 [==============================] - 15s 963us/step - loss: 0.6421 - acc: 0.7412 - val_loss: 1.0710 - val_acc: 0.5452\n",
      "Epoch 3/5\n",
      "15131/15131 [==============================] - 14s 941us/step - loss: 0.3466 - acc: 0.8955 - val_loss: 1.2627 - val_acc: 0.5420\n",
      "Epoch 4/5\n",
      "15131/15131 [==============================] - 14s 948us/step - loss: 0.2243 - acc: 0.9405 - val_loss: 1.4461 - val_acc: 0.5340\n",
      "Epoch 5/5\n",
      "15131/15131 [==============================] - 15s 965us/step - loss: 0.1701 - acc: 0.9579 - val_loss: 1.5020 - val_acc: 0.5420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7187ea9a20>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix_train,labels_train,validation_data=(sequences_matrix_dev,labels_dev),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19RCFKzmVEjV"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XUIw8gV1kVf"
   },
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,200,input_length=max_len)(inputs)\n",
    "    layer = LSTM(100,recurrent_dropout=0.2)(layer)\n",
    "    layer = Dense(200,name='FC1')(layer)\n",
    "    layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(300,name='FC2')(layer)\n",
    "    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(3,name='out_layer')(layer)\n",
    "    layer = BatchNormalization(name = 'BN4')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "RVpUE7WF2DlK",
    "outputId": "54baa8d1-76dd-4514-f7d5-053816d7ff5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model=RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "3_Iaqkdq2Hp2",
    "outputId": "71b57ca6-9d8e-4dd3-9065-aac531ba544c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "Sv1Bgpe-lgL_",
    "outputId": "0383b424-bbe4-409f-d12d-ea8c1f07f282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 25, 200)           3000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 200)               20200     \n",
      "_________________________________________________________________\n",
      "BN1 (BatchNormalization)     (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "BN2 (BatchNormalization)     (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 3)                 903       \n",
      "_________________________________________________________________\n",
      "BN4 (BatchNormalization)     (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 3,203,815\n",
      "Trainable params: 3,202,809\n",
      "Non-trainable params: 1,006\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "uFYgtVM82dTM",
    "outputId": "84d6fd2b-57d8-43b9-bdaf-eb2c09b3b3b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 15131 samples, validate on 1869 samples\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "15131/15131 [==============================] - 35s 2ms/step - loss: 1.0150 - acc: 0.4766 - val_loss: 0.9824 - val_acc: 0.5409\n",
      "Epoch 2/5\n",
      "15131/15131 [==============================] - 28s 2ms/step - loss: 0.8010 - acc: 0.6447 - val_loss: 0.9687 - val_acc: 0.5538\n",
      "Epoch 3/5\n",
      "15131/15131 [==============================] - 28s 2ms/step - loss: 0.6731 - acc: 0.7356 - val_loss: 1.1291 - val_acc: 0.5431\n",
      "Epoch 4/5\n",
      "15131/15131 [==============================] - 27s 2ms/step - loss: 0.5667 - acc: 0.8005 - val_loss: 1.1341 - val_acc: 0.5522\n",
      "Epoch 5/5\n",
      " 4544/15131 [========>.....................] - ETA: 18s - loss: 0.4483 - acc: 0.8638"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-352fc8037bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_matrix_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences_matrix_dev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(sequences_matrix_train,labels_train,validation_data=(sequences_matrix_dev,labels_dev),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zStPEWPG3WDi",
    "outputId": "0fdbb736-c85c-4d07-915c-7340720990fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madarchod mulle ye mathura me Nahi dikha tha jab mullo ne Hindu ko iss liye mara ki vo lasse ki paise mag liye the       t   co   oxf tr bly '"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sent'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vDWgpzeGcYn"
   },
   "source": [
    "## combinin train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JHX0AwsDGWTb",
    "outputId": "5a35badd-a1a0-4faa-9197-41648625bf91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 20)"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_matrix_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIfVwivzG2YN"
   },
   "outputs": [],
   "source": [
    "new_mat=np.concatenate((sequences_matrix_train,sequences_matrix_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "biljOM3wHAIi",
    "outputId": "38e5964f-b0d4-42ec-822c-f3cd98bcf05d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20189, 20)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EogDim8fHSFG",
    "outputId": "b8161ce6-54bf-42bd-8579-01cd8579e0c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18320, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZezqsToPHWGf",
    "outputId": "e0f1273e-a873-4d28-ca41-160121f05af7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "kRJLWCbNIGRk",
    "outputId": "2c9adddc-c260-4238-f559-aa6aab76f9d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<18320x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 18320 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ft6HkMAgHC8j",
    "outputId": "69dc052c-1334-4f08-dec0-e6108741563e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20189,)\n"
     ]
    }
   ],
   "source": [
    "new_label=np.concatenate((np.array(labels_train),np.array(labels_dev)),axis=0)\n",
    "print(new_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "YPAEtREPIkw8",
    "outputId": "245cf7f0-5fd7-40ba-a8bf-21d2660e2b01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ohc=OneHotEncoder()\n",
    "new_label=ohc.fit_transform(new_label.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NBZFaFumIr_3",
    "outputId": "2f2bba50-aa7a-4e71-c72d-1259cf3982ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20189,)\n"
     ]
    }
   ],
   "source": [
    "new_abuse=np.concatenate((abuse_f,abuse_fd))\n",
    "print(new_abuse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwO3GQbp1V6l"
   },
   "source": [
    "## Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Wjgncgnu31z7",
    "outputId": "5bd56857-2510-48a7-9f40-14954f444ced"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_z6Aora-c6R"
   },
   "outputs": [],
   "source": [
    "root_path = \"/content/drive/My Drive/Sentimix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "J5EV-8QQ8SHP",
    "outputId": "b6d266e2-2c9e-4dc3-ba13-2de1dd3e942e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model_emb_300 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_300\")\n",
    "#model_emb_200 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_200\")\n",
    "#model_emb_100 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tW6uIhyVdgfO"
   },
   "outputs": [],
   "source": [
    "word_index=tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5GP5jtJdflQ"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_1 = np.zeros((len(tok.word_index) + 1, 100))\n",
    "# for word, i in tok.word_index.items():\n",
    "#     if word in model_emb_100.wv.vocab:\n",
    "#       embedding_matrix_1[i] = model_emb_100[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mgn4w8FhEqXw"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_2 = np.zeros((len(tok.word_index) + 1, 200))\n",
    "# for word, i in tok.word_index.items():\n",
    "#     if word in model_emb_200.wv.vocab:\n",
    "#       embedding_matrix_2[i] = model_emb_200[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Hb0EV4g2EqKw",
    "outputId": "8a392014-1abe-410c-e34c-caef70d9cbe0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_3 = np.zeros((len(tok.word_index) + 1, 300))\n",
    "for word, i in tok.word_index.items():\n",
    "    if word in model_emb_300.wv.vocab:\n",
    "      embedding_matrix_3[i] = model_emb_300[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q_LOMuBOKYD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "class Metrics(Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.val_f1s = []\n",
    "    self.val_recalls = []\n",
    "    self.val_precisions = []\n",
    "  \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    val_predict = (np.asarray(self.model.predict([self.validation_data[0]])))\n",
    "    val_targ = self.validation_data[1]\n",
    "    val_predict=val_predict.argmax(axis=-1)\n",
    "    \n",
    "    val_targ=val_targ.argmax(axis=-1)\n",
    "    # print(val_predict)\n",
    "    # print(val_targ)\n",
    "    _val_f1 = f1_score(val_targ, val_predict,average='macro')\n",
    "    _val_recall = recall_score(val_targ, val_predict,average='macro')\n",
    "    _val_precision = precision_score(val_targ, val_predict,average='macro')\n",
    "    self.val_f1s.append(_val_f1)\n",
    "    self.val_recalls.append(_val_recall)\n",
    "    self.val_precisions.append(_val_precision)\n",
    "    print(\" — val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "    return\n",
    " \n",
    "f1_metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AJPD4XQEc9-"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRUGtTlKEMta"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPool1D,GlobalAveragePooling1D\n",
    "class Attention(Layer):\n",
    "    def __init__(self,step_dim=20,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input,CuDNNLSTM,CuDNNGRU\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "def BidLstm(maxlen, max_features, embed_size):\n",
    "    inp1 = Input(shape=(maxlen, ))\n",
    "    #inp2=Input(shape=(1,))\n",
    "    x=Embedding(len(tok.word_index)+1,embed_size)(inp1)\n",
    "    #x = Embedding(len(tok.word_index) + 1,embed_size,weights=[embedding_matrix_3],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x1)\n",
    "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x2)\n",
    "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x3)   \n",
    "    #x = Attention(maxlen)(x)\n",
    "    # x2 = Attention(maxlen)(x2)\n",
    "    # x3 = Attention(maxlen)(x3)\n",
    "    # x=  Concatenate()([x1,x2,x3])\n",
    "    x=SpatialDropout1D(0.1)(x)\n",
    "    x = CuDNNLSTM(200, return_sequences=True)(x)\n",
    "    x = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                       attention_regularizer_weight=1e-4,\n",
    "                       name='Attention')(x) \n",
    "    #x = Attention(maxlen)(x)\n",
    "    # layer = Dense(600,name='FC1')(x)\n",
    "    # layer = Dense(300,activation='relu')(layer)\n",
    "    # layer = Dense(200,activation='relu')(layer)\n",
    " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    # layer = Activation('relu')(layer)\n",
    "    # layer = Dropout(0.4)(layer)\n",
    "    x2=GlobalMaxPool1D()(x)\n",
    "    x3=GlobalAveragePooling1D()(x)\n",
    "    x=  Concatenate()([x2,x3])\n",
    "    layer = Dense(128,name='FC2')(x)\n",
    "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "   # layer=  Concatenate()([layer,inp2])\n",
    "    # layer=Dense(256,activation='relu')(layer)\n",
    "    # layer=Dense(128,activation='relu')(layer)\n",
    "    layer = Dense(3,name='out_layer',activation='softmax')(layer)\n",
    "\n",
    "    model = Model(inputs=[inp1],outputs=layer)\n",
    "\n",
    "    return model\n",
    "model=BidLstm(max_len,max_features=len(tok.word_index)+1,embed_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uITF0D84Atv"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc',km.f1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "d2DGEX_P5qWJ",
    "outputId": "5b4a3955-9cad-4907-c9af-cdf56aaaf30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 25, 300)      9794100     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_4 (SpatialDro (None, 25, 300)      0           embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_17 (CuDNNLSTM)       (None, 25, 200)      401600      spatial_dropout1d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Attention (SeqSelfAttention)    (None, 25, 200)      12865       cu_dnnlstm_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_16 (Global (None, 200)          0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_16 (Gl (None, 200)          0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 400)          0           global_max_pooling1d_16[0][0]    \n",
      "                                                                 global_average_pooling1d_16[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "FC2 (Dense)                     (None, 128)          51328       concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 128)          0           FC2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 3)            387         dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 10,260,280\n",
      "Trainable params: 10,260,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPswR_f7RF-w"
   },
   "outputs": [],
   "source": [
    "cp_filepath=root_path+'/checkpoints/lstm_self_attention.h5'\n",
    "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=5,restore_best_weights=True)\n",
    "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "b-aKbqPo4Q2R",
    "outputId": "6a858edb-2134-4fa7-e7f8-d6252d2c211f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15131 samples, validate on 1869 samples\n",
      "Epoch 1/10\n",
      "15131/15131 [==============================] - 9s 613us/step - loss: 0.8859 - acc: 0.5742 - f1_score: 0.5190 - val_loss: 0.9530 - val_acc: 0.5832 - val_f1_score: 0.6100\n",
      "Epoch 2/10\n",
      "15131/15131 [==============================] - 8s 535us/step - loss: 0.6121 - acc: 0.7510 - f1_score: 0.7752 - val_loss: 1.1272 - val_acc: 0.5522 - val_f1_score: 0.5595\n",
      "Epoch 3/10\n",
      "15131/15131 [==============================] - 8s 549us/step - loss: 0.3361 - acc: 0.8795 - f1_score: 0.9053 - val_loss: 1.4804 - val_acc: 0.5463 - val_f1_score: 0.5886\n",
      "Epoch 4/10\n",
      "15131/15131 [==============================] - 8s 533us/step - loss: 0.1741 - acc: 0.9414 - f1_score: 0.9571 - val_loss: 1.8002 - val_acc: 0.5393 - val_f1_score: 0.5521\n",
      "Epoch 5/10\n",
      "15131/15131 [==============================] - 8s 533us/step - loss: 0.0976 - acc: 0.9664 - f1_score: 0.9749 - val_loss: 2.4005 - val_acc: 0.5195 - val_f1_score: 0.5363\n",
      "Epoch 6/10\n",
      "15131/15131 [==============================] - 8s 524us/step - loss: 0.0616 - acc: 0.9787 - f1_score: 0.9839 - val_loss: 3.1045 - val_acc: 0.5152 - val_f1_score: 0.5526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4dab62b390>"
      ]
     },
     "execution_count": 165,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([sequences_matrix_train],labels_train,validation_data=([sequences_matrix_dev],labels_dev),epochs=10,batch_size=32,callbacks=[es,cp_check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MDQNjJ9wL-1S",
    "outputId": "5061711b-d311-4be3-c3a8-df9a1bd9b7d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "9epzi9CSLzYY",
    "outputId": "2f230009-33e1-433c-b5c9-924ddcbe367c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1869/1869 [==============================] - 1s 741us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.67      0.61       533\n",
      "           1       0.57      0.54      0.56       754\n",
      "           2       0.62      0.56      0.59       582\n",
      "\n",
      "    accuracy                           0.58      1869\n",
      "   macro avg       0.59      0.59      0.59      1869\n",
      "weighted avg       0.58      0.58      0.58      1869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict([sequences_matrix_dev], batch_size=32, verbose=1)\n",
    "\n",
    "print(classification_report(labels_dev_le, np.argmax(y_pred,axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxihkdmI0_YA"
   },
   "outputs": [],
   "source": [
    "s=b['hindi_clean']\n",
    "sequence_test = tok.texts_to_sequences(s)\n",
    "sequence_test_mat = sequence.pad_sequences(sequence_test,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AAqkpA7f1evt",
    "outputId": "a199c706-892b-4893-c133-980edb96c5cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 1, 42, 1268, 52, 36, 3330, 3529, 431, 5378, 18483, 16049, 4280, 159, 9292, 2336, 8, 14, 3492], [20, 20, 20, 62, 965, 684, 1579, 1202, 36, 7497, 1528, 439, 80, 439, 80, 439], [1, 1, 10, 14032, 1, 1, 1, 89, 2922, 1354, 904, 3285, 195, 577], [23690, 692, 443, 23690, 164, 954, 12, 1303, 23, 98, 25, 228], [1, 1, 5, 137, 152, 215, 2458, 170, 9, 19, 1010, 38, 65, 1977, 1992], [1, 66, 10, 469, 1, 35, 125, 206, 12, 90, 936, 936, 264], [439, 146, 10465, 31, 73, 101, 488, 4654, 4654, 593, 59, 4930, 12, 9595], [1, 53, 2096, 150, 1337, 564, 416, 2, 38, 98, 2342, 71, 65, 6, 163, 106, 147, 2232], [517, 170, 32, 5, 3038, 4, 6958, 94, 4, 1453, 3, 2743, 3, 9007, 3, 2557, 3, 1730, 3, 978, 3768, 798, 5, 2823], [1, 3799, 1, 126, 63, 95, 177, 4, 17082, 9, 476, 3447, 10202, 10855, 7810, 158, 126, 71], [1, 1, 55, 78, 278, 232, 4, 41, 71, 30, 219, 17, 116, 263, 365, 10045, 5415, 3754, 3754, 12890, 1443], [1, 1, 413, 5, 2961, 20, 1079, 3163, 17, 279, 4, 40, 443, 1399, 182, 150], [1, 80, 6, 344, 5277, 899, 2, 1856, 7, 553, 9, 87, 48, 24, 32, 464, 1965, 80, 5, 153], [504, 3222, 336, 287, 2, 26, 39, 23, 9, 186, 73, 10636, 10427, 8493, 312], [1, 48, 6, 525, 274, 283, 797, 150, 90, 2, 71, 330, 1414, 2, 30, 9, 5, 65, 311, 44], [1, 397, 5, 1, 1, 397, 5, 24, 349, 145, 330, 4, 6, 443, 993, 1096, 111, 12, 1431, 7, 138], [3054, 30, 24, 7, 466, 3341, 17, 156, 82, 370, 370], [1, 872, 126, 2222, 282, 126, 372, 112, 31, 350, 1652, 52, 884, 139, 1, 218, 582], [13, 1, 3808, 10, 2407, 1605, 2852, 10653, 1237, 378, 3897, 7918, 29, 30476, 216, 1], [1, 1, 1, 1029, 46, 18, 93, 222, 1936, 1277, 29, 2460, 93, 14, 252, 4898], [1, 174, 67, 3030, 113, 1078, 119, 4304, 17, 174, 9, 1228, 107, 155, 3316, 67, 3030, 113, 645, 174, 1241, 12], [13, 1, 74, 1005, 684, 354, 17, 279, 245, 129, 64, 119, 361, 242, 9681, 110, 56, 12, 749, 9183, 963, 88, 17, 2255, 506], [590, 36, 68, 3020, 28, 488, 3200, 5, 1095, 159, 401, 272, 273], [3608, 60, 60, 253, 83, 360, 83, 1670, 83, 6833, 27446, 360, 3791, 4738, 5112, 12, 2164, 15254, 4], [1, 1640, 2, 20, 1272, 851, 2, 851, 11, 17570, 2, 8410, 7831, 136, 13505], [1, 1, 5426, 1, 608, 48, 663, 170, 5493, 25, 2194, 6363, 28, 200, 160, 59], [13, 1, 567, 10010, 1063, 36, 5, 12730, 5, 12730, 600, 35, 542, 233, 583, 106, 179, 8, 291, 687], [1, 1, 3851, 5, 1, 1, 96, 3277, 796, 2], [1, 331, 293, 4, 7227, 3036, 4, 32207, 17, 147, 407, 4, 1349, 430, 705, 2, 147, 1745, 1576, 2, 100, 5, 135], [13, 1, 93, 70, 8, 21892, 49, 1190, 59, 154, 18, 1148, 1482, 9895, 137, 306, 1050, 1050, 2543, 52, 144, 567], [1912, 25, 56, 45, 74, 3305, 527, 125, 206, 11, 361, 19, 212, 4428, 6, 4614, 25, 49, 173, 804], [1, 44, 5, 523, 191, 1138, 38, 82, 395, 129, 2, 450, 133, 127, 4, 2, 136, 177, 4, 1467, 4], [1, 58, 1101, 8443, 1272, 66, 1854], [22621, 6475, 82, 2430, 186, 2, 25, 4, 990, 905, 17, 5063, 169, 2, 212, 209, 50, 178, 3154], [13, 1, 14, 315, 587, 73, 379, 31, 33, 193, 18, 6, 154, 15, 70, 28, 202, 1289, 2258, 27, 42, 15, 6, 14, 1209, 28, 345, 27, 8, 1019, 8880, 5661], [13, 1, 6546, 96, 119, 26116, 51, 104, 2, 29613, 2203, 100, 1323, 1965, 26116, 2535, 718, 123, 560, 1266, 117], [13, 1, 1, 2771, 10, 33, 5, 66, 8714, 1652, 10, 14, 10044, 957, 1900, 8102], [13, 1, 4810, 5, 1903, 37, 3, 8, 640, 532, 919, 920, 270, 249, 255, 1, 4037, 1340, 2602, 94, 3405, 2373, 808, 171, 1018, 2010, 256, 196, 27, 441, 27, 441, 532, 906], [13, 1, 2767, 147, 2767, 147, 2767, 147, 39, 89, 2767, 147, 252, 1, 1, 1524, 348, 345, 1342], [371, 30, 24, 5260, 19, 4010, 11, 20421, 217, 26, 204, 20422, 11, 57, 2600, 900, 203, 17, 9, 263, 129, 2, 84], [13, 1, 546, 129, 217, 18, 226, 621, 81, 82, 897, 16, 57, 4140, 11, 151, 113, 1312, 699, 8644, 23, 2447, 26, 23, 9], [1, 3386, 85, 32, 20, 3872, 495, 2, 4, 1431, 140, 860, 19, 6, 1974, 3532, 164, 293, 19, 110, 4, 13356, 494], [184, 88, 983, 64, 771, 1538, 1093, 2632, 14], [1, 1571, 2496, 1723, 2, 26, 20, 98, 10549, 447, 1058, 3, 1685, 384, 1625, 364, 815, 3512, 538], [145, 153, 18567, 12407, 16, 9, 2717, 41, 5358, 404, 32, 636, 7795, 51, 90, 88, 555, 8873, 67, 41], [13, 1, 1598, 5, 66, 355, 66, 7577, 942, 27890, 66, 686, 66, 1854], [1, 24, 60, 60, 264, 234, 15, 190, 11798, 31, 4021, 962, 22739, 5], [1, 2519, 254, 1242, 89, 34, 17, 1963, 1920, 435, 82, 214, 327, 305, 10, 2253, 152, 3651, 32, 13971, 6], [996, 5, 7620, 11, 57, 231, 105, 347, 239, 8321, 13123, 44, 27364, 820, 1614, 2829, 64, 8321, 13123, 11, 71, 231, 2516, 105, 6, 2894], [1, 1, 547, 740, 16, 330, 32, 1841, 230, 6, 85, 7, 174, 126, 64], [1, 6893, 15, 317, 1018, 173, 42, 28, 234, 6, 154, 18801], [69, 2507, 371, 30, 24, 194, 663, 45, 17, 828, 828, 253, 48, 34, 7, 728, 45, 2132, 5, 11, 183, 364, 40, 45], [1, 1231, 31788, 1128, 2905, 631, 25, 2, 956, 147, 9989, 631, 88, 631, 794, 9, 20167, 48, 1578, 1433, 17], [1, 7678, 16, 1629, 4117, 10, 966, 223, 181, 6756, 13518, 5, 5, 12025, 54, 191, 177, 5338, 12, 108, 64, 136, 133, 127], [1028, 53, 7, 922, 4, 4978, 25, 476, 29, 52, 483, 52, 2777, 5288, 56, 4467, 64], [1, 1, 43, 164, 6, 19453, 17, 928, 4, 444, 12591, 4, 25105, 164, 7, 2449, 64, 2645, 3729, 203, 2386], [1, 1, 1, 1, 15, 623, 325, 115, 168, 3149, 83, 303, 115, 115, 7, 7720, 6064], [1, 20, 879, 54, 77, 9, 51, 1010, 20, 551, 9, 1325, 63, 546, 23, 9, 416, 20], [1, 44, 42, 58, 5605, 3, 8, 22, 10, 21, 3115, 2422, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91], [1, 48, 20, 2167, 5245, 7571, 4, 4769, 404, 30, 6220, 1759, 14, 26, 1, 5, 25], [1, 1, 20, 7675, 74, 1186, 6447, 2018, 480, 4, 456, 23109, 7, 1077, 18, 17], [224, 99, 6, 559, 6, 325, 533, 357, 414, 79, 36, 14, 92, 91, 1403, 175, 379, 6, 1876, 1799, 3], [13, 1, 134, 15, 1202, 134, 15, 29, 59, 14, 146, 201, 273, 554, 1786, 28, 72, 14, 66, 1202, 492, 294, 27, 639, 27, 75, 27], [13, 1, 23542, 101, 193, 15145, 5766, 4030, 5, 4, 2412, 491, 5, 314, 159, 943, 5, 17, 2564, 29], [1, 6278, 30, 24, 235, 57, 238, 290, 190, 56, 4, 6201, 22123, 90, 235, 420, 31, 5, 135], [1, 29783, 8622, 30, 24, 7, 2002, 238, 950, 4, 10708, 27211, 234, 172, 2282, 5, 1602, 153], [1, 5, 1, 519, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 76, 61, 76, 61, 199, 3271, 16438, 332, 33, 830, 44, 329, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 37, 3, 8, 640], [9, 7587, 3424, 2194, 140, 85, 1752, 106, 4339, 2, 6, 1098, 85, 11, 461, 2262, 2], [1, 234, 2275, 35, 548, 1399, 18, 27229, 256, 196], [1, 2684, 414, 1, 1, 6546, 1, 5, 4053, 6, 10524, 8690, 618, 79, 36, 14, 92, 91, 37, 3, 8, 1328, 532], [1, 304, 7358, 647, 3512, 30, 24, 46, 18, 550, 2115, 54, 1495, 2484, 2, 2053, 685, 32365, 4, 2940, 11, 3630, 1604], [1, 2149, 733, 78, 468, 1690, 40, 88, 377, 7, 12, 1461, 19, 20, 641, 1017, 78, 199, 150, 41, 794, 1461, 74, 12], [12592, 448, 482, 937, 2178, 510, 1564, 2122, 24400, 234, 503, 11, 203, 551, 90, 4285], [1278, 1485, 4, 3300, 56, 56, 12, 157, 25, 2749, 54, 481, 3, 481, 3, 176, 3, 5777, 798, 5], [1, 1, 35, 235, 1057, 5702, 19, 12, 192, 286, 239, 30, 18376, 7, 115, 11, 69, 11, 1322], [1, 1, 1, 83, 244, 115, 33, 1173, 55, 327, 305, 10, 2253, 1541, 1054, 5180], [686, 1440, 168, 14, 1560, 10, 17316, 155, 2042, 8055, 24039, 1883], [35, 24, 5, 125, 206, 1507, 1580, 16, 2370, 538, 11, 984, 824, 117, 184, 380, 1820, 26, 1021, 54, 64, 9164], [34, 163, 283, 2271, 82, 476, 669, 19, 26, 43, 97, 54, 9, 2, 555, 90, 476, 9, 669, 19, 54], [1, 80, 808, 348, 86, 15, 2322, 1370], [1, 133, 127, 32, 23, 5320, 6893, 6188, 220, 2516, 2, 583, 1416, 2441, 4, 692, 3773, 12], [1, 24, 519, 8227, 24, 2682, 568, 12206, 94, 385, 1545, 538, 210, 2], [13, 1, 62, 42, 15, 345, 5, 1833, 511, 75, 27, 62, 55, 30941, 168, 14, 428, 15, 511, 42, 36, 123, 19069, 62, 1042, 58, 173], [1, 30, 385, 23, 1982, 1519, 393, 2625, 11, 282, 4191, 11, 45, 21104, 11, 146, 985, 168, 628, 30731, 4477, 397, 3, 8, 22, 10, 21], [1, 1, 559, 834, 1693, 94, 1545, 80, 166, 25019, 149, 72, 273, 28405, 4, 83], [4027, 1556, 68, 1407, 1890, 309, 8492, 845, 218, 29, 4464, 28, 291, 353, 40, 6589, 6, 142, 15], [1, 1641, 230, 32, 2354, 7, 1914, 2, 48, 28427, 2729, 23, 77, 10006, 9, 416], [1247, 1, 142, 15, 3503, 3898, 1650, 377, 335, 18, 80, 3503, 29], [1, 2346, 29, 46, 1598, 42, 15, 49, 173, 666, 2396, 4800, 12, 90, 1], [1, 110, 65, 145, 145, 961, 1919, 989, 17, 1840, 6, 71, 3464, 178, 589, 1690, 64, 18, 2871, 7, 6], [1, 2195, 550, 252, 4220, 27159, 609, 7153, 23, 6, 1875, 353, 17, 20985, 64, 28, 7809, 11, 23], [13, 1, 2, 308, 366, 30, 2, 308, 2755, 30, 11502, 1830, 652], [54, 2032, 3530, 4, 948, 23, 57, 1400, 40, 74, 12, 18362, 16955, 16, 28, 180, 411, 345, 6, 345, 287], [1, 1, 143, 35, 62, 246, 10, 15, 34, 188, 507, 2, 34, 777, 12876, 533, 271, 11800], [1, 768, 137, 14, 452, 55, 62, 965, 2874, 15669, 8, 1353, 1994], [13, 1, 30, 24, 4, 6993, 188, 5, 1195, 17, 6333, 124, 32, 2218, 26911, 54, 30, 24, 4, 6993, 188, 1454, 12, 5, 1248], [1, 1, 12140, 34, 3050, 18, 2275, 102, 167, 36, 68, 607, 3050], [13, 1, 366, 7273, 86, 432, 2284, 10, 575, 1536, 353, 79, 36, 14, 92, 91, 176, 3, 307, 3, 8, 260, 176, 3, 1, 1, 1, 1], [1, 6753, 1, 2107, 10112, 133, 25, 13160, 178, 220, 342, 2, 5, 237, 17, 27591, 853, 51, 181], [1, 5, 1, 96, 207, 5500, 88, 39, 282, 84, 322, 12, 795, 4127, 1632, 2, 442, 5900, 6, 6], [13, 1, 6099, 8587, 4145, 5809, 3097, 6049, 22949, 66, 10, 16, 2338, 222, 42, 13374], [1, 3153, 23, 564, 2030, 44, 779, 651, 112, 178, 19, 939, 2601, 788, 3396, 20, 9, 4, 1962, 788, 156, 129], [13, 1, 52, 189, 448, 1, 29345, 29346, 258, 198, 37, 3, 8, 27, 50, 613, 3, 70, 3357, 29347, 381, 3, 8, 37, 50, 381, 3, 8, 37, 50], [6390, 23979, 24822, 300, 231, 25982, 190, 231, 5, 2892, 8726], [1, 276, 1, 1, 851, 2, 54, 187, 9347, 679, 3670, 728, 18, 1944, 107, 3951, 586, 38, 2], [1, 1, 1, 83, 303, 115, 1457, 34, 1520, 10818, 489, 1241, 14, 9894, 83, 303, 115], [1, 192, 184, 6946, 46, 231, 213, 28304, 6566, 168, 123, 102, 1152, 2283, 24, 852, 27, 1183], [1, 2501, 6543, 213, 52, 86, 120, 1252, 1255, 7, 993, 9, 169, 3344, 58, 213, 75, 27, 75, 27], [1, 1, 4267, 5, 280, 4267, 214, 327, 305, 5, 609, 570, 3451], [1535, 26, 1778, 84, 32, 12, 241, 2, 1088, 4872, 523, 12, 3673, 69, 26, 13239, 26, 31, 12, 8138, 9555, 1728], [1, 1640, 186, 2, 268, 142, 15, 118, 14, 786, 550, 652, 518, 14, 738, 10, 1, 686], [42, 15, 1, 2010], [19290, 30681, 4, 16, 455, 4629, 455, 20281, 5, 16, 21157, 873, 5028], [1, 17, 121, 7, 25, 11334, 186, 40, 24815, 274, 40, 26, 4829, 2400, 269], [1, 73, 1, 1, 191, 177, 7, 15522, 182, 245, 3917, 3624, 1365, 976, 113, 147, 110], [1, 344, 681, 6712, 659, 815, 546, 19, 23, 41, 1010, 38, 128, 7, 6729, 17, 3165, 259, 19, 351, 36, 15, 35, 1], [1, 1378, 10707, 42, 6270, 2605, 16, 755, 478, 184], [13, 1, 68, 5, 314, 134, 15, 29, 68, 1344, 587, 73, 5191, 146, 1383, 6, 117, 68, 785, 99], [1, 1, 43, 1395, 64, 26, 3417, 148, 740, 16, 23, 43, 25, 5493, 788, 4, 861, 2073, 3642, 84, 2554], [13, 1, 310, 1263, 6325, 7600, 125, 206, 5, 206, 4268, 1142, 12526, 2, 26, 586, 6204, 1964, 7, 1007, 26860], [1, 15044, 452, 1676, 9260, 28046, 15285, 44, 184], [13, 1, 14196, 827, 29, 46, 214, 49, 70, 363, 175, 2054, 2694, 139, 2689, 10691, 1203, 352, 602, 432, 1443], [1, 52, 2742, 35, 414, 31, 14, 7366, 10, 2179, 22066, 14, 836, 18, 2692, 179, 1202, 819, 1528, 35], [13, 1, 16, 759, 179, 22440, 4001, 641, 306, 266, 2370, 44, 22440, 100, 63, 65, 4675, 19, 104, 19, 5451, 266, 170, 4, 9592], [13, 1, 48, 20, 121, 5, 4, 8151, 13149, 13031, 32197, 2066, 32198, 26, 5, 4, 8151, 2510, 17, 3473, 8782, 4307, 203, 2376, 88], [13, 1, 1, 1221, 5, 1, 4963, 1, 1, 2163, 1, 1, 1, 15, 55, 49, 3080], [1, 1, 1, 1, 5265, 5298, 373, 158, 100, 933, 867, 84, 3718, 5, 5, 9634, 11, 2086, 3795, 35, 11], [1, 1, 1, 187, 457, 115, 344, 26, 1081, 65, 669, 107, 1723, 9, 19, 322, 7988], [13, 1, 1124, 5213, 10, 66, 1584, 29, 10510, 378, 4769, 11026, 1821, 998, 489], [452, 563, 2227, 8084, 653, 4996], [13, 1, 1, 1, 519, 1613, 1956, 238, 18, 1432, 7, 25, 1308, 4590, 525], [1, 1, 1, 1, 1581, 5, 524, 6, 43, 4, 879, 25, 220, 6, 279, 25, 506, 1783], [1, 31, 1, 1, 3980, 4, 2469, 152, 625, 9, 2, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61], [1, 83, 303, 115, 295, 2223, 83, 303, 814, 83, 303, 115, 1087, 5845, 2928, 2535], [1, 1, 1, 1, 630, 331, 2, 40, 41, 621, 1012, 8435, 453, 1640, 2, 183, 928], [1, 1, 4086, 9724, 9724, 210, 7, 1364, 449, 2, 714, 17, 337, 19, 108, 283, 1842, 6, 1376, 48, 346, 108], [1, 63, 65, 20095, 45, 22465, 19, 7007, 19, 1, 1, 1619, 121], [1, 1, 3382, 275, 358, 2672, 290, 17, 1, 412, 23, 263, 525, 2276, 4, 465, 299, 19, 939, 76, 61], [299, 730, 194, 96, 1103, 787, 4, 437, 1783, 567, 14, 190, 1103, 300, 139, 1558, 5, 1103], [794, 72, 12, 48, 6, 133, 1653, 12, 78, 57, 1653, 11, 2, 136, 10553, 3036, 133, 239, 586, 14], [1, 85, 16, 283, 2271, 12, 23156, 112, 11, 180, 9, 2299, 544, 240, 3963, 87, 6, 16], [1, 31, 5, 52, 4544, 199, 6248, 6623, 228, 44, 1461, 4377, 4056, 14, 560], [1, 234, 149, 602, 764, 275, 52, 29, 68, 62, 4422, 35, 101, 132, 459, 9871, 310], [1, 1, 1, 24, 194, 233, 2108, 1791, 32, 3129, 16, 181, 495, 2, 336, 162, 2427, 12, 332], [29, 221, 168, 12069, 1675, 533, 31, 146, 5, 4695, 5, 59, 23270, 5, 5, 4036, 3195, 1180], [1, 1, 69, 2651, 12943, 3949, 6471, 1180, 12, 722, 34, 60, 2267, 1569, 51], [1, 1135, 561, 145, 116, 9, 1840, 78, 12422, 259, 1840, 96, 3527, 67, 170, 9, 2, 617, 7961, 47, 503], [1, 1, 1, 155, 2156, 97, 4850, 354, 17, 155, 1472, 2, 15742, 9], [20, 547, 85, 4, 209, 39, 3666, 54, 780, 209, 23, 19, 26, 1860, 23, 5, 8060, 649, 20, 23, 39, 150], [1, 1, 18, 146, 1648, 1867], [1, 2808, 199, 150, 41, 5134, 95, 85, 17, 3368, 7626, 2264, 44, 45, 4356, 178, 1913, 8392, 11], [487, 261, 65, 2, 728, 248, 297, 23, 9, 2, 86, 2898, 2114, 2636, 8588, 728, 25, 725, 742, 2], [1, 1, 397, 1, 1, 6766, 1, 27463, 69, 228, 106, 1430, 106, 4, 83, 26, 558], [1, 754, 2966, 24786, 354, 17, 497, 7, 39, 319, 41, 186, 2620, 48, 56, 4, 4751, 319, 3108, 10990], [1, 9611, 1, 1, 1, 1, 1687, 1, 77, 725, 779], [1, 5467, 1618, 2415, 436, 11, 12986, 1372, 9, 2], [1, 3460, 168, 723, 336, 1747, 203, 278, 262], [1, 285, 6144, 181, 589, 779, 2, 145, 429, 7421, 313, 2113, 2, 110, 4, 98], [1, 1, 101, 166, 587, 467, 2336, 15, 1274, 5639, 99, 6, 925, 26, 34, 17, 6, 2956], [13, 1, 134, 15, 1, 29, 14, 613, 3, 31934, 1611, 1037], [1, 11, 12292, 77, 162, 11819, 2, 4, 1779, 41, 7070, 12, 4611, 7911, 2, 20, 21069], [1, 1, 4194, 137, 15, 379, 609, 29, 9729, 216, 280, 2091, 17899, 8, 15, 505, 135], [300, 42, 20661, 8, 193, 168, 29047, 2286, 1254, 5], [18483, 18665, 44, 30675, 8472, 44, 4575, 3330, 11155, 310, 3595, 633, 4280, 310, 2848, 12264, 12264, 44], [1, 18, 150, 89, 39, 9, 2, 25, 9, 242, 60, 17, 155, 20422, 2, 38], [418, 148, 36, 1, 373, 281, 179, 22716], [1, 140, 107, 11, 808, 31369, 1442, 651, 11322, 29213, 252, 393, 14, 4367, 29, 19520, 56, 144], [1, 724, 1, 42, 6, 154, 14, 1383, 36, 10225, 3, 5462, 242, 762, 103, 127, 177, 7, 27025], [1, 1, 1, 7101, 63, 9, 19, 38, 2234, 1902, 3556, 2, 827, 1735, 15147, 8614], [374, 1676, 12, 106, 11, 4224, 32, 82, 420, 29058], [13, 1, 1, 4613, 829, 864, 9, 158, 7172, 222, 6585, 1425, 87, 26, 3545, 4385, 2089, 278, 365, 234], [1, 330, 11, 1776, 178, 205, 681, 601, 41, 354, 210, 120, 4, 230, 18308, 19, 1025, 140, 230, 20988], [1, 86, 304, 6111, 16, 17, 20, 516, 316, 129, 2, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [7, 10062, 310, 12128, 5378, 7223, 765], [13, 1, 612, 11815, 19323, 3, 5, 10912, 10, 14, 3522, 29, 59, 15, 27888, 357, 335], [1, 10780, 2427, 2833, 1068, 1593, 46, 1086, 175, 312, 3853, 10], [1, 83, 360, 116, 35, 35, 666, 48, 163, 34, 67, 25, 1530, 163, 34, 38, 18, 53, 16, 1189, 6947, 84, 416, 105], [13, 1, 1, 204, 53, 12, 1164, 1027, 29783, 1, 24, 7, 420, 253, 299, 1, 17, 657, 1761, 97, 1, 217, 3382], [1, 1, 219, 23, 442, 32, 162, 4651, 925, 15558, 5, 6776, 2466, 87, 4641, 12, 3192], [1, 1, 1, 1, 1, 9, 20, 1052, 9, 64, 577, 947, 5898, 25, 9], [1, 34, 11, 5221, 6, 2, 212, 34, 6, 60, 580, 20111, 19, 5221, 112, 12, 90], [18, 286, 4, 308, 1762, 292, 129, 4, 3021, 1337, 1592, 374, 45, 11425, 215, 1655, 9], [1, 5, 1, 63, 174, 95, 1820, 67, 386, 19, 136, 38, 3118, 19, 97, 174, 95, 1820, 67, 25, 11155], [1, 2120, 54, 169, 2, 317, 3, 8, 22, 10, 21, 109, 539, 4053, 4408, 181, 167, 16, 1, 2163, 7, 1399, 405, 3, 8, 22, 10, 21, 20], [1, 5793, 25, 5323, 1037, 4994, 7, 340, 1443, 3330, 3892, 469, 310, 4571], [1, 1, 4418, 4207, 70, 202, 52], [1255, 7, 4093, 4093, 167, 461, 7077, 621, 9167, 239, 1306, 45, 3205, 239, 461, 7077, 38, 151], [1, 1127, 765, 459, 276, 18, 1286, 46, 34, 11, 2496, 38, 103, 442, 11, 1862, 129, 5, 1615, 5877, 726, 2458, 2, 7315], [1, 1, 1, 1, 1, 1, 1, 794, 964, 6, 346, 12], [1, 1525, 4098, 16814, 30394, 16, 14049, 859, 1020], [1, 34, 12, 1773, 12774, 4, 11700, 259, 47, 35, 15, 55, 146, 547, 31, 56], [30, 7, 162, 53, 17, 2366, 730, 2, 6, 2367, 181], [1, 1, 843, 1323, 7665, 275, 15, 59, 14, 66, 336, 584, 4, 5758, 7, 290, 57, 6199], [13, 1, 1, 1, 1, 1208, 1, 1, 1208, 137, 982, 151, 311, 51, 87, 400, 176, 3, 8, 37, 50, 70, 6, 105], [1, 1, 5, 862, 370, 762, 3949, 161, 32059, 7, 2156, 3793, 3949, 18, 586], [1, 1676, 2783, 131, 408, 11, 1291, 934, 8394, 382, 12539], [1, 1, 4, 2, 741, 85, 1043, 208, 4298, 2, 18, 514, 4, 3593, 17, 48, 63, 65], [1, 1, 1, 16, 649, 14, 6, 25052, 1194, 1154, 7, 1947, 521, 749, 286], [1, 1, 1, 1, 2453, 1, 2028, 57, 1079, 4435, 2, 4216, 412, 135], [234, 29, 14, 66, 75, 27, 633, 736, 9084, 4231, 1384, 1384, 10231, 6556, 25113, 731, 3, 424, 27], [1, 233, 349, 153, 975, 65, 1842, 3073, 104, 14, 4, 2313, 38, 26723, 2, 2313, 38, 1386, 503, 1801, 32, 95], [1, 4839, 7256, 5, 892, 184, 1935, 1188, 84, 705, 435, 1301, 4440, 15708, 735, 2, 377, 15709, 448, 803, 37, 3, 8, 27, 50, 1357], [480, 20, 38, 21471, 44, 1, 5, 715, 791, 17, 97, 27548, 84, 705, 848, 27, 1058, 3, 165, 130, 3, 165, 130, 3, 1625, 182, 841, 425], [1000, 18, 462, 448, 231], [1, 10712, 5, 1, 1, 55, 15, 285, 45, 137, 40, 18, 1286, 1139, 73, 18, 3922], [46, 300, 419, 33, 132, 2], [1, 1, 396, 214, 288, 1677, 376, 288, 1614], [1, 814, 35, 85, 2610, 89, 4842, 2994, 4586, 7, 23, 41, 227, 1020, 26, 74, 7, 23, 41, 176, 343, 3], [1, 1, 1571, 12, 1571, 3193, 7509, 2, 20, 2491, 3, 8, 22, 10, 21, 165, 130, 3, 3, 8, 22, 10, 21, 165, 130, 3, 3, 8, 22, 10, 21, 165, 130, 3, 3, 8, 22, 10, 21, 5, 642, 6, 554, 29, 1, 81], [1, 1, 866, 4314, 162, 8375, 97, 12572, 3392, 2, 17, 3143, 153, 4, 18676, 3392, 9, 51, 269], [1, 214, 1403, 793, 4, 96, 866, 3843, 4, 8346, 2, 4420, 879, 7, 2208, 197, 12, 90], [1, 1, 29783, 371, 30, 45, 112, 7, 156, 4, 420, 264], [13, 1, 6985, 583, 27987, 170, 168, 892, 628, 1, 523, 24, 333, 10122, 144, 413, 22140, 24], [1, 1, 46, 18, 93, 190, 7809, 8703, 989, 118, 6520, 7, 6507, 398, 64, 14806, 32, 1262, 14], [1, 4180, 34, 692, 25, 883, 8156, 7091, 333, 2, 97, 34, 7, 114, 23, 692, 313, 217, 82, 34, 12, 2774, 3866], [371, 144, 146, 125, 531, 70, 30584, 565, 676, 59], [236, 18, 93, 68, 432, 4759, 243, 15, 511, 5641, 1286, 48, 15003, 54, 10883], [1, 1, 1, 52, 1617, 468, 2055, 118, 252, 394, 11008, 3161, 11], [1, 1, 1, 1432, 905, 12, 1254, 5928], [1093, 51, 5, 2151, 2, 176, 3, 8, 327, 50, 34, 23, 469, 1327, 541, 26, 30018, 8, 486], [1, 1026, 256, 49, 142, 175, 251, 30164, 2380, 55, 175, 42, 30163, 30830, 18, 33, 1014, 28, 2892, 18, 462, 4710, 8245, 5013], [1350, 19549, 17, 3168, 104, 14, 5, 3202, 18, 5196, 48, 429, 54, 650, 19, 391, 5, 1542, 23, 9, 1411, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [13, 1, 10441, 12543, 31330, 5, 12775, 2078, 401, 6099, 131, 598, 511, 165, 130, 3, 73, 1346, 13061, 1090], [1, 519, 9, 188, 1672, 538, 188, 6, 245, 416, 2, 44], [784, 3333, 732, 2774, 354, 210, 107, 2199, 16604, 11, 6694, 26, 107, 11, 445], [1, 1, 5, 1393, 871, 883, 65, 7560, 47, 1631, 65, 23, 7560, 47, 871, 124, 11, 192, 2720, 19620], [13, 1, 2701, 236, 1268, 49, 70, 33, 27], [13, 1, 533, 271, 263, 32, 12, 167, 2087, 183, 594, 413, 12, 9655, 17, 74, 5, 5, 3768, 978], [1, 35, 62, 42, 15, 86, 62, 379, 6, 154, 68, 2197, 31, 464, 4727, 5, 89, 123, 54, 820, 5], [1, 1, 20, 10128, 11, 2, 20, 5790, 857, 573, 546, 967, 5909, 6756, 17], [13, 1, 134, 15, 1, 29, 462, 312, 1018, 1784, 134, 15, 1, 4289], [35, 59, 14, 66, 118, 289, 5695, 59, 10, 56, 1166, 56, 1166, 2004], [1, 169, 2, 140, 235, 2115, 28908, 760, 17, 39, 3173, 2, 100, 2397, 172, 192, 1012, 1258, 1563], [1, 5, 1, 1, 39, 1094, 131, 181, 298, 28520, 455, 6, 265, 22713, 1119, 1170, 578, 48], [411, 1267, 212, 39, 9, 1455, 1403, 467, 175, 2417, 148, 8, 1139], [13, 1, 62, 2448, 14, 3619, 10, 2297, 5, 362, 10, 123, 4902, 32236, 28, 187, 2079, 5307, 59, 123, 6255, 114, 432], [1, 775, 3087, 2, 339, 1022, 146, 10507, 213, 224, 781, 1586, 14, 403, 5, 10087, 768, 357], [13, 1, 142, 288, 1029, 73, 956, 513, 4327, 8, 4179, 1353, 16800, 46, 18, 1353, 1203, 602, 432, 165, 130, 3, 42, 15, 166, 314], [13, 1, 19133, 396, 30275, 93, 462, 216], [1, 724, 120, 1609, 309, 26, 28760, 7, 2207, 589, 53, 7, 698, 3713, 16, 22383, 11, 51, 269, 2, 103], [1, 1, 6996, 1, 2123, 1, 6997, 5, 1, 5716, 5, 1, 6998, 1], [1, 2163, 485, 172, 880, 124, 4, 53, 7, 900, 1319, 2, 394, 436, 6333, 1399, 23, 2, 323, 342, 2, 235, 1454], [274, 283, 26, 780, 85, 30, 24, 7, 4697, 427, 2361, 47, 20, 6, 228, 882, 2, 4044, 8908, 124, 7, 1461], [1, 70, 1181, 1056, 311, 1742, 1561, 1681], [1, 27060, 122, 151, 113, 2900, 5007, 27060, 18, 14, 27062, 8434, 223], [120, 394, 1141, 1, 88, 201, 625, 2, 6012, 2179, 17, 43, 9, 3051, 7374, 4, 156, 444, 2], [1, 252, 1766, 2437, 162, 1290, 7, 140, 738, 1945, 9, 158, 100, 530, 5181, 710, 118, 458], [13, 1, 62, 623, 1521, 1556, 985, 268, 173, 62, 42, 46, 309, 80, 101, 352, 492, 4939, 1356, 166, 20471, 75, 27], [60, 60, 5565, 1448, 47, 34], [12, 185, 67, 174, 3073, 90, 43, 480, 32, 654, 6, 119, 12, 1802, 278, 25, 108, 98, 4, 3445, 5, 98, 25], [4982, 4494, 22945, 4494, 5, 70, 81, 13199, 275, 15, 902, 193, 312], [467, 306, 1876, 827, 109, 68, 3522, 18, 11934, 114, 6481, 148, 15, 142], [1, 1, 54, 150, 687, 24, 861, 6, 128, 45, 7, 839, 212, 655, 4824, 34, 857, 8402, 1127, 4, 4656, 2536, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50], [1, 1199, 238, 24, 53, 11, 533, 271, 14200, 6, 53, 12, 90, 60, 864, 138, 53, 11, 1745, 167], [820, 10, 33, 193, 340, 46, 802, 27], [12, 11040, 15, 4820, 42, 33, 216, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 4982, 5], [120, 125, 206, 11, 6146, 361, 3908, 26, 310, 12, 3690, 6602, 1265, 129, 2, 20, 228, 125, 206, 2, 8762, 29, 56], [1, 69, 20, 109, 710, 26, 20, 633, 4, 1751, 8670, 210, 64, 140, 191, 1616, 274, 19156, 1359, 17, 6, 48], [1, 1, 32, 389, 5, 16, 74, 7, 6, 9, 4435, 64], [1, 1607, 49, 49, 49, 49, 52, 1219, 3491, 1286, 54, 3309, 4, 220, 400, 3163, 1224, 3139], [40, 8086, 552, 186, 26837, 938, 1148, 2466, 1039, 26838, 298, 4406, 236, 26839, 552, 904, 199, 9211, 45, 1471, 128, 885, 7], [275, 243, 142, 1001, 1, 35, 146, 5302, 3731, 2573, 8347, 80, 2388, 966, 1832, 1931], [13, 1, 59, 142, 15394, 28, 31, 679, 42, 15, 62, 55, 202, 414, 6, 4141, 15], [18, 238, 6, 527, 125, 206, 56, 25, 2580], [13, 1, 1, 1, 1, 32, 346, 64, 122, 5, 547, 11, 1459, 9, 19139, 12246, 47, 100, 290, 82, 12], [344, 5, 2442, 47, 336, 989, 10009, 19, 9525, 6, 15897, 28, 549, 10010, 10011, 2222, 1563, 95, 31265], [623, 706, 29, 14, 132, 326, 40, 756, 7388, 199, 2916, 89, 5202, 5823, 802, 27, 907, 27, 424, 27, 258, 198, 27, 8, 1019, 27, 8, 1565, 909, 198, 27, 28856, 802, 27, 907, 27, 424, 27, 27, 8, 1019, 27, 8, 1565, 301, 1147, 27, 28856, 909, 198, 909, 198, 27, 28856, 424, 27, 27, 8, 1019, 27, 8, 1565, 907, 27, 802, 27, 909, 198, 27, 28856, 802, 27, 907, 27, 424, 27, 27, 8, 1019, 27, 8, 1565, 301, 1147, 27, 28856, 909, 198], [1, 5, 751, 865, 1780, 11916, 64, 400, 100, 943, 7, 6138, 1021, 2846, 395, 129, 73, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [378, 5189, 4174, 4015, 902, 8715, 373, 281], [1, 1, 95, 177, 4, 1079, 7, 2934, 51, 30, 11, 3, 11906, 113, 100, 309, 3648, 12, 78, 57, 177, 11, 4885, 2], [1, 12691, 1, 165, 130, 3, 165, 130, 3, 165, 130, 3, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 2544, 10, 93, 26754, 14, 8024, 1889, 380, 3394, 618, 19, 108, 2, 5, 16], [1, 1, 1047, 1079, 8712, 427, 1750, 9, 381, 3, 8, 37, 50, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 1, 1177, 1104, 24, 34, 12, 436, 140, 2428, 539, 278, 391, 100, 698, 883, 28288, 354, 12, 1149], [1, 1, 114, 43, 1400, 5, 8374, 5, 38, 183, 20985, 2, 711, 2], [1, 372, 112, 18, 7705, 112, 1925, 74, 31, 109, 32, 3081, 2496, 7, 119], [5117, 1806, 1477, 12, 90, 204, 6311, 12, 305, 1, 7, 59, 14, 66, 268, 55, 14], [3950, 5, 18661, 1305, 7, 11028, 182, 5132, 60, 5454, 2, 122, 1234, 17, 18662, 538, 164, 7, 11029], [1, 78, 1, 1, 12, 157, 10651, 23, 5985, 19, 8298, 4, 622, 5610, 17, 2626, 131, 19, 2052], [1, 1, 21236, 12341, 760, 185, 45, 1416, 23, 4491, 589, 39, 2115, 302, 136, 40, 121, 1133, 19], [1, 1, 470, 9445, 9, 13151, 4784, 28357, 4678, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 19830, 610], [13, 1, 486, 2634, 902, 10, 1266, 223, 101, 132, 2634, 1090, 513, 1705, 6, 3937, 28, 2732, 464, 46], [13, 1, 1, 102, 102, 70, 81, 5834, 69, 465, 4251, 17, 65, 1176, 3575, 6, 1581, 39, 3518, 25, 44, 369, 49, 27244], [1, 964, 483, 3266, 11, 553, 95, 3254, 67, 395, 87, 48, 316, 2694, 104, 2, 26, 6306, 6, 736], [57, 6, 323, 20, 1509, 131, 670, 43, 38, 2, 450, 85, 17, 1362, 422, 124, 7, 112, 16, 180, 2821, 1315, 54, 43, 7, 131], [1, 1, 1, 103, 128, 7, 5141, 182, 118, 860, 19, 9, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 76, 61], [1, 582, 619, 76, 61, 5791, 1320, 13281, 8122, 1900, 275, 16, 469, 381, 3, 8, 37, 50], [1, 1, 563, 56, 7923, 18, 93, 175, 614, 339, 312, 59, 184, 1716, 6469, 10, 14, 451, 58], [1, 1727, 5220, 16, 131, 604, 4, 530, 32, 425, 530, 17, 1978, 87, 2], [43, 12, 5, 2056, 3252, 89, 57, 23, 155, 437, 20, 4, 150, 437, 2, 4, 150, 20, 2, 12, 247], [1, 1, 201, 716, 480, 32, 95, 2720, 7, 1522, 51, 43, 1259, 38, 7691, 4, 2, 20, 156, 1875], [1, 40, 213, 33, 14166, 86, 20, 6, 410, 88, 3523, 84, 27698, 1620, 365, 18, 501, 188, 94, 11, 11723, 8856, 298, 38], [1106, 17, 1201, 48, 153, 26, 584, 23, 34, 25, 195, 90, 80, 5, 2426, 35, 1, 75, 27, 75, 27, 200, 160, 15, 8, 52], [2820, 144, 15, 134, 200, 29, 5771, 462, 1068, 28, 31492, 409, 37, 3, 8, 5, 198, 981, 73, 15, 149, 2533, 6], [1, 3628, 69, 1, 62, 99, 49, 173, 7860, 139, 15, 819, 178, 205, 46, 1969, 6075], [3938, 514, 1, 18, 494, 73, 116, 30, 960, 93, 10478, 415, 635, 1086, 2858], [1, 83, 244, 115, 3336, 24, 26, 683, 23, 62, 59, 123, 1842, 10, 1376], [1, 1, 6335, 4480, 254, 4411, 1258, 586, 23, 786, 587, 1619, 23, 786, 587, 2322, 775, 2, 510, 211, 29, 68, 520, 418, 148], [1, 40, 7527, 2733, 12, 4142, 1685, 17, 34, 98, 161, 390, 293, 4, 4101, 8443, 19, 1010], [1, 632, 363, 93, 305, 10, 14, 1378, 4667, 664, 15, 8632, 6, 117, 587, 611, 73, 5731, 31672, 86, 52, 469, 8100], [13501, 6272, 5356, 131, 12364, 288, 29008, 2184], [1, 1, 1, 1, 293, 7, 3518, 118], [1, 20, 6, 465, 5405, 592, 13106, 4122, 32, 23, 446, 9015, 3344, 3415, 164, 293, 17, 443, 15745], [13, 1, 277, 4450, 2660, 3], [1, 10348, 69, 1617, 20, 3146, 20, 2876, 8973, 26734, 277, 2619, 487, 233, 188, 6, 6676, 39, 6863, 41], [106, 558, 4, 295], [1, 1, 219, 5960, 74, 6, 1269, 145, 153, 437, 1756], [727, 727, 11, 3128, 169, 2, 561, 458, 1091, 5359, 4, 7, 31905, 16, 1817, 840, 2, 5433, 7], [13, 1, 1172, 101, 18, 14, 66, 31, 995, 472, 68, 27482, 28, 2269, 4327, 8, 28, 149], [13, 1, 72, 33, 3142, 127, 28, 689, 6106, 14, 3525], [13, 1, 233, 3025, 26473, 464, 220, 219, 56, 11, 1324, 1620, 821, 973, 7724, 25, 9, 158], [4, 34, 110, 4863, 5951, 104, 25, 9, 43, 32312, 263, 262, 2, 17847, 5, 237, 16, 2747, 34, 4625, 480], [12988, 3330, 3529, 20863, 633], [13, 1, 609, 1611, 4446, 45, 159, 9084, 310, 4575, 7049, 310, 42], [1, 1, 1, 1, 1, 1, 1, 1], [214, 49, 52, 200, 114, 52, 200, 55, 15, 1654, 623, 154, 581, 723, 1800, 1, 396], [1, 48, 1420, 16, 2893, 4, 735, 220, 256, 44, 18, 1119, 6167, 289, 1593, 4695, 16, 9456, 444, 48, 1321, 10957, 44, 7029, 95], [1, 1, 1, 331, 221, 6192, 1685, 3770, 242, 41, 577, 17, 683], [1, 48, 147, 4006, 7, 3415, 6, 595, 44, 191, 1, 545, 38, 1169, 210, 2, 1809, 128], [13, 1, 1357, 29, 1211, 4014, 148, 1342, 528, 270, 249, 255, 137, 477, 6, 72, 1266, 2180], [1, 1, 1, 102, 52, 421, 499, 58, 148, 5, 237, 20, 40, 841, 1251, 17, 879, 90, 76, 61, 83], [13, 1, 1379, 49, 448, 909, 198], [1, 1, 20, 9524, 40, 2957, 1218, 12, 1109, 2, 20], [1, 1, 1886, 10872, 26, 12, 3305, 4, 5, 1179, 4, 2600, 7, 5, 463, 19, 365, 26, 248, 23, 41], [13, 1, 46, 300, 6864, 190, 147, 185, 3699, 36, 1, 3, 3008, 231, 985, 560, 5978, 3, 3699, 318, 173, 858], [266, 7, 1005, 17, 5, 186, 4, 5183, 18677, 517, 1215, 6061, 2100, 131, 12, 13265, 3155, 25, 369, 670, 131, 3209, 1295, 1295, 1295, 1295], [1, 1, 696, 5, 524, 15, 55, 93, 2131, 6, 1233, 415, 6434, 12, 2202, 12, 77, 26, 28, 5334, 77, 26], [1, 26846, 80, 101, 18, 3353, 3361, 266, 89, 23, 9, 51, 104, 1313, 8814, 12, 2636, 17, 471, 6, 870], [13, 1, 29, 656, 746, 11600, 93, 315, 1573, 86, 988, 1021, 70, 1383, 18, 1139, 73], [13, 1, 58, 224, 7344, 33, 27, 4366, 154, 878, 6295, 29, 1387, 29, 656, 9092, 1173, 94, 127, 177, 7, 192], [1, 34, 417, 17, 217, 4, 9021, 4, 6097, 12338, 2435, 17, 2829, 30, 24, 5, 463, 97], [1, 11, 631, 5, 1078, 16, 7209, 16, 5250, 17, 670, 16, 1741], [1, 290, 57, 359, 30, 2096, 4, 2096, 4, 1640, 186, 235, 387, 553, 30, 442, 113, 232, 71, 1546], [137, 146, 1376, 168, 123, 116, 35, 296, 412, 296, 475, 296, 1530, 146, 106], [13, 1, 1, 1, 1, 1023, 88, 57, 1630, 1350, 435, 323, 65, 4568, 2, 4, 34, 43, 88, 6560, 9], [1, 291, 1650, 18, 52, 46, 3136, 18, 52, 453, 376, 1233, 58, 5023, 309, 3847, 528, 1197, 249, 255], [13, 1, 4, 1495, 94, 8809, 1383, 6, 117, 518, 117, 1480, 137, 419, 117, 2090, 28, 137, 419, 117, 494, 3019, 826, 694, 889, 281, 6, 1357], [1, 14193, 1, 96, 6, 1082, 19, 108, 4, 1592, 9, 126, 616, 5564, 26, 1774, 119, 5276, 32, 126, 711, 601, 9, 343, 3, 8, 260], [3919, 5, 5, 8587, 5, 20921, 5, 1221, 5, 135], [13, 1, 1826, 878, 15, 142, 99, 804, 179, 11001, 8], [1, 1, 1, 35, 145, 235, 2416, 40, 1057, 64, 6, 34, 85, 1422, 968, 295, 244, 1461], [13, 1, 4079, 1, 24, 211, 29, 68, 102, 2118, 28, 985, 4118, 62, 55, 59, 2556, 6, 123, 3043], [70, 81, 6, 1, 488, 6, 5768, 276, 419, 16, 401, 272, 306, 33, 403, 688], [5, 1, 5, 341, 11021, 29, 596, 168, 596, 7801, 24878, 8587, 13490, 28919, 2041], [6127, 200, 22568, 6456, 200, 160, 596, 58, 149, 755, 6, 72, 1724, 10], [243, 15, 55, 31, 5247, 720, 9813, 18, 8, 15, 190, 1209, 3], [8700, 397, 7088, 14, 1820, 280, 421], [1, 5, 3285, 97, 815, 5715, 152, 23, 9, 3285, 5715, 400, 17, 14345, 124, 4, 4, 969, 45, 97], [1, 1, 1, 1, 1, 93, 6, 4491, 1991, 1821, 1071, 5, 66, 10], [1, 1, 1, 1023, 1, 1, 211, 35, 818, 1070, 26393, 40, 76, 61, 83, 6251, 2321, 76, 61, 76, 61], [1, 1, 26, 425, 26859, 15228, 8531, 89, 43, 770, 11, 23, 692, 337, 3500, 2, 18], [1, 321, 34, 7, 34, 7, 450, 23, 13705, 34, 57, 5514, 2186, 6, 106, 4, 4453, 57], [2877, 14983, 35, 759, 146, 18, 7510, 67, 34, 2370, 51, 104, 241, 6, 299, 54, 2927], [13, 1, 5, 25705, 4, 465, 16, 163, 16, 5, 237, 17, 5, 3871, 5, 203, 903], [1, 20857, 1, 26, 48, 26, 9, 3673, 26, 887, 124, 12, 157, 876, 9, 6261, 229, 1916, 23, 39, 1158], [13, 1, 14979, 1459, 6, 467, 1, 18, 146, 531, 86, 488, 1445, 73, 85, 18, 1984, 866, 86, 2284, 18, 73, 116, 30], [1, 285, 854, 20, 23, 227, 9, 2, 464, 614, 45, 887, 95, 2325, 12, 169, 2], [1, 3040, 24, 214, 2178, 793, 31, 5, 144, 218, 15, 149, 3391, 14, 786, 200, 6623, 56], [13, 1, 5, 80, 1052, 137, 1224, 18, 15, 149, 202, 4746, 33, 531, 75, 27, 214, 202, 246, 6, 251, 46, 37, 3, 8, 37, 50, 28959, 9753, 28960, 28961], [1, 1, 133, 127, 1226, 133, 3925, 7, 5, 4738, 452, 1226, 135], [1, 1128, 182, 2858, 77, 9, 138, 133, 9211, 4, 1909, 7, 20, 6, 1321, 11, 183, 2, 18, 540, 11, 990, 242, 25, 540], [30, 24, 7, 10670, 1302, 1004, 1355, 12, 157, 59, 14, 66, 1341, 1694, 5], [1, 127, 177, 4, 1797, 2204, 3052, 11, 3868, 21629, 40, 53, 1038, 40, 20, 6436, 497], [1, 1, 1, 751, 40, 735, 399, 2014, 2, 52, 37, 3, 8, 27, 50], [1, 2661, 301, 301, 301, 301, 301, 54, 2, 44, 24, 4, 377, 7, 787, 12, 167, 3241, 13446, 9, 258, 198, 258, 198, 258, 198, 26, 145, 13446, 9, 6, 377, 290, 1075], [1, 99, 5, 2599, 1541, 389, 996, 86, 18, 93, 955, 119, 12, 119, 121, 40, 2023, 11], [42, 18, 2996], [1, 5113, 15, 686, 31, 273, 68, 8290, 18, 146, 18003, 2025, 30850, 62, 55, 246, 10, 15, 499, 58, 148], [140, 1, 32, 1506, 5, 237, 16, 53, 7, 727, 124, 7, 18824, 11076, 254, 186, 6, 120, 20, 6250], [13, 1, 452, 73, 5019, 18, 93, 679, 600, 4140, 894, 5, 4733, 224, 1402, 1300, 11068, 158, 7262], [14974, 3909, 11, 6347, 2, 20, 1501, 17, 4050, 461, 26, 2774, 20, 2922, 4, 2903, 65, 41, 14, 471], [13, 1, 1, 338, 9020, 9020, 17947, 173, 37, 3, 8, 37, 50, 37, 3, 8, 37, 50, 25, 1844, 19521, 1296, 24, 37, 3, 8, 37, 50, 1888, 3, 8, 37, 50, 1888, 3, 8, 37, 50, 1888, 3, 8, 37, 50, 1888, 3, 8, 37, 50, 1888, 3, 8, 37, 50], [48, 39, 1166, 242, 472, 7, 114, 9, 737, 2243, 38, 4125, 939, 344, 1329, 26, 990, 317, 359, 197, 1329], [13, 1, 1, 1, 5, 1393, 871, 883, 65, 7560, 47, 1631, 65, 23, 7560, 47, 871, 124, 11, 192, 2720, 19620, 19, 44, 19], [1, 20, 285, 4789, 26786, 259, 26787, 108, 64, 5, 3094, 136, 8058, 67, 3, 8, 22, 10, 21, 26788, 108, 96, 9], [1, 1, 20, 7012, 26, 23, 1747, 19, 416, 14, 140, 34, 164, 1038, 22176, 7, 4313, 16, 44, 11283], [1, 2247, 600, 109, 315, 1469, 6, 626, 1650, 18, 236, 1071, 5, 6607, 137, 10246, 18, 46, 647, 1040], [13, 1, 1, 22463, 2102, 69, 1896, 1222, 6], [1, 1, 1232, 30, 24, 7, 8757, 182, 118, 63, 574, 197, 1109, 25, 19, 911, 5862, 229, 4, 63, 358], [1, 1, 1, 3040, 1, 1, 1044, 1, 1, 5, 153, 6, 7303, 984, 9], [1, 1, 26, 944, 266, 1503, 426, 128, 7, 10449, 9, 19579, 385, 266, 1503, 426, 5197, 19580, 47, 20], [424, 27, 52, 189, 33, 3810, 338, 1, 24, 424, 27, 301, 384, 160, 16, 166, 388, 918, 474, 11, 850, 919, 920], [289, 49, 12666, 2820, 488, 49, 4274, 99, 80, 985, 289, 5940, 28, 4100, 6, 1258, 12762], [63, 19, 6, 27171, 89, 199, 1880, 354, 4, 791, 721, 1230, 2, 75, 27, 3, 171, 141, 42, 15, 33, 8337, 135], [1, 274, 374, 7094, 522, 9, 259, 374, 7094, 4798, 2, 85, 818, 39, 1600, 433, 522, 16, 9, 697, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [5, 570, 5], [13, 1, 276, 577, 1780, 118, 43, 112, 1364, 113, 5518], [13, 1, 1, 2149, 733, 2436, 12, 1117, 89, 126, 4415, 38, 12, 2, 26, 48, 71, 95, 5900, 7, 5, 1904, 17, 463], [13, 1, 1, 8164, 5, 1, 1, 1, 8164, 69, 173, 41, 1749, 111, 16, 2283, 24, 12, 783], [233, 1554, 995, 281, 59, 15], [1, 517, 1964, 7, 9183, 316, 406, 129, 2, 128, 32, 711, 633, 87, 4, 594, 413, 48, 56, 11, 533, 271, 2, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 2662, 9, 71, 74, 4, 24317, 11, 5914, 64, 26, 96, 770, 12, 8356, 95, 1046, 25, 1604], [1, 5, 3262, 5394, 18, 291, 2106, 29, 2568, 40, 352, 582, 3640, 483, 144, 861, 49, 1190], [1, 3156, 1676, 1676, 1577, 1479, 28, 146, 1051], [958, 15, 102, 70, 81, 855, 1, 24, 159, 15, 99, 329, 1577, 524, 1786, 28, 499, 9679, 14, 1597], [1, 1, 105, 57, 150, 6, 2, 57, 6, 63, 3175, 19, 23, 19, 212, 96, 119, 685], [623, 706, 1429, 4021, 58, 83, 360], [1, 311, 2596, 40, 20, 2693, 40, 715, 442, 297, 32, 16, 1636, 6432, 6, 20, 5, 463, 5454], [1078, 638, 7, 85, 97, 5030, 1486, 1, 1, 1, 23, 390, 183, 78, 85, 12, 25], [1, 5031, 30, 24, 7, 156, 4, 420, 7751, 110, 89, 23, 5056, 11, 3294, 17, 230, 7], [1, 18, 223, 11, 692, 1602, 27119, 64, 42, 46, 223], [13, 1, 211, 6, 1589, 68, 472, 1, 513, 6566, 6, 1422, 14, 804, 144], [1, 3460, 1644, 1, 1072, 168, 222, 3265, 30, 413, 1511, 1735, 18], [1, 12126, 79, 36, 14, 92, 91, 86, 202, 460, 2619, 1851, 518, 2180, 29, 58], [1, 1568, 3, 1568, 3, 6517, 1501, 618, 19, 4296, 18, 15073, 2557, 3], [13, 1, 1, 1, 1, 1, 42, 370, 4379, 29817, 338], [13, 1, 5, 42, 33, 2083, 28, 324, 29, 314, 3492, 236, 246, 10, 276, 3491, 193, 18, 306, 6589, 1712, 46], [13, 1, 52, 189, 2293, 18987, 1, 24, 10377, 160, 16, 18988, 510, 474, 18989], [35, 34, 4, 3179, 40, 18, 206, 16, 54, 206, 1207, 322, 40, 9164], [4971, 302, 19749, 39, 3075, 9859, 9, 1693, 140, 7, 19, 6, 1891, 9, 4250, 7], [304, 140, 23935, 169, 1032, 6, 74, 3982, 316, 151, 2454, 34, 1032, 93, 3238, 782, 2110, 3, 8, 22, 10, 21, 799, 3], [712, 511, 49, 781, 224, 5442, 8536, 29, 362, 8537, 55, 781, 6, 567, 14, 223, 567, 58, 357], [281, 19, 194, 303, 1, 24, 53, 4, 1287, 509, 34, 26, 2161, 1287, 2, 4, 34, 4239, 507, 1686], [13, 1, 16, 12159, 33, 42, 29, 33, 2592, 28, 268, 719, 1855, 29, 415, 33, 2592], [1, 1, 1332, 1, 20, 8488, 11, 5, 1158, 16, 436, 422, 2135, 7, 2286, 16, 25, 483, 41, 1972, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 45], [29807, 18, 38, 7995, 29808, 4, 5, 2972, 23, 5483, 16, 7795, 1114, 16, 963, 29809, 969, 2, 71, 65, 23, 3818], [1, 1, 236, 18, 224, 2459, 28, 27402, 144, 2197, 2913, 674, 9, 24900, 6787, 2, 2888, 15437, 6787, 2], [424, 27, 52, 189, 33, 482, 482, 338, 1, 24, 424, 27, 3498, 2124, 384, 160, 16, 33, 353, 166, 388, 918, 736], [13, 1, 5537, 5538, 1, 52, 2742, 562, 160, 117, 28, 438, 117, 700, 5, 118, 166, 510, 166, 388, 977], [214, 49, 70, 29, 2091, 462, 781, 746, 10, 11600, 6288, 17134, 28, 118, 45, 1855, 4170, 531], [622, 5, 718, 78, 85, 676, 801, 104, 594, 413, 48, 43, 11, 676, 9, 2, 5, 89, 122, 5428, 135], [1, 14, 132, 10, 33, 1573, 1222, 4623, 2669, 15, 224, 1239, 1593, 42, 624, 1378, 1597, 18, 706], [6009, 7, 2468, 178, 589, 3199, 377, 19, 697, 4316, 6009, 6009, 491, 3804, 589, 87, 5, 286], [13, 1, 5, 5, 83, 6833, 83, 1670, 1, 37, 3, 8, 640, 256, 196, 1, 11256, 154, 46, 76, 61, 70, 5], [1, 218, 24, 362, 2732, 10, 5, 774, 499, 1006, 3285, 27119, 11, 4622, 9943, 169, 418, 148, 270, 249, 255], [1, 11289, 15, 317, 52, 376, 1360, 306, 58, 629, 200, 160, 15, 1407, 1126, 37, 3, 79, 36, 14, 92, 91, 8122], [1, 280, 459, 31, 2916, 5267, 1161, 705, 2, 2414, 45, 18143], [1, 20, 27612, 7, 32, 4, 4229, 126, 7731, 95, 9692, 5264, 43, 17, 2651, 11, 11], [139, 49, 52, 49, 28, 49, 70, 28, 713, 619, 45, 2339, 493, 5856, 6, 15, 14, 823, 7461, 5174], [1, 39, 25, 19, 298, 1299, 7, 393, 4800, 210], [1, 1, 54, 150, 35, 34, 308, 2187, 354, 5410, 38, 95, 1115, 82, 7183, 15, 55, 432, 2105, 29, 59, 10, 117], [1, 1, 550, 1188, 197, 164, 417, 537, 21421, 717, 161, 1678, 1857, 1117, 1618, 625, 182, 6005], [1, 38, 1518, 2718, 6871, 5896, 782, 1140, 47, 71, 96, 794, 3784, 4545, 121, 26, 12573, 531], [1, 414, 1159, 1623, 121, 4191, 1145, 193, 1575, 1873, 1065, 2, 212, 2170, 9, 5183, 8625, 1870, 186, 40, 4], [13, 1, 2728, 10878, 29, 2818, 248, 3856, 3175, 2312, 291, 1152, 4030, 32310, 6, 17272, 291, 20380], [13, 1, 998, 489, 5209, 3497, 10433, 396, 2898, 1050, 7708, 116, 30, 10434, 6282, 31, 5, 1, 5015], [1, 1, 175, 484, 59, 14, 4763, 59, 14, 184, 1037], [1, 30291, 359, 12180, 406, 129, 47, 4, 5, 1615, 7, 6236, 2536, 11, 308, 337, 30919, 64, 381, 3, 8, 37, 50, 176, 3, 8, 327, 50], [1, 1, 1, 1, 814, 35, 1896, 183, 67, 3994, 19, 4539, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 35, 908, 497, 7, 24237, 186, 2, 4357, 12, 10070, 1364, 41, 1798, 67, 26, 2220, 65, 5, 933, 11, 5, 135], [20, 2172, 38, 11075, 9, 2, 11075, 9, 2, 358, 2248, 2, 616, 11075, 4, 227, 9, 11075, 490, 39], [288, 3405, 28, 55, 1190, 272, 375, 266, 170, 978], [1, 1, 1, 11233, 119, 65, 163, 15798, 12, 332, 25, 83, 303, 115, 12, 8846, 1294], [1, 19530, 1, 363, 488, 327, 327, 305, 10, 1, 510, 202, 393, 14, 170, 10, 656, 510], [1, 1, 1, 1552, 6, 1492, 25, 220, 1406, 9805, 1819, 9, 220, 53, 4, 230, 7, 4, 12401], [1, 8318, 1794, 8619, 86, 4918, 299, 3140, 4, 455, 12, 5, 124, 32, 6296, 5833, 2349, 5, 5, 31073, 4, 928, 12], [1, 192, 183, 589, 2099, 251, 4687, 11, 672, 44, 19, 6, 5, 54, 4067, 161, 78, 1386], [9984, 4, 2, 20743, 4, 4, 1998, 4, 8424, 20, 2, 32245, 4, 1917, 4, 14535, 4, 1223, 4, 4956, 4, 96], [2344, 2798, 2953, 9453, 8710, 5361, 2684, 2953, 136, 12, 5063, 17, 4805, 1038, 3313, 3313, 12, 5075, 21373, 2062, 19, 631, 13313], [13, 1, 3156, 2221, 1310, 2331, 234, 15, 317, 845, 218, 28, 2091, 146, 619, 3235, 134, 15, 29, 68, 1165], [70, 81, 855, 301, 301, 301, 301, 70, 329, 329, 1696, 10, 14, 132, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 1], [1, 63, 65, 9183, 646, 20, 155, 107, 2550, 238, 5, 947, 6426, 43, 140, 292, 259, 104, 63, 23501], [200, 160, 15, 7156, 76, 61], [1344, 5652, 29, 1, 656, 372, 4834, 231, 15797, 656, 372, 4303, 231, 42, 16, 114, 15, 21516], [1, 1, 1, 523, 450, 12, 436, 9, 4324, 86, 1204, 12, 436, 2505, 18, 759, 49, 1870, 643], [1, 1, 35, 62, 1790, 55, 10052, 6, 99, 3584, 24, 28, 15, 8, 371, 30, 24, 986, 2033, 56], [1, 1, 1, 25442, 1, 2661, 2210, 12, 1580, 2509, 621, 23, 1826, 1546, 1626], [1, 17105, 4480, 519, 9, 358, 77, 23, 9, 2, 63, 519, 23, 1043, 9, 19, 63, 60, 60, 8915], [1, 1, 1, 7358, 24, 85, 7, 722, 461, 138, 85, 347, 203, 43, 12, 498, 12], [143, 628, 1886, 30, 24, 12, 489, 3972, 16, 2821, 354, 12, 90], [1, 2269, 1148, 1088, 19, 1461, 64, 162, 90, 8485, 9, 3994, 17612, 3552, 4231, 3001, 10, 22335], [1, 4638, 2041, 1833, 5, 16, 2359, 570, 1961, 62, 379, 18347, 300, 4774, 5301, 5, 7162], [1, 108, 254, 3466, 12543, 3427, 254], [1, 868, 30, 24, 7, 818, 957, 118, 30, 24, 65, 377, 7, 310, 16849, 12, 174], [1, 316, 2350, 77, 1896, 1452, 131, 2, 1761, 681, 1896, 7334, 4, 1181, 221, 67, 721, 597, 2, 45, 6510, 23], [1, 2346, 33, 304, 116, 30, 24, 34, 17, 822, 162, 226, 204, 53, 4, 4381, 188, 2540, 5201], [52, 189, 1], [13, 1, 1579, 3603, 31, 14, 6873, 2185, 66, 29, 1, 2019, 35, 211, 1, 35, 1, 4275, 35, 2497, 25892, 3378, 3378], [1, 1, 85, 516, 665, 12, 56, 7, 366, 700, 297, 87, 120, 20, 38, 158, 71, 1416], [1, 1, 1, 31573, 7, 579, 17, 5662, 221, 95, 6683, 1265, 4797, 1522, 109], [1, 1, 1, 1219, 99, 101, 18, 224, 56, 114, 46, 1042, 226, 226, 226], [1, 1, 1, 1, 1, 1, 1, 1], [1, 18897, 5, 12, 39, 1224, 9, 3991, 3887, 10131, 18, 6623, 17, 5, 16, 30, 24, 23, 183, 9], [347, 7, 364, 47, 122, 6, 125, 206, 527, 1717, 94], [1, 28274, 11, 3728, 1052, 9, 313, 1007, 659, 741, 2, 71, 1052, 313, 2, 5, 2779, 5, 29605], [1, 39, 12869, 32423, 1215, 7, 77, 405, 9, 4373, 45, 44, 663, 1215, 7, 39, 1934, 7539, 2, 16042, 9], [1, 1, 59, 56, 16, 5010, 6099, 390, 970, 17, 2132, 5, 7, 690, 390, 321, 350, 7, 1160, 292, 1070, 2], [511, 4242, 891, 1, 10231, 5378, 24656, 18585, 44, 5378, 231, 3707, 108, 4434, 6904], [9209, 24883, 5, 24883, 4815, 8587, 9209], [13, 1, 289, 2670, 1106, 15352, 96, 15353, 228, 1384, 64, 37, 3, 8, 27, 50, 3, 8, 22, 10, 21], [33, 1407, 31, 1818, 12159, 16, 268, 18, 175, 14, 5, 125, 206, 40, 1031, 5, 89, 1897, 18, 74], [389, 5707, 6930, 5, 20664, 80, 378], [1, 20, 53, 12, 45, 6751, 2, 20, 6009, 12, 1515, 23, 2, 45, 1685, 17, 23], [1, 1, 9245, 6, 9, 1128, 104, 1198, 711, 4113, 4113, 12, 155, 174, 4, 1751, 197, 164, 30848], [1, 649, 5593, 204, 22477, 16, 356, 1835, 16, 377, 411, 124, 157, 280, 6, 154, 15, 31], [53, 16, 5, 3708, 242, 1, 47, 38, 98, 7, 128, 1832, 17, 207, 9, 3733, 279, 7, 6, 497, 32, 3163], [13, 1, 15, 55, 966, 414, 3800, 6090, 28, 518, 42], [1, 1806, 9, 2184, 4531, 6, 1871, 193, 24, 269, 415, 25, 65, 2316, 19, 262, 47], [1, 404, 6, 740, 25, 2694, 1250, 3163, 11606, 285, 6849, 19, 54, 6009, 7], [13, 1, 18, 20767, 1078, 2390, 1269, 5286, 113, 1, 631, 2930, 454], [83, 360, 53, 417, 26007, 53, 614, 183, 538, 480, 53, 2746, 103, 1638, 12906, 4236, 480, 7, 244, 8846], [321, 5380, 152, 1064, 51, 23, 12475, 182, 1065, 9, 6, 403, 750, 17, 362, 3551, 9, 15029, 6, 17, 362], [349, 5871, 17, 1134, 2873, 5, 5681, 158, 4444, 760, 29877, 1116, 760, 760, 2153, 310], [1, 74, 4, 776, 221, 2987, 1177, 147, 7, 23, 1308, 4279, 1047, 710, 147, 599, 1038, 221, 321], [1, 66, 10, 469, 6, 15, 28, 29, 607, 5, 524, 10, 296, 412, 296, 475, 296, 1530], [2641, 40, 1768, 6, 154, 137, 187, 99, 36, 415, 45, 40, 18, 175, 2512, 6, 1614, 1067, 32235], [2886, 69, 78, 346, 2, 16, 5189, 16, 26998, 158, 217, 69, 3, 8, 79, 50, 2886, 55, 865, 278, 95, 6683, 16, 2350, 2, 16], [440, 440, 440, 53, 12, 959, 26, 1751, 197, 164, 25383, 8813, 2392, 208, 34, 23, 1048, 5635], [1, 3032, 7786, 6275, 24, 394, 489, 4635, 238, 122, 95, 235, 370, 6338, 2], [13, 1, 304, 35, 15863, 18, 14, 28170, 10, 2772, 819, 5824, 2836, 4777, 139, 103], [627, 750, 394, 47, 5, 32552, 75, 27, 2196, 2010, 75, 27, 5, 14], [804, 2982, 28, 2820, 73, 14, 170, 15, 149, 99, 179, 1691, 1564, 62, 55, 759, 29, 123, 607, 31], [1, 25, 62, 99, 6, 15, 6223, 1258, 252, 853, 2465, 170, 4908], [1, 1, 54, 158, 1432, 3, 8, 22, 10, 21, 79, 36, 14, 92, 91, 1975, 10607, 4, 10220, 67, 1092, 9, 263, 129, 54, 3, 8, 22, 10, 21, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91], [1, 1, 4454, 6, 147, 110, 222, 2359, 113, 20, 332, 16, 359, 1945, 51, 1783, 2, 145, 425, 98, 227, 610], [13, 1, 1622, 5, 39, 9, 2461, 1093, 100, 683, 25, 103, 128, 4, 2780, 193, 89, 22784, 2, 978], [1, 1, 131, 2769, 8818, 6442, 1689, 1929, 4, 2769], [1, 1492, 229, 215, 20, 888, 111, 18, 2428, 215, 10326, 9, 2535, 866, 7, 1108, 941, 229, 7337, 157], [1, 1, 1, 105, 105, 105, 66, 112, 513, 268, 329, 1542, 31, 46, 8066, 286, 216, 250, 314], [13, 1, 1162, 8653, 18, 414, 1162, 179, 892, 18, 59, 306, 62, 55, 1237, 8653, 5, 214, 5, 116, 5099, 5, 116, 5, 4733], [1, 429, 6045, 69, 38, 23, 69, 140, 377, 465, 25, 316, 247, 2298, 2655, 750, 65, 542, 69, 25, 5682], [1, 1, 1, 374, 19, 416, 2, 86, 770, 556, 770, 18, 14, 4483, 10, 43, 76, 61, 76, 61, 76, 61, 76, 61], [30, 16, 5, 271, 519, 4920, 8117, 4475, 1728, 368, 98, 116, 4920, 6, 4920, 7, 25, 16539, 44], [1, 63, 65, 2694, 3882, 9, 19, 1426, 230, 1426, 111, 7, 8757, 386, 19, 140, 342, 2, 111, 4466, 158, 2], [1, 107, 3185, 2693, 10, 30219, 6119, 28, 1791, 28, 414, 20326, 111, 18, 73, 2308], [13, 1, 6, 3810, 354, 12, 412, 1967, 9478, 1606, 3257, 4104, 25015, 114, 313, 2511, 42, 75, 27, 313, 2, 703, 119, 1259], [1, 2599, 5, 516, 1848, 164, 71, 487, 1692, 19, 1646, 71, 6, 98, 7, 2735, 1085, 2, 1184, 2735, 39, 9, 4686], [448, 131, 16844, 1453, 3, 764, 6, 72, 992, 75, 27], [13, 1, 413, 5, 1, 649, 5593, 204, 22477, 16, 356, 1835, 16, 377, 411, 124, 157, 280, 6, 154, 15, 31, 12077, 135], [13, 1, 268, 879, 26725, 11393, 6163, 21522, 58, 18, 20406, 73, 2187, 473, 460, 6, 1855, 29, 46, 162], [1, 1, 2163, 147, 127, 177, 32, 777, 1896, 3296, 2, 4126, 9043, 1412, 4, 592, 3644, 1531], [13, 1, 424, 27, 52, 189, 33, 482, 482, 338, 1, 24, 424, 27, 258, 198, 384, 160, 16, 166, 388, 918, 474, 11, 850, 919, 920], [1, 1, 127, 177, 32, 616, 11076, 25, 573, 254, 2, 8757, 521, 127, 177, 11, 655, 41, 1550], [136, 473, 18, 210, 16968, 28833, 4791], [1, 411, 580, 498, 32, 18, 11991, 17, 1057, 2879, 740, 16, 230, 32, 1441, 18368, 87, 1098], [1, 14, 315, 716, 1172, 9832, 10444, 1356, 18, 1959, 611, 218, 521, 683, 23, 287, 203, 430, 104, 47], [1, 1, 1, 22575, 26, 126, 54, 121, 100, 87, 20368, 12, 1128, 17, 221, 6779, 591, 11, 44], [5, 3367, 190, 3624, 20585, 2260, 806, 7576, 12245, 8904, 18, 806, 15], [1, 720, 2171, 86, 15, 1553, 72, 166, 4666, 139, 273, 144, 15, 55, 123, 3355, 36, 530, 442, 29, 56, 31, 478, 2205], [1, 1, 39, 399, 9, 116, 30, 24, 161, 5309, 240, 3939, 17, 807, 18491, 12922, 145, 16, 4991], [1, 35, 12, 370, 2419, 253, 18971, 11, 3746, 12, 2406, 547, 119], [1, 25, 3177, 556, 2734, 15, 6, 3, 635, 462, 2570, 8, 117, 211, 29, 24402], [347, 203, 15742, 43, 7, 792, 1449, 117, 203, 43, 690, 19, 8052, 844, 107, 155, 690, 19, 6040, 45, 65, 475], [79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 618, 241, 2113, 15947, 836, 12, 119, 89, 29777, 185, 78, 618, 19, 108, 78, 618], [1, 127, 11, 3978, 133, 3092, 2136, 10798, 139, 5019, 730, 16, 1003], [1, 1, 1, 1, 221, 5466, 96, 1657, 547, 97, 444, 1069, 85, 4, 18510], [143, 35, 296, 157, 11, 475, 2186, 3294, 548, 86, 16, 4884, 3252, 11, 475, 440], [1, 1, 6360, 446, 57, 333, 4, 9, 186, 20, 12235, 4, 491, 38, 969, 1241], [15, 518, 16, 1465, 28990, 176, 3, 381, 3, 8, 37, 50, 37, 3, 8, 640, 5, 1095, 3776, 184, 8573, 18, 2801, 14394, 1190, 154, 1, 49, 1190], [1, 1, 1916, 609, 111, 152, 4331, 69, 11, 20, 7128, 9, 997, 1010, 71, 145, 23], [13, 1, 1, 566, 5, 4504, 4, 1118, 44, 84, 561, 65, 265, 4324, 75, 27], [3492, 188, 385, 7, 2515, 188, 262, 41, 912, 60, 6045, 5087, 7603, 6879, 12, 19, 57, 177, 4, 13846, 13907, 19, 29884], [13, 1, 7351, 121, 1299, 72, 72, 2720, 187, 26, 48, 26, 591, 1748, 266, 535, 1590, 1329, 245], [45, 275, 102, 28, 102, 70, 28, 713, 619, 45, 2339, 493, 6, 350, 565, 85, 676, 244], [97, 5329, 7, 924, 25, 9, 44, 8888, 569, 145, 153, 128, 11, 44, 19, 63, 501, 4, 208, 314], [1, 140, 1948, 5703, 26, 3254, 11, 4208, 44, 158, 6, 7736, 4, 152, 2851, 220, 25, 437, 18, 472, 12, 899], [13, 1, 523, 1, 155, 6670, 23, 393, 9, 259, 54, 20, 152, 645, 12, 9, 5049, 6048, 180, 9], [1, 1, 1044, 1, 1044, 467, 30, 24, 7, 34, 880, 3065, 1735, 17, 64, 328], [1, 1280, 2987, 53, 26810, 440, 261, 1842, 10, 1376, 4, 178, 2639, 2484, 63, 124, 32, 63, 880, 65, 1461], [13, 1, 8249, 1134, 80, 5, 16, 3476, 29620, 2, 1217, 65, 18981, 4, 208, 4805, 7329, 2, 3278, 4570, 4, 23, 727], [13, 1, 2898, 403, 492, 5, 4955, 17232, 464, 4139, 28, 32070, 168, 5481, 26638, 257, 340, 46, 612], [570, 15590], [1, 57, 702, 669, 62, 12612, 57, 702, 669, 66, 508, 1921, 1488, 1404, 3, 1404, 3, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [13, 1, 414, 58, 18, 45, 59, 15, 1, 73, 2448, 29, 622, 10, 14, 1, 7707, 17615, 73, 1, 8420], [1, 1120, 1, 3676, 7, 240, 25329, 164, 1038, 1500, 11576, 4899, 3161, 164, 26, 44], [1, 5, 22254, 6, 9, 19, 37, 3, 8, 37, 50, 37, 3, 8, 37, 50, 5, 443, 150, 9, 1089, 19, 1292, 3, 5, 5917, 6, 800, 19, 613, 3, 613, 3, 5, 17889, 60, 19, 176, 3, 176, 3, 5], [1, 1, 67, 94, 4, 8455, 590, 3228, 31, 619], [1, 14444, 1, 404, 3749, 121, 4480, 1966, 239, 9, 1291, 3, 8, 22, 10, 21, 79, 36, 14, 92, 91], [1, 810, 55, 1445, 2985, 10691, 5596, 1699, 2591, 25, 9, 6, 269, 538, 11], [1, 194, 962, 12, 90, 60, 60, 264, 62, 379, 6, 154, 15, 31, 116, 30, 24, 170], [13, 1, 4137, 3499, 1173, 499, 36, 30842, 8, 13385, 59, 68, 193, 28, 145, 2290, 4, 917, 19, 525, 6], [655, 982, 9, 779, 6, 9914, 18970, 494, 86, 731, 3], [1, 86, 114, 15, 175, 6732, 31078, 2483, 178, 51], [13, 1, 211, 6, 59, 33, 785, 218, 10480, 29, 1792, 6290, 34, 4, 139, 30165, 849, 400, 746], [926, 16, 293, 207, 7640, 215, 293, 443, 2, 2592, 1174, 251], [1231, 1, 7512, 4954, 9, 3637, 6721, 476, 2, 1128, 5217, 156, 2, 3344, 9, 349, 6750], [1, 635, 10, 351, 16164, 94, 4, 6959, 72, 1330, 2], [142, 56, 8108, 1552, 17277, 31, 1584, 10, 3177, 2608, 611, 3177, 2608, 18, 218, 5807, 10, 1263, 31776], [1, 1, 2237, 3740, 143, 6, 6757, 6543, 35, 28, 3401, 1006, 6, 154, 14, 962, 10], [18, 546, 12, 157, 6181, 6, 2995, 10694, 12, 167, 25, 506], [69, 1884, 148, 56, 10243, 6, 2065, 125, 862, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 48, 156, 1795, 1721, 675, 3035, 14200], [70, 81, 31, 2006, 1, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 175, 5, 642, 1225, 536, 1458, 536, 1458, 536, 1458, 536, 1458, 311, 1140, 8012, 11604], [2816, 48, 39, 43, 709, 2804, 195, 584, 43, 4, 13192, 6, 794, 1591, 3507, 396, 14, 3421, 18, 14, 112, 6220, 36], [1, 80, 319, 5428, 800, 5428, 86, 15, 1277, 1018], [330, 12, 412, 9, 4, 1265, 1010, 2, 3714, 692, 443, 11522, 28524, 2, 4379, 7656, 4, 208], [1, 1, 630, 20, 48, 3621, 910, 242, 25, 12533, 4410, 85, 6, 41, 2794, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91], [1, 1, 551, 162, 1188, 4, 39, 8831, 9, 2, 2226], [1, 991, 14, 3671, 98, 11, 994, 17, 1057, 26, 3478, 4584, 7, 3391, 427, 1057, 37, 3, 8, 37, 50], [13, 1, 401, 8, 478, 5680, 132, 16263, 16264, 7645, 10, 193, 18, 1482, 166, 16265, 691, 16, 103, 9871, 3350], [13, 1, 46, 18, 137, 1101, 42, 14, 30325, 10, 575, 1, 1, 1, 1, 202, 16831], [1, 1, 52, 102, 102, 52, 485, 20, 8642, 3042, 8642, 111, 5, 237, 20, 163, 45, 163, 107], [1, 3112, 31, 7, 3076, 2, 2708, 13490, 9640, 905, 4, 389, 184, 1929, 17, 32569, 12, 38, 1349, 2949, 18, 226, 546, 104], [1, 1, 1843, 1293, 24, 2129, 24, 4942, 10898, 2129, 24, 3150, 427, 34, 204], [1, 1, 1, 1, 1, 1, 1, 1, 8160, 1, 29814, 1185, 679, 6926], [1, 80, 101, 3844, 315, 7383, 28, 274, 283, 18, 483, 2241, 59, 55, 845, 335, 483, 39, 5123, 9, 9730], [1, 70, 81, 1407, 1898, 534, 188, 3725, 345, 7077, 311, 4055, 637, 74], [13, 1, 8700, 1861, 3050, 2, 204, 168, 233, 1, 309, 137, 52, 1408, 8886, 679, 1805, 4422], [13, 1, 1, 1674, 239, 60, 601, 105, 1781], [13, 1, 33, 992, 175, 1168, 71, 17153, 33, 193, 149, 556, 72, 14, 786, 652], [1, 80, 3173, 69, 20, 100, 2054, 1149, 29939, 808, 16, 786], [13, 1, 619, 583, 3177, 1874, 268, 6, 6417, 11066, 6, 518, 68, 5578, 14, 5233, 10, 68, 2269, 17425, 995, 115], [1, 25453, 276, 643, 3, 8975, 32, 6029, 1003, 11, 1003, 7642], [1, 1, 170, 56, 11, 103, 57, 305, 145, 24674, 420, 69, 56, 527, 812, 1142, 639, 27, 9427], [13, 1, 8576, 8, 478, 5680, 132, 62, 55, 10, 193, 1042, 268, 62, 1459, 6, 2068, 373, 6622, 8, 2389, 6, 486, 28, 768], [1, 1049, 547, 30, 24, 26, 1293, 24, 12, 1150, 1901, 47, 5, 12, 167, 31, 32, 400, 10551, 7, 9, 604, 135], [1, 1, 1, 1, 609, 1, 35, 5, 892, 10, 59, 143, 35, 34, 57, 822, 135], [1, 1, 1, 120, 219, 111, 282, 161, 14376, 997, 254, 315, 43, 3879, 4, 150, 1102, 51], [13, 1, 1379, 1, 24, 336, 1267, 65], [1, 194, 156, 4, 60, 60, 35, 95, 71, 25669, 1365, 2, 13776, 120, 57, 5380, 25], [2, 20, 2, 20, 30889, 2143, 3, 487, 757, 23584, 645, 147, 309, 3847, 270, 249, 255, 199, 48, 308, 2, 1060, 3, 2299, 476, 20, 896, 195, 135], [1, 77, 1471, 127, 426, 9, 7813, 145, 6, 1138, 11, 16909, 207, 670, 302, 45, 20, 6165, 1788, 302, 127, 4, 399], [1, 1, 7134, 1, 1, 14364, 2769, 60, 5611, 2, 7134, 24, 11, 2491], [1, 125, 1268, 179, 15, 144, 1597, 52, 29, 1022, 20, 2, 815, 730, 68, 8600, 1991, 14044], [1, 1, 8941, 12, 4657, 8941, 3821, 369, 47], [13, 1, 1062, 812, 36, 1019, 77, 725, 779, 37, 3, 8, 27, 50, 12475, 950, 11, 1493, 278, 129, 2, 55, 15, 813, 29, 1885, 5, 14], [1, 339, 256, 59, 642, 389, 451, 500, 2689, 847, 86, 460, 62, 1583, 62, 29103, 500, 99], [1, 151, 17, 311, 217, 35, 336, 90, 2461, 53, 9215, 10458, 16, 2, 59, 14, 102, 66, 35], [5951, 34, 1866, 3812, 245, 5116, 47, 1925, 248, 29309, 5587, 1610, 11367, 282, 145], [13, 1, 18, 345, 114, 5, 5, 12, 67, 387, 6097, 51, 1591, 96], [1, 274, 24, 34, 692, 5358, 19, 692, 3769, 19, 11733, 4, 1771, 34, 466, 714, 151, 16, 4483, 19, 116], [1, 1, 1, 10721, 231, 7228, 4655, 17, 721, 369, 2, 1871, 883, 16260], [1, 1, 1, 1, 23141, 364, 1298, 207, 2, 209, 11, 9, 644, 3, 38, 1109, 241], [13, 1, 10322, 2724, 5, 25064, 5, 6044, 10944, 1233, 14, 1095, 31, 1172, 62, 55, 3734, 2197, 10, 200, 28], [236, 16, 152, 157, 9, 15209, 16, 1446, 236, 1538, 16, 23, 8644, 435], [1, 52, 1989, 562, 160, 117, 510, 438, 117, 700, 5, 388, 977, 28], [1, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 1640, 2, 13083, 77, 6, 5148, 84, 247, 72, 570, 857, 30, 30, 968, 5916], [1, 1, 1, 1, 1, 1, 178, 1624, 4, 98, 4, 1710, 307, 3, 8, 260, 307, 3, 8, 260], [1, 107, 637, 9730, 25, 344, 110, 107, 12, 2072], [11377, 16, 761, 2616, 4, 4451, 153, 4, 9918, 4, 2643, 229, 90, 5888, 17, 23, 1580, 1065, 3882, 6, 455], [1, 407, 11, 433, 2703, 108, 2, 5, 203, 17, 1721, 67, 30, 11, 3301, 794, 1690, 327, 340, 18, 527, 125, 206, 176, 3], [1, 510, 7421, 2073, 4, 1230, 2, 12130, 77, 7860, 19, 85, 43, 6, 128, 6247, 16, 25, 9, 1149, 204], [70, 7637, 1, 4754, 69, 307, 3, 307, 3, 381, 3, 8, 37, 50, 176, 3, 176, 3, 900, 1088, 275, 398, 49, 382, 510, 949, 14, 2685, 10, 477, 29651, 758, 3, 29651, 758, 3], [13, 1, 256, 218, 174, 29, 1172, 101, 114, 66, 109, 14, 428, 4249, 137, 2462, 581, 4149, 33, 1939, 18, 529, 6, 3395, 216], [696, 362, 5193, 10, 2141, 286, 1224, 4222, 3708, 18, 14, 2146, 11, 4270, 6, 43, 144, 1166], [1, 333, 11, 621, 546, 12, 5328, 17, 227, 1632, 2, 2685, 142, 1233, 3166, 26], [13, 1, 766, 24, 1, 52, 189, 503, 7690, 353, 12, 2298, 2120, 11, 81, 819, 117, 188, 1844, 841, 616, 57], [1, 1, 102, 102, 211, 35, 45, 1355, 313, 217, 4, 34, 7009, 800, 3108, 870, 97, 235, 238, 238, 64], [1, 1, 60, 6018, 98, 180, 41, 1362, 19955, 67], [13, 1, 631, 27753, 3581, 631, 5224, 19096, 16, 113, 131, 113, 247, 3581, 16, 131, 113, 1443, 16], [1, 1, 4, 6746, 18, 66, 13355, 29, 16], [1, 2997, 7, 579, 16, 1911, 87, 30183, 1112, 7, 6876, 16, 1911, 87, 628, 24934, 7, 579, 12], [1, 1, 4076, 221, 63, 880, 65, 25, 5, 880, 1657, 7, 2255, 386, 19, 49, 1168, 2314, 2203, 23], [1, 96, 1631, 12, 65, 47, 215, 38, 703, 1133, 1116, 638, 48, 10232, 97, 47], [1, 18, 1497, 1647, 2589, 4, 19, 1230, 471, 32313, 571, 430, 1230, 18, 285, 1497, 7], [1, 13, 35, 32, 5, 1005, 16, 26618, 4, 38, 11, 1966, 1700, 297, 76, 61, 270, 249, 255, 76, 61, 270, 249, 255, 2877, 1], [1, 1, 35, 101, 493, 9, 152, 23, 182, 20, 111, 25, 2, 43, 7, 1841, 2, 374], [1, 1, 109, 1685, 5, 5, 4, 3013, 538, 368, 5106, 840, 109, 479, 455, 44, 19, 6, 122, 2655, 1665], [1, 11003, 2285, 10, 610, 28529, 147, 636, 16, 64, 6, 964, 126, 23, 64, 1155, 1419, 4412, 240, 17, 51, 41, 2633, 471], [2266, 2632, 166, 13053, 407, 649, 4882, 1534, 233, 11807, 4049, 2882, 1720], [1, 6, 96, 4602, 12660, 108, 517, 25734, 4, 13019, 26, 11, 44], [1, 1, 1, 1, 26, 8032, 1324, 468, 621, 881, 994, 11, 8303, 342], [195, 90, 5, 5274, 1888, 3, 8, 2844, 50, 199, 691, 37, 3, 8, 27, 50, 4239, 434, 532, 52, 759, 848, 27, 3918, 4937, 1404, 3, 4170, 1888, 3, 8, 37, 50, 9803, 4, 434, 644, 3, 18645, 50, 18646], [1, 629, 772, 2923, 14, 786, 280, 639, 5278, 5279, 28, 960, 68, 10293, 554, 191, 36, 14, 786, 132, 144, 46], [1, 1, 1, 13437, 11124, 11, 447], [256, 86, 243, 33, 622, 148, 31, 3426, 45, 179, 59, 224, 46, 18, 29, 3560, 1730, 3, 1730, 3, 27, 8, 1019, 27, 8, 1019, 27, 8, 1019, 27, 8, 1019], [1, 1, 1, 1, 1, 7101, 1, 1, 1129, 181], [13, 1, 774, 5, 1, 4054, 9, 2, 257, 467, 579, 213, 31, 462, 167, 4407, 2824, 1154, 18, 218, 13482], [1, 134, 15, 8817, 37, 3, 8, 27, 50, 15, 202, 518, 16, 2090, 134, 15, 29, 59, 575, 1577, 985, 28, 520, 424, 27, 424, 27, 76, 61, 76, 61], [1, 9163, 24193, 855, 8348, 234, 218, 1639, 289, 59, 2859, 288, 1533, 289, 2859, 2998], [1, 629, 96, 30670, 65, 5235, 525, 6, 606, 525, 1527, 52, 469, 8, 14, 533, 419, 31702, 533, 419, 4287, 18, 14, 66], [4163, 4164, 1, 523, 24, 333, 1, 3647, 562, 24, 1085, 4, 38, 333, 9065, 4, 9674], [4147, 352, 1050, 28, 23437, 28, 55, 1050, 59, 593, 14, 125, 1379, 4579, 1379], [20792, 2407, 1605, 2852, 1, 3647, 562, 24, 4, 5115, 82, 4073, 422, 2407, 20793, 1069, 7121], [1, 1, 2850, 1, 20, 455, 11, 1164, 7300, 14962, 124, 11, 1530, 1257, 455, 188, 54, 9615, 16], [1, 885, 19675, 453, 3421, 1388, 154, 52, 250, 52, 3, 8, 22, 10, 21], [5, 3504, 1035, 27, 114, 68, 1, 68, 19722, 28, 22060, 1677, 4439, 3, 8, 22, 10, 21, 42, 1589, 68, 641, 36], [1, 1098, 309, 41, 2, 19912, 197, 11, 2692, 2279, 23, 2799, 220], [1667, 6, 72, 3281, 18, 2653, 10, 2598, 18, 1821, 30, 24, 5242, 3079, 145, 721, 31137, 5, 653, 30, 27682], [273, 2022, 42, 13131, 8726, 168, 430, 6540, 5, 27392], [1, 1, 5, 324, 39, 3681, 2304, 121, 45, 9, 169, 3266, 121, 497, 32, 3681, 353, 4, 633, 254, 44], [1, 1, 5, 11, 6, 876, 64, 63, 28853, 32, 6, 95, 25, 2456, 7, 265, 87, 8117], [373, 281, 6977, 5, 563, 36, 6954, 10, 8339, 1080, 5, 457, 3172, 2965, 5, 6977, 5, 563, 36, 6954, 10, 11652], [402, 15069, 16707, 2, 1954, 103, 2873, 373, 2, 389, 1661, 642, 10, 619, 165, 130, 3, 165, 130, 3, 165, 130, 3], [355, 362, 42, 24059, 6, 1, 29, 14, 23240, 28, 1, 29, 6032, 16, 457, 12884, 144, 14, 231, 135], [1425, 3785, 395, 88, 1061, 71, 158, 993, 277, 1388, 71, 4470, 444, 356, 4584, 585, 2366, 3350], [1, 323, 6, 25857, 19, 108, 2, 1126, 37, 3, 1167, 48, 97, 240, 26, 625, 23, 15710, 9, 904, 323, 128], [1, 3807, 1, 275, 15, 59, 14, 102, 66, 28, 118, 68, 66, 35, 1352], [342, 2, 20, 34, 11, 3482, 2, 43, 1030, 11, 6, 14, 1691, 64], [1, 145, 21753, 9, 64, 6, 4, 9534, 25, 16, 11526, 7, 24594, 251, 603], [338, 352, 146, 366, 4637, 338, 120, 468, 2407, 1605, 2852, 446, 26903, 13046, 4, 13046, 25, 278, 705, 39, 2511, 8873, 188], [13, 1, 623, 706, 6, 359, 151, 10066, 129, 302, 228, 20, 228, 74, 302, 165, 130, 3, 75, 27], [43, 4, 5, 947, 2528, 188, 2282, 5, 203, 4616, 8141, 378, 295, 127, 6052], [1, 1, 246, 10, 15, 35, 24, 42, 15, 49, 173, 608, 2493, 210, 57, 226, 34, 17, 1798, 4, 3299, 6838], [3103, 16, 2684, 399, 131, 1315, 9174, 4, 39, 468, 19, 150, 990, 12877, 86, 35, 394, 1992, 2219, 24, 654, 16, 394, 370, 1501], [1, 1, 1, 728, 4, 26, 5615, 17, 9, 51, 416, 34, 4, 20], [1, 1, 28, 1184, 366, 42, 7, 891, 11443, 131, 27401, 4280, 466, 11920, 1037, 25], [1, 137, 786, 29, 11445, 121, 353, 2331], [13, 1, 25, 486, 414, 15, 554, 234, 15, 59, 949, 58, 2497], [1, 1, 1, 35, 4699, 3495, 12, 65, 9, 204, 577, 700, 2287, 67], [1, 894, 240, 24670, 11, 39, 1570, 9, 129, 48, 402, 4, 508, 6461, 12, 25, 298], [1, 73, 18, 102, 52, 1643, 111, 85, 112, 522, 41, 2535, 48, 1973, 1973, 111, 54, 8462, 102, 52], [1278, 1485, 658, 24109, 72, 114, 122, 2727, 2829, 12239, 28072, 263, 262, 5, 29, 5, 21569, 36, 798, 5, 5777], [4, 22890, 32, 1392, 5122, 32630, 215, 1406, 12, 26783, 67, 1061, 38, 1180, 11322, 39, 9], [1, 2117, 876, 16, 115, 7910, 16, 1499, 16, 1638, 17, 18406, 9, 3346, 11, 292, 1150, 6, 9, 442, 836], [1, 70, 81, 35, 234, 3661, 722, 68, 2270, 6, 2270, 2958, 647], [1, 1084, 32, 5, 463, 38, 6650, 5, 16, 856, 14, 1475, 1087, 572, 11655, 188, 856, 38, 71, 973, 507, 25], [13, 1, 233, 5, 23535, 12, 90, 57, 26697, 7, 1914, 108, 26, 20, 471, 153, 2345, 1008, 145, 153, 44, 20, 3105, 56, 27179, 44, 263, 525, 344], [101, 10, 810, 642, 326, 68, 249, 18, 1331, 28, 8867, 13368, 128, 4, 744, 44, 406, 525, 94, 373, 17, 3669], [1, 1, 9, 7386, 20, 6726, 309, 1543, 73, 85, 1851, 288, 325, 866, 3542, 488], [1, 1, 10348, 1, 1280, 1112, 4, 519, 3559, 25, 2447, 20, 6, 2158, 2, 343, 3, 8, 260, 3, 8, 260], [1, 10824, 97, 382, 282, 7146, 210, 476, 760, 382, 1057, 31139, 287, 456, 2, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 45, 246, 18, 5208, 760, 78, 1265], [1, 282, 604, 25, 236, 18, 318, 1311, 326, 236, 18, 2488], [1, 1, 55, 404, 24, 571, 2, 34, 277, 1540, 9, 19, 82, 57, 238, 20, 571, 51, 1250, 18, 238, 7594, 11], [1, 1815, 48, 60, 1442, 14, 8839, 26, 8829, 1098, 23, 183, 1551, 7, 204, 1163, 23892, 1399, 7113], [13, 1, 14, 373, 6772, 179, 14, 7107, 31, 179, 14, 189, 10, 373, 5, 2263, 3164, 4983, 239, 317], [1, 356, 23, 182, 3523, 2942, 205, 4, 815, 2609, 82, 25, 2775, 3892, 525, 162, 1746, 16, 854, 22981, 45], [13408, 1, 52, 2742, 562, 160, 510, 438, 117, 700, 5, 388, 977, 510, 474], [363, 1589, 376, 353, 1261, 250, 31, 679, 193, 723, 15492, 114, 36, 8244, 1468, 273, 440, 224, 31, 193], [13, 1, 575, 448, 86, 55, 28, 187, 2619, 24749, 488, 93, 31436, 6, 216, 8], [1, 35, 34, 4, 3262, 9121, 251, 20, 27470, 1513, 45, 53, 7, 824, 2, 14759, 2, 194, 6651, 2661, 2129, 43, 7646, 1513], [1, 1, 443, 5504, 4053, 23, 319, 40, 187, 55, 5023, 6425, 45, 2459], [1353, 8, 3195, 6911, 473, 473, 18, 17, 122, 54, 870, 16, 2557, 3, 2557, 3, 81, 7947, 26, 54], [1, 1, 1, 1, 1, 5, 1, 1, 1], [43, 32, 161, 28618, 26322, 2949, 48, 34, 65, 163, 4, 208, 667, 111, 82, 10963, 369, 5, 237, 153], [1, 1, 20, 6, 240, 126, 85, 45, 1599, 32, 788, 4, 124, 67, 186, 30, 54, 346, 365, 274, 32, 54], [1, 23107, 254, 131, 2, 3566, 504, 650, 3746, 220, 266, 535, 282, 693, 9226, 6765, 4], [1, 1, 2617, 24, 34, 162, 12, 1667, 7, 9, 564, 3772, 1054, 20, 98, 95], [1, 1, 4064, 40, 4, 659, 186, 40, 458, 29756, 20593, 11, 538], [1, 1, 20, 1016, 30183, 558, 4, 83, 180, 9, 245, 322, 1405, 98, 569, 2, 4, 1405, 1236, 126, 4, 9, 1335], [20, 11961, 610, 5, 12, 90, 38, 1790, 7, 2636, 669, 27322, 47, 667, 111, 67, 3, 8, 22, 10, 21], [13, 1, 3808, 1415, 18, 14, 328, 10, 478, 2197, 86, 362, 55, 93, 2131, 6, 10147, 348, 29, 46, 2407, 1605, 310], [1, 1, 55, 9, 9, 2441, 12, 90, 40, 9, 344, 657, 23, 186, 2447, 2957, 12336], [1, 68, 366, 1654, 1839, 36, 5825, 21803, 179, 478, 1667, 419, 16, 340, 1996, 5342, 2397, 21804], [818, 53, 12, 12, 16, 7523, 5264, 2697, 2, 35], [1, 1, 38, 761, 185, 24528, 7, 31773, 711, 761, 4, 1779, 25, 131, 761, 2301, 140], [1, 1120, 1, 1, 6293, 2509, 181, 369, 19, 1234, 20, 5258, 26, 5, 159, 12, 167, 120, 19], [42, 28499, 4162, 340, 15, 4162], [1, 1084, 26, 3137, 32, 53, 7, 1770, 849, 26, 16, 2375, 219, 427, 98, 4, 879, 3746, 227, 2], [1, 4128, 187, 317, 175, 2459, 109, 187, 317, 500, 2555, 12566, 1403, 218, 243, 187, 831, 14, 101, 276, 847, 8493, 72], [98, 4, 16, 1115, 41, 323, 26, 195, 7656, 7, 5597, 2587, 1488, 47, 8402, 391, 6, 199, 458], [49, 482, 10, 15, 4589, 37, 3, 8, 27, 50, 75, 27, 344, 48, 1831, 153, 10688, 525, 671, 3, 8, 22665, 1289, 1, 59, 14, 66, 37, 3, 8, 27, 50, 75, 27, 798, 5, 1], [240, 2, 39, 192, 180, 287, 364, 8848], [1, 7, 10173, 183, 43, 32, 126, 2979, 7, 45, 4547, 1230, 16, 5948, 7543, 40, 2257, 30888, 297], [59, 175, 16119, 1707, 1688, 28, 2242, 36, 3514, 58, 29, 49, 4148, 5384, 439, 1026], [1, 39, 29170, 12, 1046, 113, 589, 509, 122, 761, 5000, 3114, 509, 761, 7], [13, 1, 175, 3243, 36, 72, 916, 28, 72, 2613, 7202, 149, 1717, 94, 62, 149, 72, 1006, 29, 15, 1, 42, 15], [1, 3894, 113, 151, 2492], [13, 1, 1, 560, 1, 5, 89, 41, 6306, 1429, 10803, 1123, 3, 20, 10011, 509, 450, 23, 1234, 12, 409, 9902, 196], [122, 85, 1009, 394, 157, 25], [1, 28668, 1, 1330, 1, 12463, 1, 1, 1, 142, 62, 2309, 179, 46, 1675], [131, 4214], [13, 1, 334, 62, 32171, 179, 2407, 1605, 2852, 29, 388, 211, 766, 388, 188, 11364, 12, 90, 1, 4210], [1, 1, 591, 11, 620, 605, 64, 5406, 87, 162, 44, 3277, 1550, 265, 51, 4186, 684, 126], [1, 2237, 394, 1373, 17, 1555, 5, 9537, 174, 835, 1337, 116, 461, 229, 69, 795, 1680, 2, 1658], [1, 5626, 371, 30, 24, 16, 1160, 96, 2301, 435, 4, 34, 417, 28831, 26, 12937, 26, 417], [1, 1, 18, 175, 684, 511, 257, 438, 148, 72, 114, 1, 63, 20158, 136, 20159, 682, 63], [1, 1, 1, 38, 13112, 366, 29, 179, 12138, 27046, 135], [1, 7, 6, 290, 23, 9, 26, 523, 12, 7239, 67, 3481, 104, 95], [80, 2305, 30, 32, 1790, 7, 219, 17, 1770, 297, 87], [12988, 633, 44, 2905, 9551, 144, 202, 42, 15, 75, 27], [1, 1, 639, 12821, 134, 200, 15, 6466, 12728, 8698, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 194, 27938, 3505, 113, 1434], [70, 81, 1, 159, 15, 686, 114, 2054, 350, 7804, 764, 275, 15, 602, 10, 14, 764, 48], [1, 143, 374, 2405, 5, 15, 1936, 518, 5219, 5170, 11, 106, 38, 152, 308], [1, 1, 1, 20, 1204, 53, 2, 2145, 1444, 2, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 8320, 11467, 2703, 108, 2, 544], [13, 1, 3694, 38, 4497, 2862, 8836, 8836, 51, 104, 47, 711, 8506, 5, 314, 1202, 7, 300, 1079, 12, 17, 30972, 12, 585, 2862, 13732, 7272], [6520, 101, 4687, 719, 438, 15, 18, 6, 518, 2180, 29], [1, 374, 4, 156, 7690, 156, 2, 374, 252, 204, 3797, 10495, 3797, 480, 4, 319, 6718], [13, 1, 13430, 103, 113, 29, 1078, 200, 10863, 6184, 2899, 324, 6184, 32406], [13, 1, 1, 85, 12, 28388, 119, 222, 13158, 3488, 7, 1770, 1921, 12, 90, 28388, 1435, 2189, 13158, 85, 556, 1459], [1, 1, 1813, 97, 180, 683, 107, 4178, 9, 1028, 989, 192, 2, 38, 551], [13, 1, 137, 231, 937, 143, 1, 1, 29, 1716, 6386, 1, 137, 1739, 1], [13, 1, 8615, 1191, 100, 56, 25, 56, 12167, 1178, 56, 4355, 1178, 5, 56, 2295, 2175], [1, 513, 1579, 5, 436, 5, 382, 29, 15, 8885], [218, 582, 5801, 3, 171, 141, 2161, 63, 17, 5197, 2, 258, 198], [1, 33, 3294, 18, 107, 927, 3098, 1900, 635, 319], [1, 16, 85, 112, 4421, 9, 87, 108, 85, 112, 57, 44], [1, 404, 1, 103, 103, 2126, 534, 127, 3100, 235, 725, 51, 269, 25, 2073, 721, 51, 1156, 364], [1, 1, 1, 26920, 1231, 11, 284, 754, 356, 2483, 2, 6, 1664], [1, 1, 1, 703, 227, 25, 9, 2, 2952, 12, 1087, 2939, 74, 12, 157, 47, 1522], [1, 1166, 565, 175, 623, 251, 27378, 5620, 59, 1785, 905, 4, 3076, 26, 48, 1785, 9, 2], [13, 1, 2491, 1272, 2, 5, 203, 89, 595, 1171, 1473, 833, 59, 68, 486, 6607, 362, 20053, 28, 567, 46, 3293, 686, 31], [56, 3843, 10009, 19, 695, 112, 12, 90, 22911, 56, 125, 206, 156, 269, 2, 18201, 12, 17, 2428, 2902], [13, 1, 1, 610, 475, 161, 3013, 5882, 207, 17, 207, 222, 45, 11637, 201, 11, 8988, 6, 227, 2], [1, 1, 1516, 371, 30, 185, 1582, 3100, 31520, 2, 261, 82, 95, 1137, 11, 708], [13, 1, 30, 24, 5168, 534, 6163, 229], [1, 1824, 392, 61, 270, 249, 255, 392, 61, 270, 249, 255, 126, 729, 185, 662, 17797, 188, 12422, 5251, 39, 183, 131], [292, 183, 140, 155, 501, 12, 128, 2441, 32, 126, 169, 6, 48, 857, 2700, 12, 13082, 12, 1821, 2674, 84, 90, 262], [13, 1, 38, 23, 2, 335, 18, 1960, 80, 170, 31, 3840, 114, 46, 101, 15, 18710, 29, 9123, 28, 467, 772, 723, 5605], [1, 1, 1127, 31, 4174, 46, 146, 5205, 62, 55, 173, 28, 11889, 62, 1029, 31, 46], [11748, 781, 5378, 249, 5521, 1368], [13, 1, 49, 62, 3824, 529, 123, 1152, 7589, 123, 1152, 1148, 26846, 58, 18, 166, 375, 20416, 644, 3], [1, 1, 1, 1, 1, 430, 78, 3666, 60, 313, 2, 39], [26254, 17, 3858, 1798, 2545, 64, 2545, 64, 120, 23, 2, 26, 349, 23, 1008, 96, 1240], [70, 81, 6, 15, 5929, 637, 3425, 1140, 826, 694, 311, 1140, 826, 694, 6216, 1140, 394, 691, 4285, 910, 83, 1226, 5194], [1, 83, 244, 814, 83, 244, 115, 20, 53, 818, 407, 25, 2, 407, 16, 245, 104, 47, 926, 16, 2810, 39], [1, 1, 1, 35, 145, 235, 2416, 40, 1057, 64, 6, 34, 85, 1422, 968, 295, 244, 1461], [148, 16, 43, 12, 769, 1679, 5779, 740, 16, 1949, 12, 5, 10818, 1150, 1313, 5, 2979, 7, 9, 405, 5, 12080], [143, 3378, 11784, 1103, 36, 4494, 37, 3, 8, 27, 50, 1, 1, 78, 9036, 821, 290, 6, 37, 3, 8, 27, 50, 151, 311, 456, 14861], [13, 1, 733, 1009, 16, 35, 213, 1006, 29, 15, 1541, 189, 1, 718, 96, 172, 11, 4856, 2, 154, 80, 543, 605, 172, 2034, 16, 865, 278], [1, 1031, 516, 2, 20, 1087, 2482, 3706, 1405, 109, 111, 1166, 542, 201, 4859, 633, 825, 2155, 39, 23, 245, 113], [9556, 18, 93, 14, 6462, 10, 28, 2562, 10, 415, 55, 93, 14, 10, 778, 49, 40], [13, 1, 175, 5, 642, 413, 5586, 81, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 69, 70, 81, 31, 2006, 75, 27, 1, 311], [1, 102, 52, 483, 94, 235, 4078, 1741, 124, 4, 2662, 2018, 4, 2100, 1741], [30, 45, 594, 413, 4, 1627, 17, 63, 53, 7, 8749, 9, 2513, 12, 2277, 17, 356, 9000, 104, 19], [1, 29188, 765, 46, 18, 49, 16], [1, 731, 3, 381, 3, 8, 37, 50, 48, 2414, 5363, 367, 196, 952, 1173, 4, 3900, 595, 45, 178, 2639, 298, 731, 3, 367, 196], [1, 1, 1, 1, 1, 1, 462, 15, 55, 1286, 5, 135], [1, 2267, 158, 8358, 108, 147, 25, 112, 164, 2600, 82, 2896, 1535, 51, 104, 14, 471, 100, 60, 265, 129, 314], [321, 119, 120, 60, 1880, 2, 120, 116, 371, 30, 24, 3084, 238, 116, 11, 2690, 10404, 956, 5219, 3809, 45, 29581, 29582], [585, 4737, 1781, 32220, 215, 1781, 83, 360, 83, 106], [1, 4579, 7041, 2128, 145, 133, 127, 12, 32, 535, 4396, 12768, 2566, 471, 147, 3958, 128, 1653, 32, 1000], [1, 1291, 4795, 3830, 1737, 105, 74, 97, 54, 461, 2, 54, 9, 29420, 287, 2, 20, 4140, 15807], [13, 1, 199, 4687, 2, 1072, 7, 57, 354, 12, 22430, 608, 192, 28093, 1908, 51, 247, 193, 3522, 27], [243, 833, 136, 69, 1990, 768, 42, 28, 988, 583, 6, 16, 258, 198, 909, 198], [1, 142, 8, 15, 29, 1533, 4801], [151, 2300, 16283, 510, 5192, 510, 510, 4027, 31, 5147, 5814], [1, 31, 17, 10439, 6, 803, 218, 2063, 1110, 1085, 1140, 26, 96, 17710, 7080, 355, 51, 317], [1, 38, 112, 53, 124, 157, 9, 2965, 2429, 701, 4208, 516, 174, 825], [1090, 123, 3418, 3210, 29544, 6446, 4500, 403, 3683, 6446, 10, 5176, 1513, 58, 202, 2751, 581], [1, 85, 1679, 43, 32, 11142, 241, 6, 31737, 7, 282, 2119, 54, 1020, 30, 1096, 111, 11, 1531], [1, 143, 69, 2203, 138, 1915, 10713, 11, 2408, 1, 281, 235, 18922, 18922], [1, 1, 437, 13581, 110, 12460, 785, 47, 49, 159, 25, 412, 47, 170, 11], [13, 1, 1, 3866, 16, 26236, 431, 12281, 1102, 7, 7, 859], [1, 1, 38, 233, 96, 1848, 164, 1129, 241, 40, 26, 28072, 803, 16, 22492, 26, 26, 26, 608, 39, 9, 3, 8, 22, 10, 21], [1, 1, 1, 497, 7, 4517, 564, 51, 11425, 11, 7337, 9, 2, 7623, 4, 7], [1, 5, 1, 1, 78, 11243, 1065, 710, 113, 5348, 4, 78, 23625, 4, 11243, 181, 3511, 10237, 19, 1413, 3, 8, 22, 10, 21], [83, 868, 115, 83, 3100, 7211, 83, 2678], [5, 3828, 1026, 340, 15, 49, 173, 15, 55, 101, 10, 33, 102, 66, 486, 28, 13472, 73, 29, 14, 125, 15], [13286, 75, 27, 639, 27, 10944, 4821, 6937, 31265, 5, 214, 5, 214, 464, 30748, 376, 340, 46, 1577, 951, 1326, 3245, 10, 14, 5, 5], [1, 1, 430, 368, 4034, 20385, 12, 994, 182, 6237, 26, 1497, 17, 853, 15594, 97, 11], [1, 1, 1824, 40, 18, 643, 611, 40, 1793, 291, 170, 1721, 1306, 476, 5003, 30154, 891, 150, 2], [1, 295, 303, 3715, 62, 139, 7027, 510, 115, 3667, 18, 123, 1205, 200, 40, 6566, 329, 181], [13, 1, 1, 2314, 9, 3984, 64, 3088, 7, 7515, 5147], [231, 4881, 168, 16, 1, 252, 243, 15, 99, 1344, 184, 768, 567, 46, 300, 102, 341, 1216], [20, 6000, 2, 20, 109, 2], [770, 43, 43, 770, 770, 43, 43, 43, 43, 11704, 11704], [1, 15, 790, 58, 1185, 328, 32586, 851, 842, 64, 40, 3554, 291, 10866, 28], [1405, 23, 199, 208, 85, 7, 174, 87, 4194, 219, 5, 159, 4, 6252, 675, 412, 20, 158, 494], [1, 1, 20516, 204, 30123, 67, 13206, 2, 4349, 10494, 2, 293, 78, 851, 19, 108], [1, 1, 276, 1, 25841, 8786, 490, 85, 26, 43, 287, 2705, 7453, 6, 1013, 9, 797], [609, 9622, 378, 10687, 5378, 27544, 5224, 736, 203, 131, 2083, 309, 1543, 528, 249, 255], [6, 33, 962, 1343, 1393, 23, 19, 311, 646, 692, 25, 1773, 4330, 2291, 45, 1660, 7, 1320], [1, 136, 42, 15, 166, 7016, 75, 27, 75, 27, 75, 27], [13, 1, 42, 49, 173, 3659, 205, 54, 2, 69, 402, 1744, 736], [1, 15988, 48, 884, 2204, 12, 125, 206, 429, 10045, 6130, 4, 15989, 2], [5084, 23, 178, 205, 35], [376, 1107, 68, 198, 72, 21050, 930, 31, 200, 28, 930, 488, 31, 16, 335, 18, 166, 375, 4232], [31, 389, 1062, 13351, 24, 2542, 1, 71, 2115, 122, 113, 104, 47, 2423, 9, 816, 6, 23, 54, 2434, 3034, 1054, 48, 1443], [1, 1, 78, 3921, 2, 293, 4, 7227, 5549, 163, 107, 155, 841, 742, 2, 344, 458, 13374], [13, 1, 11, 7284, 148, 310, 23989, 6366, 210, 159, 284, 608, 2277, 8323, 2848, 1967], [1, 5447, 9, 5, 125, 206, 89, 1032, 1097, 2481, 32, 4292, 156, 51, 74, 7, 2370, 6188, 2566, 798, 5, 978], [1, 1, 1, 331, 1653, 4, 3233, 773, 150, 1369, 20, 728, 1001, 445, 104, 14, 221], [1, 56, 1, 1, 56, 1, 3326, 159, 62, 2533, 123, 5205, 6, 28374], [13, 1, 1375, 18, 14, 132, 62, 59, 99, 615, 1006, 29, 14, 10, 1, 2019, 14, 10, 1, 31], [96, 4983, 11, 2690, 3215, 6313, 44, 19, 51, 1086, 11, 4478, 3482, 19, 108, 21067, 25, 342, 2, 3511, 18363], [1, 1, 286, 184, 188, 1204, 45, 63, 65, 12067, 19, 344, 20, 6, 637, 242, 25, 10963, 2], [13, 1, 70, 28917, 132, 139, 46, 3278, 10], [159, 44, 431, 310, 131, 338, 6861, 210, 11188, 431, 310, 26319, 135], [1, 12615, 6051, 458], [1, 30, 24, 194, 156, 4, 253, 86, 204, 14352, 12, 475, 5310, 3737, 2191, 24, 1912, 25, 454, 104], [13, 1, 1, 33, 146, 10120, 5240, 7626, 115, 24, 213, 16183, 179, 16184, 1816, 14, 184, 10, 16185, 40, 4510, 29], [1, 44, 84, 161, 504, 127, 7, 121, 45, 1159, 1030, 7, 3958, 395, 181, 2245, 265, 697, 18, 24573, 1329], [1, 1, 1, 1, 1, 1, 1, 1, 83, 19, 3050, 24, 102, 280, 421], [77, 7929, 6, 162, 223, 4, 201, 23951, 1622, 7, 857, 1951, 538, 67, 2722, 821, 752, 137, 912, 785, 20, 65, 11], [13, 1, 1, 439, 339, 224, 34, 65, 5103, 1003, 262, 30, 24, 652, 116, 263, 262, 176, 3, 176, 3, 176, 3], [568, 2, 18, 281, 203, 12, 299, 103, 7310, 26, 103, 1341, 19, 525, 515, 515, 619, 281, 1147, 1147, 309, 743, 196, 745, 677, 678, 1225, 1226], [1, 1160, 5043, 6265, 4, 12049, 552, 220, 21412, 3545, 698, 2, 45, 8033, 385, 17137, 509, 7567, 509, 71, 3921, 1257], [1, 2107, 6152, 96, 7, 1365, 4687, 476, 9, 449, 5, 1770, 326, 58, 6, 4464, 906], [1, 1026, 340, 16223, 914, 165, 130, 3, 175, 14284, 59, 14, 184, 3351, 532], [1, 1, 1, 43, 235, 21121, 264, 7, 2217, 2], [13, 1, 1, 1, 854, 54, 2, 1047, 63, 80, 2985, 6343, 1369, 1065, 1774, 3747, 4, 6343, 4, 2758, 2, 71, 38], [1, 1, 1, 1401, 145, 147, 880, 2961, 3015, 5974, 43, 12, 7346, 15121, 2, 6, 9685, 19], [1, 83, 83, 360, 1608, 2014, 82, 115, 100, 2323, 2, 7319, 586, 83, 244, 115, 800, 1664], [13, 1, 1, 35, 24, 139, 951, 102, 1661, 8845, 831, 1168, 29, 30, 24, 3558, 2279, 101, 213, 2741, 2015, 312], [96, 1538, 164, 44, 692, 285, 47, 643, 3, 4958, 6, 15884, 436, 90, 668, 6112, 1690, 47, 3, 8, 79, 50, 167, 88, 15885, 1771], [111, 18, 2065, 6692, 48, 111, 7, 4, 3286, 9, 25695, 67, 6000, 201, 26, 95, 808, 7, 2982, 7755], [1, 3799, 74, 6872, 44, 674, 4, 8645, 186, 105, 71, 38, 127, 1176, 82, 41, 980, 177, 7, 545, 444, 41, 1362], [1, 22820, 12778, 761, 89, 1865, 1865, 846, 95, 591, 17, 1917, 541, 28, 3329, 235, 3589, 761], [1, 15, 202, 833, 327, 93, 4125, 808, 2958, 175, 15, 965, 251, 73, 28, 9128, 258, 198], [13, 1, 712, 511, 366, 781, 6, 7845, 73, 3376, 11970, 1806, 18, 273, 14, 1012, 5, 377, 31, 56, 56, 76, 61, 48, 103, 6708], [3226, 5657, 25, 9, 648, 17829, 7, 17830, 6019, 461, 2, 6, 7967, 7414, 17831, 167, 17, 1722, 2748, 138, 1721], [583, 428, 29, 415, 6, 251, 340, 15], [13, 1, 1, 1, 10748, 1, 1, 641, 14037, 898, 710, 1362, 11392, 32, 265, 265, 84, 898, 2], [70, 189, 99, 341, 937, 24540, 132, 33, 19827, 28254], [1, 309, 275, 16, 469, 134, 1, 309, 9746, 3774, 3466], [1, 15393, 160, 33, 20520, 8, 166, 10, 289, 341, 4403, 252], [1, 5921, 2, 18, 465, 16, 39, 6, 358, 2, 38, 3635, 7, 457, 313, 2, 458, 4861, 1440], [936, 891, 2, 53, 4, 16223, 209, 26230, 17, 53, 41, 17507, 162, 53, 16, 115], [1, 69, 125, 206, 101, 132, 2, 136, 4857, 361, 48, 153, 11, 2298, 4456, 32320, 2891, 125, 206], [1, 1018, 1063, 392, 61, 392, 61, 3005, 137, 1805, 392, 61, 392, 61, 12665, 563, 6, 15, 35, 1, 1, 3382], [13, 1, 353, 8109, 4698, 5, 2014, 1667, 1474, 8, 5379, 378, 1376, 1275], [1159, 12, 40, 399, 220, 1159, 2585, 84, 40, 84, 1250, 277, 1324, 538, 12, 54, 25897, 220, 1404, 3, 2557, 3], [13, 1, 200, 2542, 16, 6, 1125, 59, 14, 66, 18, 1985, 6, 602, 2312, 326, 15, 13745, 15, 965, 3028], [318, 3654, 6, 28204, 2598, 318, 1088, 6, 28204, 6, 72, 8332, 31, 5846, 5189, 1, 5, 1896, 569, 12095], [1, 1, 1, 1, 547, 85, 23, 39, 4382, 4, 9, 2, 85, 28, 43, 698, 25, 6953, 12, 118, 2], [80, 101, 31, 46, 125, 467, 306, 4173, 1118, 1210, 628, 1, 523, 24, 333, 18, 14, 315, 409], [1, 1, 24, 110, 271, 6292, 9, 64, 6292, 9, 2, 26, 19, 974, 236, 352, 10477, 135], [1, 35, 143, 515, 515, 30, 35, 299, 16991, 3079, 5030, 48, 10527, 263, 51, 53, 7, 113, 292, 34], [1, 724, 1, 1486, 653, 865, 1486, 48, 547, 23, 181, 695, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [70, 81, 6, 15, 33, 304, 35, 1, 2669, 94, 266, 1561, 311, 6216, 987, 235, 1226, 1059, 838, 8245, 81, 320, 8245, 301, 31714, 75, 27, 367, 196, 31714, 367, 196, 75, 27, 76, 61], [96, 38, 5808, 691, 2, 751, 3068, 2, 63, 117, 11, 16576, 38, 975, 10405, 2, 63, 699, 19, 38], [13, 1, 1092, 405, 670, 2, 938, 96, 2, 28400, 4, 96, 23, 725, 8628, 122, 2207, 212, 41, 4499, 529, 3], [13, 1, 16362, 5, 5, 237, 4, 12158, 12, 1772, 145, 71, 5247, 16, 186, 2, 5, 1920, 310, 407, 5, 668, 41, 344, 882, 19, 5, 71, 9301, 552], [847, 7470, 193], [13, 1, 478, 16066, 2901, 2338, 36, 14, 2108, 611, 80, 101, 5101, 218, 17353, 14, 1486, 13207, 326], [1107, 16, 664, 15, 3441, 68, 5734, 309, 3847, 1197, 249, 255, 11289, 52], [1, 1, 1, 230, 4, 2, 784, 23, 5518, 26, 230, 9, 2, 6, 946, 25, 5518], [1, 1, 1, 213, 20644, 82, 204, 577, 120, 890, 4, 20426, 430, 269, 2, 895, 9, 108], [13, 1, 27, 106, 4, 2930, 23529, 26, 2159, 4037, 37, 3, 8, 640, 746, 2287, 2006, 5, 746, 11112, 5342, 5342], [13, 1, 752, 46, 4057, 2887, 268, 6, 2814, 22014, 49, 448, 37, 3, 8, 27, 50, 366, 22409, 256, 196, 256, 196], [62, 42, 14, 12399, 13472, 661, 10, 1266, 494, 3, 357, 10, 289, 1152, 22, 11549], [1, 8728, 69, 7588, 26, 2490, 23, 39, 885, 186, 2, 4, 9, 683, 162, 1053, 89, 11793, 2], [13, 1, 334, 18, 14, 389, 889, 10, 619, 6392, 312, 18046, 477, 5, 314, 339, 583, 132, 981, 28, 2066], [1, 1, 859, 859, 1, 33, 7270, 76, 61, 528, 270, 249, 255], [1, 443, 39, 4, 2642, 9, 28808, 133, 10679, 2, 1054, 616, 443], [1, 1, 15, 272, 2874, 68, 557, 10045, 851], [15, 42, 6, 16, 114, 46, 581, 556, 30413, 15, 492, 2790, 3931, 2], [1, 195, 31401, 5380, 1047, 4847, 20, 551, 2720, 12, 11, 1493, 2, 9, 95], [94, 33, 1635, 2524, 117, 8732, 117, 123, 29729, 160, 117, 36, 46, 341, 493, 1491, 123, 2619, 28, 1998, 75, 27], [1, 1036, 1554, 12, 32272, 32273, 16, 57, 23, 207, 9, 3742, 491, 145, 695, 471, 669, 491, 32274, 9, 44, 25, 32275], [1, 18, 12, 1423, 251, 10541, 71, 251, 47, 175, 1286, 40, 18, 12745, 45, 93], [1, 35, 759, 49, 3175, 1106, 622, 16, 23, 172, 1087, 448, 2046, 369, 19], [1, 240, 856, 844, 551, 110, 1515, 43, 12, 12633, 16, 669, 2, 203, 943, 107, 155, 12, 355, 1020], [13, 1, 4778, 278, 4667, 4681, 179, 123, 32219, 12910], [1, 3954, 368, 1501, 5377, 16, 2585, 1194, 3587, 72, 1, 1280, 730, 6, 12785, 128, 57, 9048, 11], [9317, 2169, 2531, 29, 33, 4759, 1035, 27, 224, 4045, 14, 10251, 49, 1190, 4100, 6, 1062, 2307, 58, 273, 453], [13, 1, 286, 5, 5, 15347, 3783, 847, 5, 2826, 6012, 847, 5, 2826, 877, 847, 5, 2987, 847, 5, 15348, 847, 5, 1370], [5, 4547, 5, 49, 5, 5826, 5, 5, 5, 5432, 215, 1560, 696, 8393, 36, 14, 6590, 344, 5552, 20, 9436, 11], [1, 137, 1654, 1633, 377, 1353, 4061, 10, 588, 1041, 553, 19, 377, 67, 1449, 405, 64, 1365], [1, 13028, 389, 5, 493, 4, 1977, 111, 201, 45, 506, 247, 347, 88, 74, 74, 45, 163, 74, 25], [1, 35, 34, 669, 19, 194, 43, 164, 4977, 9, 2415, 355, 16, 9, 5409, 45, 20, 2747], [1, 1135, 7690, 170, 89, 11600, 729, 4952, 27709, 27595, 41, 3469, 232, 188, 4667, 4469, 345, 916, 3586], [13, 1, 5, 18, 763, 176, 343, 3, 243, 772, 3238, 31, 28, 99, 14, 5018, 6, 684, 68, 1152, 46, 18, 29, 15, 5, 642], [1, 2163, 34, 155, 19, 952, 6, 20239, 8845, 38, 789, 47, 780, 976, 9, 1315, 19, 1446, 542], [1, 2128, 147, 13604, 11791, 38, 3963, 87, 844, 63, 5236, 4, 884, 1, 35, 7, 817, 9, 2902], [30, 24, 7, 1504, 1504, 281, 733, 8268, 4, 702, 310, 30, 30], [274, 24, 172, 5622, 427, 85, 7, 121, 51, 182, 4965, 534, 692, 2412, 2, 43, 7, 12441], [66, 37, 3, 8, 1113, 37, 3, 8, 1113, 613, 3, 3521, 1296, 496, 1, 24, 10224, 160, 117, 8, 166, 388, 918, 474, 34, 24, 122], [1, 102, 52, 18, 17, 3761, 11, 1309, 138, 35], [1, 108, 12, 65, 238, 43, 7, 1841, 82, 39, 2, 38, 108, 12, 90, 207, 313, 19, 577, 44, 25, 1221, 5, 1221, 1336, 44], [1, 95, 53, 12, 9711, 10081, 5695, 12, 2741, 19, 875, 2, 555, 353, 12, 90, 34, 7, 57, 378, 10180], [13, 1, 1, 181, 75, 27, 101, 10, 33, 809, 231], [143, 35, 1, 1287, 2, 34, 5, 237, 89, 53, 7, 17, 15134, 6187, 12], [14892, 333, 32], [1, 5216, 159, 1311, 137, 2657, 10500, 15, 124, 7, 504, 120, 98, 2, 180, 11337], [1, 121, 7, 3231, 118, 805, 74, 385, 17, 4192, 121, 961, 1919, 7, 18998, 4685, 188, 976, 118, 11143], [1, 42, 15, 49, 173, 166, 75, 27], [1, 828, 1859, 332, 548, 2, 3371, 7, 14267, 1, 35], [1, 109, 7, 277, 6, 227, 9, 3179, 570, 3552, 20, 602, 345, 298, 71, 1840], [13, 1, 1, 5, 52, 469, 19328, 554, 72, 146, 28, 250, 62, 149, 72, 29, 15, 59, 14, 428], [13, 1, 720, 1407, 7071, 10, 42, 26302, 646, 1, 246, 10], [13, 1, 720, 1, 28, 68, 2820, 170, 1, 1, 1, 28, 2040, 499], [1, 1, 1617, 1603, 227, 64, 4, 96, 784, 56, 1146, 90, 77, 383, 169, 9, 4360, 17202], [2100, 5867, 3743, 67, 2, 151, 156, 278, 151, 156, 9434, 1, 978], [1, 687, 400, 38, 53, 614, 207, 126, 117, 12, 90, 194, 3524, 17, 253, 449, 217, 45, 34, 12, 3642, 11198], [13, 1, 1, 1, 1, 1, 1, 11, 7200, 1, 794, 1, 1, 1, 1], [1, 168, 25448, 58, 5661, 42, 33, 22700, 1106, 426], [146, 4624, 194, 227, 2, 89, 32, 2008, 23, 194, 641, 10677, 1116, 4681, 1520, 33, 59, 641, 481, 3, 1], [5, 166, 642, 1816, 154, 33, 913, 42, 15, 1], [1, 1, 1, 1, 794, 72, 4556, 133, 239, 11, 3978, 540, 12, 133, 127], [1, 316, 69, 1, 264, 161, 727, 82, 30, 24, 17, 4082, 24097, 4, 1287, 182, 4171, 1159, 16, 3818], [13, 1, 414, 62, 554, 304, 62, 55, 70, 6, 1273, 73, 123, 3125, 688, 18, 4290, 144, 1], [1, 46, 10, 2270, 18, 1053, 10, 2780, 28, 93, 574, 3416, 248, 100, 163, 85, 26, 7], [1, 1, 3405, 8731, 34, 11, 305, 19, 108, 608, 48, 5, 463, 358, 207], [295, 3614, 2146], [1, 2325, 78, 436, 19, 525, 298, 103, 226, 4, 208, 298, 26, 2325, 4, 3880, 2174, 77, 3669], [1, 22664, 1, 1, 1, 1, 1, 1, 5170, 89, 1418, 358, 2491, 3, 8, 196, 593, 758, 3, 8, 196, 593, 758], [13, 1, 5, 125, 206, 3223, 2, 1, 2551, 15393, 5553, 4255, 146, 19, 1018, 12581, 231, 29, 125, 206], [13, 1, 28, 120, 627, 7100, 97, 22707, 42, 5766, 1435, 7625, 7625, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 826, 694, 826, 694, 826, 694], [31, 817, 12, 124, 32, 7984, 38, 152, 9, 4721, 44, 4783, 817, 11, 1605, 14106, 2], [13, 1, 11503, 1889, 580, 5079, 54, 2095, 942, 1889, 71, 476, 2095, 611, 323, 2630, 3484, 2], [13, 1, 1, 5, 720, 16308, 37, 3, 8, 27, 50, 29, 5, 774, 934, 37, 3, 8, 37, 50, 94, 390, 347, 1585, 172, 6078, 735, 2813, 1546, 269, 1875, 1585, 94, 235, 45], [1, 145, 584, 1547, 471, 6, 16, 17, 128, 359, 9, 126, 279, 17, 14, 3230, 48, 5160, 12, 2930], [1, 1, 655, 60, 77, 2795, 7, 813, 2113, 2, 683, 86, 439, 813, 6, 42, 574, 1357], [13, 1, 289, 1729, 1, 1, 5, 20, 39, 222, 1501, 2534, 12, 1708, 446, 359, 84, 8175, 18, 1518, 1332, 32, 4626, 186, 6], [13, 1, 1260, 2497, 5, 5, 1195, 36, 403, 132, 7344, 1, 2019, 16076, 1707, 10, 6469, 132, 5], [1, 1, 814, 69, 18, 2349, 4, 205, 715, 6924, 48, 394, 1671, 16, 2, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 425, 299], [1, 2610, 7, 6752, 6, 595, 38, 39, 23, 19, 274, 8981, 45, 31314, 31484, 8250, 29, 408, 93, 29, 2980], [1, 4183, 1161, 391, 212, 248, 850, 9, 108, 48, 6, 1843, 181, 368, 1350, 4, 228, 2084, 1413], [13, 1, 13050, 5, 1, 1, 4437, 7, 5692, 19, 695, 5, 103, 39, 23, 9, 1223, 113, 129, 266, 7, 1683, 32, 23, 2680], [1, 1831, 17, 570, 779, 12, 674, 4, 776, 633, 44, 381, 3, 8, 37, 50, 381, 3, 8, 37, 50, 381, 3, 8, 37, 50, 381, 3, 8, 37, 50, 368, 4181, 3552, 2928, 51, 87, 145, 17], [180, 259, 47, 242, 65, 359, 197, 6376, 7583, 32, 5424, 95, 3747, 11, 185, 98, 161, 1673, 82, 26817, 1214, 886], [1, 284, 6953, 11, 57, 25, 11677, 40, 111, 284, 4216, 7, 2167, 174, 87, 521, 12601, 180, 1259, 5, 9], [46, 213, 33, 992, 326, 213, 31, 1507, 1099, 836, 46, 18, 80, 9395, 33, 992, 273, 236, 18, 70, 29, 16, 2091, 33, 7803], [1, 1, 15, 55, 1185, 328, 14, 146, 274, 283, 18, 234, 10, 216, 40, 18, 66, 31, 59], [30, 24, 20, 625, 40, 34, 32, 3584, 24, 7, 489, 903, 9, 87, 1995, 489, 2078, 5522, 188, 3584, 24, 308, 66, 1788, 40], [1, 466, 53, 16, 1808, 104, 47, 26, 51, 104, 47, 4, 1087, 16267, 32, 11153, 17, 10528, 51, 12], [1, 1, 11298, 268, 3856, 55, 15, 3480, 15, 467, 363, 93, 107, 168, 1743, 363, 107, 318, 28], [1, 7842, 1, 66, 10, 469, 1407, 2939, 859, 6545, 29812, 401, 2097, 273, 366], [52, 189, 562, 766, 1, 24, 3, 171, 141, 3, 171, 141, 3, 171, 141, 3, 171, 141, 3, 171, 141, 3, 171, 141, 3, 171, 141, 267, 267, 496, 191, 25, 707, 76, 61, 76, 61, 766, 24, 484, 68, 7301, 5139], [1, 78, 23, 4785, 3987, 3987, 261, 23, 192, 4478, 11785, 87, 64, 38, 147, 9139, 89, 6357, 108], [1, 24653, 18, 93, 3435, 58, 18, 102, 2687, 2156, 1509, 12, 32589, 126, 6, 37, 3, 8, 37, 50], [1, 5, 16, 43, 7, 5463, 16, 2137, 9, 824, 895, 1701, 12, 1450, 7, 43, 10270, 269, 2], [1, 452, 15, 468, 15, 7579, 647], [1, 1, 19106, 46, 982, 18, 14, 66], [1, 535, 5, 1, 414, 1979, 2205, 598, 192, 634, 4800, 4, 3970, 282], [1, 5586, 42, 1067, 68, 4758, 49, 4100, 6, 2230, 46, 1677], [1, 1, 1, 1478, 4367, 299, 2655, 25500, 3, 8, 79, 50, 3, 8, 79, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50], [1, 519, 62, 55, 8, 69, 31, 1370, 94, 122, 602, 345, 5363, 418, 148, 418, 148, 418, 148], [13, 1, 15790, 608, 85, 7, 5, 25, 209, 44, 2622, 525, 1, 1, 3, 8, 22, 10, 21], [13, 1, 7854, 228, 308, 4764, 88, 11, 7864, 1927, 432, 4632, 256, 196, 270, 249, 255], [1, 1016, 6134, 498, 25, 544, 3589, 26, 2720, 9810, 110, 8982, 7, 2371, 87], [1, 1, 1, 1, 242, 1033, 124, 7, 4276, 182, 87, 541, 69, 248, 2141, 3665, 11, 4739, 97, 2, 2324], [13, 1, 1723, 535, 10, 74, 7921, 63, 98, 491, 1298, 1138, 7921, 78, 23093, 17593, 537, 7922, 491, 11, 2811, 6], [16, 33, 622, 33, 508, 2288, 36, 58, 69, 692, 1174, 2, 199, 6, 963, 29809, 391, 2638, 2638, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 16, 69, 6, 876, 17, 44, 1060, 3, 1058, 3], [1, 1, 1, 78, 219, 2436, 455, 3322, 7, 2061, 17, 476, 2912, 365, 1631, 455, 24589, 4], [1, 282, 175, 379, 6, 26286, 33, 26598, 32101, 29, 348, 4968, 216, 344, 3, 8, 79, 50, 2607, 3489, 1544, 17, 192, 3428, 670, 2], [1, 1898, 6, 43, 17, 12471, 983, 138, 35, 24, 695, 43, 4, 7595, 1419], [1, 1, 439, 58, 18, 19676, 612, 256, 196, 256, 196, 1438, 4, 370, 735, 28400, 19, 269, 40, 394, 3085, 12, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50], [301, 52, 189, 33, 1807, 1, 338, 24, 301, 424, 27, 384, 160, 16, 166, 388, 918, 474, 11], [1, 146, 485, 394, 599, 39, 207, 9, 51, 322, 485, 172, 3744, 1094, 2358, 2, 485, 874, 626, 16, 23], [13, 1, 1832, 1929, 6772, 7051, 6772, 8, 35, 35, 26012, 1477, 6604, 1], [8436, 5944], [82, 522, 299, 778, 16, 83, 303, 115, 82, 19, 705, 152, 18, 82, 23, 2579, 113, 3412, 2617, 24], [1, 935, 72, 2035, 78, 147, 177, 4, 51, 1180, 1008, 600, 19339, 1695, 10, 74, 133, 963, 1013, 356, 2740], [1, 1, 1, 9422, 5, 1, 1, 1, 638, 293, 98, 7, 8204, 564, 631, 2], [1, 63, 578, 3730, 365, 6815, 103, 815, 19811, 659, 945, 4, 150, 279, 3653, 167, 16, 5197], [1, 5, 1, 1, 24, 97, 299, 150, 17, 6378, 217, 163, 26, 163, 94, 25, 10451, 792], [1, 26, 5493, 4053, 1003, 108, 136, 5, 510, 1012, 67, 696, 43], [13, 1, 333, 3, 171, 141, 3, 171, 141, 189, 562, 766, 1, 24, 3, 171, 141, 3, 171, 141, 3, 171, 141, 3, 171, 141, 3, 171, 141, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 267, 267, 496, 191, 25, 707, 76, 61, 76, 61, 334, 18, 33, 5, 314, 829, 1237, 1537, 1787, 3429, 148], [1, 1, 1, 20374, 127, 24305, 698, 992], [1, 1, 20, 11578, 17, 12479, 447, 1721, 347, 112, 11, 23863, 107, 1163, 7, 23864, 5576, 309], [1, 344, 57, 238, 114, 51, 118, 359, 309, 311, 19, 697, 2008, 17, 2206, 217, 3376, 3120, 108], [11621, 1, 1, 1460, 11, 716, 2993, 19, 365, 105, 29944, 393], [1, 77, 23, 931, 15803, 31, 1416], [1, 276, 69, 31, 119, 4063, 17, 77, 9, 53, 16, 107, 3951, 16, 2, 2115, 2966, 7263, 4, 2625, 9, 124], [13, 1, 276, 1, 1578, 43, 990, 1364, 44, 113, 113, 12, 110], [42, 33, 671, 8995, 10, 23228, 3690, 1793, 16, 29, 58], [1, 1440, 5106, 17, 1017, 20, 65, 3489, 192, 553, 4901, 1992, 9183, 47, 119, 1495, 170, 88, 431], [1, 2209, 9, 659, 19993, 170, 2, 74, 7902, 10, 11895], [340, 340, 28, 340, 3761], [1, 12, 5, 237, 16, 11, 1394, 9, 19, 2633, 7849, 5, 80, 957, 10196, 82, 5], [1, 147, 177, 11, 54, 225, 1461, 140, 1789, 7, 39, 319, 9, 109, 63, 65, 7, 1158, 5, 314, 6205, 8551, 6, 9], [13, 1, 20816, 6, 326, 213, 1458, 6, 42, 556, 2943, 49, 52, 36, 5392, 389, 451, 28, 334, 40, 6486, 14, 2026], [1, 2880, 6, 47, 20, 145, 998, 489, 1045, 103, 7, 38, 3905, 41, 555, 17, 162, 53, 16], [328, 273, 52, 1665, 3245, 533, 6, 2104, 304, 8906, 2104, 18, 52, 1703, 5082, 2546, 47], [13, 1, 1320, 22548, 1198, 4815, 8655, 583, 37, 3, 8, 27, 50], [11844, 1710, 3061, 13313, 447, 9, 11017], [363, 49, 70, 1525, 805, 4480, 165, 130, 3, 294, 27, 52, 483, 33, 560, 613, 3, 294, 27, 1, 26368, 5, 59, 33, 2522, 18, 8723, 37, 3, 8, 1113, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50], [1, 1, 74, 7169, 4, 1495, 7647, 9, 354, 825, 3, 8, 22, 10, 21], [20, 2, 537, 26, 1811, 11, 12189, 26, 239, 1334, 4, 57, 7, 2027, 56, 229, 368, 293, 4, 30573], [20, 1744, 570, 1015, 3, 1015, 3, 1015, 3, 1015, 3, 1015, 3, 1015, 3], [13, 1, 6489, 2221, 889, 281, 6, 59, 3582, 28, 6520, 159, 94, 1807, 160, 15, 28, 1491, 59, 68, 6473], [1, 362, 10, 14, 52, 1300, 6, 518, 117, 70, 18, 4, 115, 728, 263, 697, 26, 2132, 5, 6168, 5, 23], [1, 20, 6, 30, 24, 2862, 2285, 8637, 40, 1, 24, 336, 3962, 17, 3618, 40, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 34, 30, 801, 4], [143, 6, 33, 1216, 10490, 28, 1099, 20238, 5, 15, 419, 58, 175, 114, 4442, 15, 719], [1, 1, 5, 6797, 16, 5, 824, 1278, 16, 44, 11034, 468, 3739, 2685, 10, 56, 16, 5, 2, 20, 1476, 10939, 16, 44], [1143, 3710, 6, 2040, 28, 118, 137, 1402, 15, 70, 80, 101, 352, 14, 328, 6, 518, 68, 19508, 29, 15, 86, 15, 4979], [1, 594, 7697, 43, 11, 183, 25, 818, 3301, 2, 321, 13559, 9, 374, 17, 443, 5309], [11688, 283, 24, 32, 152, 57, 1757, 11, 31273, 437, 126, 274, 3893, 85, 7, 283], [13, 1, 14, 428, 33, 1173, 1121, 16, 213, 6297, 3175, 460, 49, 173, 1842, 73, 556, 109, 1851, 1465, 656], [1, 1, 1, 784, 3333, 83, 303, 115, 986, 2467, 106, 558, 4, 83, 778, 4], [1, 3156, 2221, 35, 5, 17542, 251, 19, 262, 47, 2335, 836, 3242, 1938, 1709, 6488, 11, 503, 153, 23, 8710, 9, 395], [1, 1, 20, 540, 3612, 9, 19, 3342, 26, 479, 491, 13796, 6, 20, 11705, 23915, 3135, 208, 1620, 491], [1378, 517, 170, 18, 7699, 29, 1178, 77, 4, 4843, 17, 5, 9634, 279, 4, 650, 3086, 2, 45, 77, 4, 167, 4, 798, 5], [1, 1, 2641, 1763, 15, 2699, 1324, 4471, 36, 14, 223, 59, 52, 566, 1380], [1, 1583, 914, 59, 10, 14, 27, 103, 51, 23, 1207, 262, 4222, 24, 62, 4681, 246, 10, 3667, 11, 40, 367, 196, 367, 196, 367, 196, 367, 196], [1, 142, 15, 1775, 28120, 3563, 36, 1464, 947, 101, 174, 3568, 28121, 2, 26, 95, 28122], [13, 1, 1, 1, 1, 77, 65, 136, 2920, 263, 525, 2, 219, 98, 7, 10123], [193, 18, 3245, 103, 1941, 89, 750, 2, 103, 21164, 89, 762, 2, 118, 6, 22117, 2, 757, 247, 6, 691, 2, 826, 694, 694, 52, 477, 2143, 3, 2143, 3], [1, 46, 18, 49, 49, 49, 36, 1667, 1174, 144, 58, 18, 339, 432], [1, 4076, 731, 3, 1430, 555, 2577, 39, 9, 731, 3, 71, 9, 241, 6, 122, 54, 259, 5543, 3], [1, 1, 1971, 1, 96, 18785, 277, 32148, 476, 2, 521, 2043, 103, 5193, 550, 241, 2, 431], [1, 1, 1, 48, 2946, 63, 1608, 69, 39, 3128, 9, 60, 947, 47, 118, 3892, 63, 23, 32401, 416, 19, 5], [1, 3679, 23881, 3077, 80, 23881, 439, 240, 1083, 2, 1031, 78, 1963, 383, 224, 275, 29, 2390, 325], [1, 2643, 750, 18, 1022, 503, 11121, 16, 729, 311, 12964, 16022, 28, 202, 767, 2200, 75, 27, 771, 199, 19, 141, 661, 424, 27], [66, 10, 469, 29, 14, 607, 39, 3173, 131, 8089, 3508, 426, 607, 16, 4823, 4, 2941, 1519, 4076, 3660], [155, 7, 1308, 265, 87, 2730, 12, 688, 215, 265, 2621, 126, 20, 207, 43, 45, 4983, 7, 470, 11545, 197, 4, 9987, 6, 9, 314], [1, 2641, 1730, 3, 86, 1285, 280, 2885, 1254, 1528, 960, 3955, 188, 765], [13, 1, 211, 6, 338, 62, 99, 462, 52, 759, 6242, 2602, 94, 1, 10572, 7689, 172, 67, 262, 24, 37, 3, 401, 49, 2353, 326, 59], [1, 347, 1771, 19749, 11, 10496, 2, 31, 2344, 3152, 11, 23, 19, 83, 298, 80, 319, 218, 582, 1, 26], [1, 1, 594, 413, 660, 4746, 43, 676, 271, 11, 183, 39, 23, 51, 322, 2, 116, 98, 51, 416, 2], [1, 276, 1, 6749, 5120, 23, 378, 89, 6903, 370, 2, 3, 8, 22, 10, 21], [1, 34, 23, 336, 3254, 4, 127, 19, 651, 358, 9, 229, 34, 7, 374, 17, 1077, 19, 1010], [1, 1, 1, 683, 23, 38, 43, 11, 888, 138, 1337, 1080, 5, 181, 589, 433, 522, 16, 5563, 135], [33, 66, 508, 960, 587, 28, 363, 49, 1445, 6, 99, 1639, 324, 3121, 28, 4565, 46, 3245, 2369, 246], [1, 38, 170, 297, 247, 30537, 122, 32, 219, 23, 302], [13, 1, 58, 1960, 14, 66, 820, 536, 3847, 528, 249, 255], [1, 823, 7787, 2913, 3333, 24, 295, 868, 115, 295, 868, 115, 295, 868, 115, 295, 868, 115, 295, 868, 115, 295, 868], [1, 8173, 5, 1, 4065, 41, 10718, 12345, 3996, 1198, 551, 110, 17548, 32, 25, 85, 7, 18, 1955], [1, 1, 1185, 460, 80, 406, 45, 36, 33, 1528, 5, 1211, 2945, 224, 146], [1, 39, 150, 9, 1193, 1595, 354, 4, 39, 1636, 9, 2, 38, 665, 129, 2, 44, 71, 1830, 652, 11, 2590, 6986], [7547, 1404, 3, 1404, 3, 1064, 262, 19, 342, 120, 349, 1699, 23, 282, 1404, 3, 1126, 1436, 3, 1], [70, 81, 6, 14, 372, 341, 1, 5138], [259, 2, 122, 882, 811, 56, 17, 3835, 1100, 7977, 226, 811, 56, 17, 14, 13057, 12128, 73, 213, 142, 556], [1, 767, 70, 202, 35, 94, 235, 1981, 691, 1741, 1226], [13, 1, 1, 52, 477, 766, 24, 1561, 1834, 24, 384, 559, 834, 25156, 338, 24, 42, 338, 24, 712, 4875], [39, 140, 839, 4, 34, 775, 9, 6, 3507, 23, 39, 1287, 182, 1582, 6, 544, 1446, 625, 34, 131, 625, 450, 98, 2, 38, 431], [329, 329, 143, 24, 120, 34, 32, 53, 4, 26, 624, 2081, 1647, 8351, 1070, 456, 217, 53, 7], [1, 1, 54, 2020, 6300, 67, 12966, 761, 12, 185, 67, 178, 205, 10256, 364, 48, 94, 1553, 99], [1, 1484, 1, 218, 1832, 1190, 272, 375, 13171, 13884, 24164, 1832, 24, 15, 175, 484, 1344], [1271, 12, 90, 2594, 4, 9, 1319, 2, 100, 163, 12625, 4, 798, 5, 798, 5, 2394, 12626, 5, 24548, 5], [1, 1, 20, 1712, 65, 106, 16, 9, 2113, 6, 106, 5, 996, 153, 2919, 9, 2113, 2213, 26, 279, 25, 721, 1230], [1, 1, 35, 62, 99, 1837, 29, 1597, 235, 349, 12, 1224, 12, 90, 264, 83, 360], [13, 1, 329, 143, 6, 244, 1, 24, 36, 291, 1227, 2279, 144, 66, 520, 3144], [1, 25940, 1, 1, 565, 1, 1, 20, 201, 716, 1679, 11980, 43, 12, 2], [1, 5, 19316, 2956, 4776, 7154, 5054, 100, 39, 26, 9, 7275, 4293, 1333, 3], [1, 1, 163, 4367, 197, 17, 9, 595, 67, 1795, 138, 22911, 29211, 3143, 9, 526, 3488], [13, 1, 4133, 12646, 11, 16049, 689, 393, 15], [13, 1, 258, 198, 75, 27, 75, 27, 3, 171, 141, 3, 171, 141, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 4422, 15, 59, 1880, 1834, 31884, 256, 196, 256, 196, 418, 148, 418, 148, 1, 1, 1, 1], [1, 1, 1, 9, 69, 204, 12785, 16, 6, 9, 14, 48, 20, 24077, 2, 5530, 20, 3553], [1, 1, 143, 30, 24, 6, 2065, 123, 116, 1206, 652, 83, 360, 195, 1943, 631, 702, 17, 420, 264], [1, 1, 1154, 149, 325, 2241, 768, 5, 1542, 18, 238, 11926, 5496, 10, 6535, 5496, 10, 6535, 5496, 10, 6535, 20, 4166], [1, 1486, 228, 3171, 16, 890, 278, 269, 2, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 30, 24, 299, 1532, 60, 383, 183, 51, 269, 2, 82, 657, 57, 18614, 2, 4, 9723, 16, 119, 727], [1, 752, 49, 22481, 1, 18, 1012, 101, 951, 566, 3511, 13019, 30530, 89, 1071, 509, 40], [1, 1, 1, 5, 69, 125, 206, 3192, 3982, 13244, 167, 6508, 1073, 66, 2088, 5, 1254], [1, 1, 20, 1653, 12, 3933, 12772, 65, 12772, 7, 174, 386, 2, 43, 7, 152, 9, 5669], [39, 131, 600, 175, 14, 403, 361, 78, 11994, 2, 2, 183, 191, 147, 4077, 361, 26, 23, 47], [46, 201, 2845, 6, 72, 36, 33, 530, 1787, 60, 60, 22341, 18, 1858, 12158, 22342, 7, 613, 3, 613, 3, 3413, 548, 26], [354, 4, 9, 24, 666, 230, 32, 5, 463, 7551, 11, 8675, 87, 2, 5, 1602, 167, 12963, 1010, 17], [1, 1, 4787, 5, 1, 1, 1, 1, 34, 161, 12752, 67, 886], [1, 2143, 31780, 17, 272, 2, 34, 22399, 26, 1686, 985, 7, 808, 586, 6103, 29213], [106, 88, 498, 263, 597, 2, 53, 88, 116, 6120, 263, 2, 2198, 1, 1], [790, 689, 15316, 325, 33, 32008, 563, 2336, 1037, 86, 376, 1029, 348, 9748], [1, 14, 28, 5787, 2985, 55, 2021, 656, 6141, 55, 1018, 15, 960, 146, 421, 8, 59, 312], [1, 1, 8854, 1738, 991, 33, 982], [452, 15, 511, 59, 3987, 4589, 2407, 11832, 4031, 1768, 6, 1469, 6, 16, 326, 251, 14602, 18, 167, 469, 5495], [1, 1, 1254, 89, 21049, 285, 1, 26898, 4691, 12, 90, 65, 41, 835, 54, 644, 3, 1, 425, 181, 875, 109], [1, 6, 95, 3677, 7, 95, 3448, 17, 598, 541, 44, 71, 98, 181, 298, 6, 593, 181, 181, 298, 26, 19, 1974, 298], [1, 958, 27937, 102, 70, 81, 37, 3, 8, 37, 50, 81, 320, 159, 94, 160, 68, 353, 69, 27, 441, 139, 56], [162, 1, 32, 6993, 4, 78, 8333], [1, 34, 11, 796, 2101, 1069, 23, 158, 2, 1924, 3667, 194, 637, 311, 15425, 26, 5, 238, 290, 34, 161, 736], [1, 35, 12229, 17, 39, 207, 41, 138, 2194, 770, 5973, 289, 43, 302, 71, 34, 4, 209, 7], [1, 49, 70, 6, 154, 15, 2022, 35, 31, 5, 223, 159, 15, 3324, 7079, 11798, 35, 95, 8915], [1, 1, 1, 1, 754, 857, 2447, 586, 3137, 3130, 30, 1755, 111, 9, 601, 84], [1, 1, 1, 1, 5921, 163, 9332, 2, 1551, 9, 856, 728, 12, 1046], [5, 9353, 5, 566, 5886, 29827, 5, 14917, 231, 29434, 5, 80, 319, 984, 5], [1, 247, 2062, 1442, 777, 40, 959, 3411, 2831, 881, 16295, 3411, 2, 20, 98, 40, 2062, 12913, 1800, 32, 535], [1, 35, 46, 18, 224, 432, 88, 5500, 89, 823, 795, 103, 237, 103, 28933, 853, 186, 2, 140, 3780, 422], [1, 15, 55, 634, 641, 1315, 19, 28209, 19, 647, 624, 16, 99, 68, 1152, 11853, 379, 2687, 473], [296, 412, 296, 475, 296, 1104, 29212, 106, 367, 196, 367, 196, 1056, 1056, 1055, 837, 1055, 837, 56, 56, 219, 57, 226, 371, 30, 1532, 43, 4983, 5, 5, 37, 3, 8, 27, 50], [1, 413, 1158, 32563, 20, 374, 17, 41, 7772], [13, 1, 1, 1, 9427, 1, 275, 350, 170, 59, 14, 66, 28, 33, 2866, 1048, 1048, 9617], [1, 331, 7811, 6055, 12458, 7, 6, 155, 354, 12, 22430, 10256, 158, 64, 8696, 209, 455, 6, 220], [13, 1, 211, 35, 62, 59, 831, 335, 31, 19955, 235, 1930, 271, 801, 215, 116, 11, 60, 60, 6047, 194], [13, 1, 222, 42, 5589, 375, 679, 574], [120, 23, 6, 683, 5951, 7, 585, 4589, 229, 606, 225, 6739, 16, 262, 180, 120, 107, 12, 328, 10, 4707, 67], [1, 1, 512, 210, 17937, 19, 108, 2, 3, 8, 22, 10, 21, 79, 36, 14, 92, 91, 3, 8, 22, 10, 21], [1, 1, 35, 1412, 227, 2, 330, 722, 2, 17, 1901, 19, 212, 24602, 82, 27527], [1, 1074, 33, 353, 2331, 59, 22968, 315], [13, 1, 5, 7919, 1, 628, 1066, 1994, 18, 102, 3358, 29, 117, 1, 1605, 2852, 10653], [7303, 1092, 9, 449, 18, 53, 11, 2349, 4217, 461, 337, 5861, 40, 248, 151, 16, 22105, 4850, 3986, 40, 17765, 17765], [325, 14, 452, 31, 8047, 20471, 5919, 5, 1095], [327, 295, 13253, 335, 18, 1139, 36, 68, 808, 327, 295, 13253, 14, 23955, 1614, 6741, 4680], [1, 1, 147, 110, 4379, 203, 30, 7, 11474, 25, 19, 183, 6825, 77, 841, 9, 26], [20, 1394, 350, 2210, 11, 1069, 3088, 16, 23, 2883, 4, 497, 186, 45, 787, 210, 39, 41, 7357, 23, 41, 1149], [1, 1, 115, 4477, 12618, 2832, 2873, 24, 8651, 5015, 687, 5123, 24, 3741], [1, 85, 286, 103, 391, 6, 43, 7, 23, 30537, 229, 85, 12, 989, 3963, 9, 26, 43, 12, 90, 7], [1, 97, 6378, 217, 180, 4, 20, 6, 13510, 158, 663, 773, 947, 82, 43, 7, 3880, 41, 444, 45, 577, 17, 156, 391, 392, 61, 392, 61, 392, 61, 392, 61], [2638, 2638, 4213, 525, 3387], [1, 52, 469, 9042, 234, 58, 59, 5347, 218], [1, 1, 1, 1, 8920, 1, 1, 1, 2487, 1, 1], [1, 3881, 247, 3180, 2394, 4, 20172, 4, 5019, 2769, 4476, 7, 227, 186, 2, 38, 2645, 5019, 97, 5528, 47], [204, 53, 11], [13167, 1705, 8, 17712, 4471, 36, 33, 6566, 168, 31, 439, 7205, 137, 4274], [1, 1, 18964, 28072, 257, 1614, 8, 8000], [1, 1, 1, 1031, 1891, 20, 282, 2, 65, 425, 9, 1024, 1891, 20, 2, 347, 4550], [5, 1950, 2486, 542, 233, 17752, 36, 373, 5, 2734, 614, 5, 3409, 614, 5, 19193, 25], [1, 144, 15, 142, 1125, 31873, 59, 312, 7257, 3002, 134, 15, 2258, 27, 1326, 8764], [211, 6, 1, 40, 202, 1402, 184, 29, 14, 2187, 2985, 432, 28, 2373, 2086, 3701, 181], [1, 5724, 6, 124, 7, 1234, 45, 4019, 3854, 3008, 24481, 41, 38, 1075, 365, 71, 1613, 19, 1229, 1258, 1058, 3], [1, 5, 4700, 7738, 2912, 782, 6, 232, 2, 5430, 78, 316, 2554, 64, 1621, 710, 8114, 284, 56, 11], [57, 19450, 17, 627, 4040, 5, 7361, 7630, 20, 245, 12, 4, 4, 209, 2794, 6, 3848, 51, 221, 32], [1, 62, 55, 3599, 15, 36, 433, 2673, 4222, 24, 11, 2298, 9892, 4714, 394, 1802, 40, 5003, 2], [1, 1, 5092, 5, 1, 1, 1, 1, 1, 1], [326, 6, 1736, 55, 2057, 5, 892, 473, 712, 964, 12640, 11, 285, 11777, 5, 510, 473, 105, 3216, 5, 892, 473, 180, 5, 510, 473, 55], [236, 18, 93, 1850, 4061, 45, 1452, 236, 18, 1844, 933, 867, 75, 27], [1, 1, 1, 9596, 1, 1, 20, 6, 147, 3805, 88, 4914, 1425, 108, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 764, 149, 3, 348, 8832, 42, 15], [1, 1, 19, 34, 4239, 26, 720, 6, 59, 432, 1790], [13, 1, 720, 23113, 8595, 7304, 858, 3611, 15, 960, 218, 720, 1867], [1, 3152, 25, 788, 2092, 2962, 34, 7, 39, 945, 9, 169, 136, 3152, 7, 443, 945, 169, 34, 7, 1365, 207], [13, 1, 24760, 24761, 123, 2185, 1342, 31, 14, 5, 518, 2117, 146, 942, 2312, 901, 415, 2778, 16, 696, 2117], [1, 2002, 836, 6357, 12, 57, 1892, 7, 5085, 64, 5096, 291, 486, 419, 804, 10, 33, 66, 508, 3, 8, 22, 10, 21, 96, 1722], [31, 6371, 88, 2174, 832, 233, 32, 6381, 553, 23, 9, 3203, 11, 80, 2305, 86, 1204, 5, 6317, 32, 23, 277, 9, 1720], [1, 1036, 35, 62, 1029, 31, 68, 4551, 28, 520, 118, 58, 35, 15, 149, 652, 118, 421, 29, 14], [13, 1, 336, 2183, 11678, 20336, 12, 702, 17, 194, 299, 156, 4, 420, 264], [13, 1, 4616, 5, 5171, 4, 1088, 9513, 572, 4, 442, 32, 11102, 5530, 442, 11, 412, 9, 87, 45, 2098, 9, 4, 215, 1091], [13, 1, 1, 7802, 178, 2084, 113, 2128, 16, 5245, 5546, 4, 6752, 5, 463, 19, 2150, 2, 56, 16, 23, 158, 6], [1, 2227, 233, 25, 27068, 16896, 64, 30, 11, 38, 653, 7921, 12105, 952, 117, 7, 84, 19798, 167], [1, 1310, 374, 88, 195, 127, 404, 332, 140, 10498, 308, 1761, 624, 6159, 6, 34, 11, 34], [1, 1, 1, 1, 1, 1, 1, 1260, 15184, 24], [137, 68, 688, 2312, 16, 25752, 5304, 114, 56, 15, 3365, 3, 8, 1121, 1659], [1, 1660, 522, 16, 1795, 180, 710, 821], [1, 120, 11, 29404, 1550, 51, 358, 395, 4, 3487, 11, 18252, 32315, 6617, 25, 9, 1940, 3971, 16, 680], [1, 1384, 15, 20, 13135, 4, 762, 2, 4294, 751, 3918, 19, 453, 363, 93, 382], [204, 53, 16, 145, 2849, 2, 25, 9, 32222, 16, 32223, 9, 2, 6, 29, 20, 2466, 2466, 2198, 1141, 11, 22451], [13, 1, 42, 7897, 611, 58, 73, 339, 1697, 29, 676, 6, 28413, 1099, 32310, 144, 902, 144, 40, 23691], [1, 1, 6087, 272, 375, 1621, 2730, 953, 26186, 29, 68, 992, 2730, 605, 29], [1, 1359, 89, 29056, 156, 4, 420, 253, 45, 308, 659, 762, 4, 150, 85, 4, 1359], [1, 7734, 1, 15, 55, 3844, 623, 499, 15, 144, 33, 2277, 488, 1241, 16, 853, 8471, 3154, 4, 727, 77, 26, 1325], [1, 1, 1, 233, 1, 1211, 18, 755, 6, 72, 272, 62, 149, 154, 4609, 7792, 5172, 20868, 3457, 1867], [1, 101, 10, 14, 1689, 3879, 409, 10, 85, 497, 257, 114, 46, 1654, 3, 227, 9, 274, 881, 18, 7, 487, 22790, 104], [1, 1208, 34, 7, 5786, 156, 4, 828, 5, 253, 947, 30, 7, 579, 3431, 1953, 215, 34, 12], [1, 1, 48, 133, 5237, 227, 2749, 145, 594, 12, 16, 10662, 138, 471, 78, 98], [13, 1, 1252, 2591, 279, 25, 782, 87, 125, 206, 818, 2, 1035, 27], [1, 729, 4786, 25, 47, 18, 98, 25, 411, 241], [4364, 22084, 26, 4364, 545, 1618, 25, 3566, 4, 4782, 5082, 298, 20, 74, 760, 5, 1983, 1215, 4], [13, 1, 48, 20, 5245, 797, 32, 4, 1506, 5, 237, 17, 43, 4, 209, 220, 26, 30, 116, 64, 26, 117, 1150, 25, 2114, 3195], [1, 1, 1, 62, 142, 2356, 33, 2157, 8893], [1, 1, 460, 186, 2, 590, 19, 733, 144, 925, 71, 8563, 12557, 4, 12874, 11034, 282], [1, 331, 6, 78, 20, 924, 2, 12, 30, 4, 2451, 240, 2, 331, 3017, 12, 734, 43, 45, 15260, 85], [1, 4787, 5, 1, 1, 35, 34, 23, 1365, 183, 1947, 163, 43, 11, 4458, 197, 17, 77, 9], [13, 1, 5537, 5538, 52, 189, 496, 1, 24, 160, 596, 8, 2457, 388, 977, 474, 28], [1337, 651, 586, 8551, 122, 110, 39, 9, 2, 26, 122, 7, 2428, 153, 9, 481, 3, 3, 8, 8451, 358, 19, 25, 9, 8037, 2947, 3, 8, 22, 10, 21, 49, 5188], [1, 1318, 2326, 2463, 9, 1737, 298, 4631, 215, 2782, 4631, 157, 25971, 4, 393, 2, 71, 6756, 13518, 1294], [1, 191, 177, 32, 1796, 760, 7060, 19, 365, 12, 11052, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [13, 1, 45, 21472, 795, 21473, 108, 154, 80, 543, 605, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 236, 18, 462, 482, 7567, 21474, 258, 198, 151, 311, 7097, 295, 37, 3, 8, 27, 50, 151, 310, 568, 4014, 148, 1342, 4014, 148, 1342, 408, 324, 135], [13, 1, 6943, 2604, 158, 39, 150, 9, 74, 5, 16, 59, 357, 19, 108, 316, 12, 864, 395, 3, 8, 22, 10, 21], [1, 1, 435, 97, 1753, 8538, 298, 9629, 131, 449, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 4, 7, 3852, 321, 119, 11, 1138, 155, 64, 693, 90, 284, 15117, 25422, 32, 2290, 4949, 4, 2014], [1, 1, 74, 16, 15287, 17450, 108, 658, 635, 319, 1541, 14, 3542, 18, 2547], [1, 1, 1, 9, 93, 1794, 26, 60, 648, 2705, 1293, 539, 957], [1, 547, 20, 430, 54, 232, 2, 547, 16, 1], [1, 687, 142, 833, 15, 696, 1589, 68, 42, 3402, 3254, 8, 33, 27, 488], [1, 21125, 178, 205, 21126, 16, 1144, 32, 5, 107, 21127, 7, 3806, 12, 668, 5473, 215, 5040, 9914], [13, 1, 909, 198, 339, 33, 1477, 5584, 46, 1566, 10793, 214, 93, 144, 4650, 144, 1459, 6, 72, 731, 3, 2169, 8, 16, 8, 13425, 31, 1370, 94, 37, 3, 13621], [1, 3032, 489, 3608, 106, 209, 5270, 2210, 16, 2883, 4, 650, 692, 287, 135], [1, 1208, 1, 68, 7088, 166, 624, 29, 15, 275, 80, 275, 86], [1, 706, 505, 257, 99, 1967, 71, 77, 830, 25, 1057, 64, 6, 1967, 245, 87, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 6437, 1701, 26, 43, 12, 4, 2231, 16, 44, 922, 1087, 164, 2, 351, 36, 73], [1, 1, 1, 1, 1, 724, 1, 374, 2968], [1, 66, 1002, 10, 14, 451, 1268, 114, 5515, 89, 10886, 1832], [26, 10752, 1186, 4721, 287, 19, 365, 33, 4721, 29, 276, 1353, 835], [1, 351, 36, 15, 68, 353, 347, 3081, 185, 67, 884, 51, 1133, 19, 621, 22436, 5591, 2521, 23, 1257, 19, 63, 65], [401, 7, 46, 18, 451, 10, 5055, 29, 16, 3, 8, 22, 10, 21, 42, 5, 37, 3, 8, 37, 50, 37, 3, 8, 1113], [1, 1, 1, 20, 16964, 1918, 961, 32, 98, 684, 589, 9257, 2, 667, 111, 215, 832], [1, 5, 1, 516, 2, 96, 261, 606, 17, 1149, 509, 96, 261, 77, 443, 40, 2765], [1, 18, 7466, 67, 38, 2618, 2, 1229, 1493, 895, 122, 389, 649, 1123, 3, 1123, 3, 20, 65, 1663, 11035, 25, 9, 386], [1, 65, 6229, 43, 7, 1488, 1313, 3131, 128, 7, 23, 330, 4, 727, 5088, 188, 278, 695, 3950, 18897, 7, 25593, 54, 430, 232, 2], [1, 201, 609, 1, 1, 40, 18, 318, 547, 801, 4, 730, 9, 2, 621, 212, 30, 2357, 16], [13, 1, 1, 1, 1, 1, 1, 454, 32, 4, 11221, 1220, 18, 14, 1791], [13, 1, 1, 52, 189, 766, 24, 819, 160, 16, 388, 2499, 28, 474, 2500, 24, 2024], [1, 1, 211, 1902, 25, 31059, 1577, 6, 154, 15, 317, 357, 1805, 6, 518, 68], [1, 18, 11, 490, 25, 62, 99, 6, 706, 5, 45, 5, 524, 145, 1045, 12, 5, 136, 4906, 5, 2480, 3982, 19, 1974, 1024, 28, 375, 187, 55], [173, 42, 6, 33, 1407, 139, 3867, 1621, 1, 29, 13773, 16, 6, 10, 190, 486], [1, 94, 19308, 160, 15, 202, 2936], [13, 1, 1, 228, 1442, 64, 162, 1423, 2174, 9959, 4301, 64, 5, 19549, 11], [1, 625, 65, 7, 4328, 6153, 12, 332, 378, 51, 416, 140, 654, 16, 1723, 241, 6, 135], [1, 6021, 119, 235, 4651, 4, 82, 7186, 217, 45, 503, 11, 380, 3894, 1970, 82, 2915, 6914], [85, 500, 1651, 93, 5561, 73, 187, 847, 611, 10, 274, 283, 315, 145, 153, 85, 5, 947, 9, 19, 939, 1180], [352, 12678, 11178, 357, 10, 1129, 1393, 17, 872], [13, 1, 1, 6278, 2756, 69, 561, 34, 7, 43, 4, 156, 82, 60, 60, 264, 48, 10422, 3418, 97, 12, 7791, 51], [1, 58, 965, 72, 1697, 252, 376, 15, 1239, 272, 12508], [1, 4588, 42, 15, 49, 173, 258, 198], [1, 1, 2808, 1, 1, 534, 103, 16, 23, 156, 2, 861, 385, 2129, 12, 157, 509, 26], [1139, 93, 8892, 328, 31, 33, 2950, 28, 224, 376, 250, 137, 58, 18], [1, 1, 1, 1, 1, 234, 1897, 170, 352, 14, 3572, 73, 15, 99, 36, 348], [1, 1, 1192, 9, 980, 1815, 1081, 650, 63, 124, 4, 85, 26, 1539, 110, 27898], [1, 592, 654, 245, 129, 2, 1102, 129, 2, 2372, 5128, 1156, 1512, 2, 1], [1, 1096, 111, 4, 1081, 20623, 2, 25192, 25192, 12, 30, 6411, 1522, 816, 12184, 2, 44, 4, 98, 4, 2243, 44, 4], [1, 1, 575, 588, 216, 660, 93, 325, 3433, 39, 43, 3283, 1098, 3433, 21316, 44, 7024], [13, 1, 13, 243, 15, 1794, 1, 58, 213, 366, 17, 60, 60, 1647, 948, 10901, 5], [13, 1, 4137, 277, 7626, 169, 225, 232, 26, 195, 17, 282, 969], [911, 51, 3470, 374, 7, 53, 12, 82, 5299, 12, 90, 2585, 51, 3470], [1, 83, 19, 33, 531, 33, 2105, 612, 367, 35, 384, 982, 370, 29294, 2, 1160], [1, 10207, 5, 4087, 11, 64, 627, 8507, 797, 1118, 4, 23657, 1707, 1449, 302, 29651, 758, 3, 2660, 3, 3, 8, 22, 10, 21], [13, 1, 334, 62, 901, 12331, 1832, 115, 6757, 16513, 14, 953, 10, 350, 14, 11812, 10, 817, 62, 925, 7184], [1, 35, 5, 892, 10, 59, 143, 35, 34, 57, 822, 35, 57, 1818, 6099, 51, 118, 1217, 39, 23, 409], [1, 136, 526, 109, 95, 4, 735, 23767, 136, 979, 7, 4476, 4, 3224, 1332, 859, 84, 118, 1143], [136, 221, 2529, 27755, 1216], [4376, 160, 16, 28, 59, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 76, 61, 802, 27, 802, 27, 802, 27, 802, 27, 802, 27, 802, 27, 802, 27, 802, 27, 802, 27, 802, 27, 3654, 189, 19, 977, 29, 628, 1066, 76, 61, 802, 27, 1, 24, 4, 1000], [1, 1381, 1, 1, 144, 40, 738, 7, 328, 541, 190, 12, 39, 13762, 1257, 608, 6601, 19, 1974, 11, 34, 222, 1257, 19], [1, 5724, 1, 1, 109, 732, 5, 5, 785, 2, 402, 344, 1, 4, 356, 23, 4716, 9, 4], [2568, 531, 276, 18, 378, 28, 31030, 174, 427, 13, 23, 390], [1, 5994, 1, 2107, 1, 85, 1030, 866, 1030, 570, 72, 32220, 67, 6463, 100, 866, 4, 976, 1604], [1, 5559, 1130, 21031, 2, 1695, 3172, 651, 1760, 32, 3296, 9, 21032, 21033, 5340, 3120, 129, 2, 96, 2708], [1, 1, 6061, 1, 17745, 12, 11859, 133, 161, 5, 237, 17, 287], [1, 95, 13, 41, 4, 161, 2306, 3, 8, 79, 50, 180, 481, 3, 8786, 5543, 3, 5543, 3, 14707, 7, 227, 430, 697, 19323, 3, 3527, 307, 3, 8, 260, 500, 5999, 759, 176, 3, 8, 37, 50, 176, 3, 8, 37, 50, 176, 3, 8, 576, 176, 3, 8, 576], [1, 1, 1, 794, 57, 447, 12479, 2, 39, 293, 1182, 13939, 633, 38, 88, 457], [13, 1, 958, 244, 1, 1036, 102, 70, 81, 33, 1387, 29, 902, 28, 2613, 193, 68, 9664, 28, 858, 421], [1, 1, 20, 53, 4, 2450, 498, 38, 116, 302, 4613, 2450, 11, 174, 23, 41, 5, 16], [13, 1, 11, 23, 410, 40, 22466, 287, 169, 2, 5, 463, 19, 365, 190, 131, 225, 269], [30, 24, 1085, 5, 1195, 12481, 12, 6694, 17, 450, 1130, 1406, 57, 4408, 354, 12, 22430, 2301, 1153, 2487], [1, 35, 83, 558, 254, 28, 720, 29, 29, 845, 66, 29, 1652, 115, 728, 28, 1535, 83, 2223, 83, 303, 115], [1, 30, 24, 7, 720, 13070, 30, 24, 20, 1066, 28207, 100, 97, 936, 2262, 150, 729, 57, 25, 150, 1057, 6787], [49, 432, 1, 1, 7937, 62, 340, 15, 798, 5, 978], [1942, 1524, 190, 139, 14, 2108, 1216, 248, 100, 943, 19, 578, 1963, 119, 178, 19, 578, 6048, 1942, 629, 39, 44, 29616], [998, 489, 24, 122, 2349, 16738, 34, 17, 60, 1070, 51, 631, 40, 4, 34, 5, 4, 1745, 1745, 466, 360], [1, 1, 7526, 1, 1, 71, 1450, 2, 57, 28072, 2, 242, 43, 12, 5, 1728, 12420, 2, 385, 5238, 2, 54], [1, 647, 99, 6, 6968, 600, 256, 6, 93, 72, 256, 1887, 16, 243, 379, 6, 1274, 510, 80, 647, 467], [1, 69, 561, 394, 45, 394, 3606, 45, 534, 43, 111, 12, 90, 6, 204, 1, 1, 1534], [48, 20, 634, 6, 182, 7689, 3580, 5370, 136, 26927, 488, 58, 18, 1908, 73, 243, 187, 257, 114], [1, 1, 1, 1, 368, 911, 84, 77, 1232, 663, 3036, 45, 107, 16351], [1, 5, 414, 18, 2780, 4516, 29, 15, 743, 61, 68, 252, 214, 28377, 29], [1, 2385, 1, 1028, 116, 30, 24, 7, 1611, 313, 217, 26, 3413, 17, 32255, 313, 217, 4, 956, 1028, 53], [1, 674, 447, 18, 11179, 2592], [1, 30, 24, 53, 12, 1299, 1606, 36, 7, 3515, 53, 98, 431, 98, 1309, 697, 34, 12, 116, 2287, 89, 3531], [20, 2, 29887, 111, 164, 20, 201, 2, 732, 90, 481, 3, 331, 53, 4818, 48, 6, 2115, 23, 4301, 7743, 11, 398, 78], [1, 9703, 745, 677, 678, 328, 301, 301, 143, 370, 370, 253, 26, 264], [1, 5, 49, 341, 58, 213, 8230, 231, 179, 33, 486, 3291, 310, 5891, 29736, 37, 3, 8, 27, 50, 532], [2877, 10000, 2440, 4, 1043, 2575, 180, 2, 266, 45, 12968, 316, 495, 56, 11, 23, 292, 138, 54], [202, 72, 33, 1351, 5278, 5279, 4733, 5, 2041], [13, 1, 1, 516, 1064, 232, 4214, 29237], [13, 1, 978, 439, 4057, 62, 99, 1639, 137, 1986, 511, 99, 6, 355, 6, 117, 120, 2130, 4268, 1142, 12, 156, 11, 4668, 263], [26938, 5231, 404, 12, 90, 39, 287, 1505, 9495, 182, 1563, 9, 6, 402, 95, 34, 7, 1219, 181, 5206, 26, 98], [1, 1007, 238, 7421, 390, 471, 1419, 195, 22358, 4, 2215, 2139, 195, 22358, 4, 2215, 60, 60, 287, 19, 391], [52, 397, 34, 1798, 26152, 1340, 17], [1, 16439, 9, 7731, 19, 811, 283, 37, 3, 8, 640, 37, 3, 8, 640, 37, 3, 8, 640, 3103, 23, 884, 18, 16439, 29, 15], [13, 1, 246, 1346, 29, 3384, 144, 244, 1, 3759, 1227, 31, 1694, 5, 4337, 14549, 2171, 28, 134, 14, 823], [13, 1, 1, 70, 81, 159, 99, 59, 14, 1383, 28, 1573, 2706, 1603, 12964, 45, 12111, 42], [1, 1, 5, 1221, 6, 539, 3515, 6670, 18, 238, 728, 599, 44, 19, 525], [1, 7746, 1141, 62, 286, 4319, 57, 947, 23, 9, 1972, 43, 7, 43, 7, 100, 39, 608, 88, 23, 947, 9, 825], [20, 1337, 77, 124, 16, 17, 38, 5, 237, 1004, 16, 816, 4, 209, 2299, 25, 489, 4, 6029], [1, 24, 194, 156, 60, 60, 264, 34, 57, 151, 4, 8266, 2, 4, 34, 2875, 1299, 2271, 7, 545], [1, 1, 1, 2808, 15, 55, 101, 10, 14, 372, 66, 947, 2141, 1583, 79, 36, 14, 92, 91, 37, 3, 8, 640, 79, 36, 14, 92, 91, 3, 8, 22, 10, 21, 1070, 2, 34, 230, 12], [1, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 2107, 5731, 5, 153, 506, 2045, 2, 7471, 577, 109, 5, 153, 23, 9, 5003, 76, 61], [1, 11397, 72, 70, 202, 94, 31501, 160, 137, 492, 289, 8208, 1553, 72, 12123, 168, 415, 4609, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 933, 867], [1, 278, 1413, 44, 402, 100, 914, 1004, 493, 592, 158, 64, 13479, 11, 39, 1374], [1, 73, 14, 319, 8, 22802, 6759, 1684, 3102, 2, 7132, 2, 6, 155, 138, 28, 155, 2, 6], [1, 1, 1, 143, 1, 24, 246, 29, 62, 723, 107, 83, 868, 115], [1, 24, 23, 5614, 1177, 4, 208, 241, 225, 104, 47, 95, 2812, 4570, 12, 1987, 16, 13646, 17], [1, 1, 1, 3638, 638, 7, 1408, 9, 840, 28, 951, 18, 14, 1068, 1584, 29, 348, 439, 2010, 18, 328], [1, 5, 97, 534, 150, 5847, 1794, 5847, 1315, 823, 3778, 2323, 4550, 97, 42, 4519, 8779, 597, 2, 304], [1, 63, 57, 80, 11, 888, 19, 221, 261, 364, 491, 1525, 1419, 43, 12, 21559, 2111, 201, 716, 88, 5079], [13, 1, 12388, 70, 6, 15, 1066, 69, 37, 3, 8, 27, 50, 15, 55, 327, 1839, 6, 16, 75, 27, 2725, 5, 723, 33, 12304, 23690, 28, 1407], [20, 117, 7321, 4, 9, 3346, 386, 712, 1954, 20, 2050, 1250, 712, 1121, 196, 270, 249, 255, 137, 14, 452, 18, 755, 36, 414], [13, 1, 218, 4939, 1598, 29, 676, 144, 40, 10852, 14, 1, 3683, 5], [580, 69, 4742, 12255, 14637, 7, 4, 420, 253, 8515, 3385, 1, 1], [1, 1, 23898, 74, 239, 54, 534, 25, 1051, 32, 4, 2, 6893, 351, 36, 347, 1423, 17, 122, 32, 832, 7, 6076], [1, 5665, 18, 565, 1695, 10, 56, 808, 58], [1, 1, 1, 1, 439, 296, 3030, 798, 188, 138, 52, 361], [1, 1415, 26, 6911, 6, 133, 25, 227, 430, 269, 40, 21645, 82, 17, 162, 17, 443, 1221, 5, 1221, 6, 204, 577, 12, 3541], [1, 65, 48, 1166, 30, 136, 2041, 30, 12, 17, 1647, 12710, 95, 183, 4, 150, 522, 17, 506, 40, 2358, 2, 84], [1, 1757, 6293, 68, 8886, 31, 4011, 4772, 28, 8019, 42, 18, 175, 612, 288, 2958, 500], [1, 532, 1279, 16393, 10205, 154, 80, 543, 605, 828, 9673, 2905, 1079, 24, 10206, 37, 3, 8, 27, 50, 42, 10205, 1, 3, 171, 141, 1035, 27], [1, 351, 36, 7763, 1584, 67, 1372, 884, 51, 129, 40, 55, 93, 3935, 306, 27179, 11631], [1, 1, 1, 38, 609, 9, 245, 136, 1369, 322, 71, 350, 9, 2, 54, 69], [74, 4, 3191, 16, 90, 39, 727, 9, 2, 2344, 26, 2798, 7, 4, 11361, 197, 188, 1282], [1, 3156, 2221, 14628, 378, 97, 5, 3159, 5030, 1424, 16819, 363, 139, 6138, 199, 1051, 10805, 3], [2741, 4624, 74, 366, 1806, 21646, 18, 173, 272, 375, 350, 11970, 1806, 3836, 2295, 204, 577], [1, 1, 1, 1, 1, 765, 154, 1963, 272, 2, 204, 577, 4407, 3, 8, 22, 10, 21], [62, 134, 2727, 29, 144, 4030, 31, 3256, 1513, 486, 5, 393], [1, 1042, 58, 93, 114, 33, 2287, 18, 1824, 8, 123, 101, 12993, 8, 123, 469, 14, 719, 1855, 2885, 3, 8, 22, 10, 21], [1, 1, 1, 1, 1, 944, 519, 83, 303, 115, 734, 480, 7, 83], [1, 31, 368, 1047, 12, 29445, 279, 38, 854, 13031, 313, 555, 1339, 16, 6005, 279, 7312, 7203, 14806], [1, 1, 316, 94, 32, 117, 2058, 4, 1549, 2763, 27196, 4, 208, 8923, 40, 11664, 4, 6144, 447, 2592], [13, 1, 13408, 1, 52, 189, 562, 160, 510, 438, 117, 700, 5, 388, 977, 510, 474, 267, 4297], [1, 1, 1, 7430, 1, 1, 1, 1, 1, 2808], [119, 11, 475, 119, 11, 1530, 143, 116, 35], [1, 1, 1, 912, 540, 11, 14092, 1653, 133, 127, 1678, 739, 7, 7041, 14093, 14094], [13, 1, 14, 1307, 1110, 3674, 58, 179, 5809, 9308, 1890, 62, 317, 49, 246, 10, 348, 802, 27, 802, 27, 802, 27, 5677], [1, 52, 189, 766, 24, 819, 160, 16, 388, 2499, 28, 474, 2500, 24], [120, 10594, 2984, 72, 983, 158, 39, 1164, 2, 3110, 2299, 28094, 12, 13239, 26, 1164, 7], [1, 20, 797, 1062, 16, 5909, 34, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 318, 52, 4844, 256, 196, 418, 148, 767, 1665, 1809, 512, 248, 742, 138, 2660, 3, 165, 130, 3, 165, 130, 3], [13, 1, 1, 24, 338, 5970, 122, 119, 25485, 25, 8093, 422, 2, 394, 215, 25, 17507, 4878, 4558, 5], [52, 189, 35, 85, 112, 676, 35, 205, 122, 119, 24709, 157, 2], [1, 146, 106, 12, 146, 116, 1282, 29673, 30, 24, 7, 219, 17, 116, 972, 12, 28628, 28628, 143], [13, 1, 1, 35, 1, 7, 30, 4, 2334, 178, 84, 1434], [13, 1, 549, 66, 1141, 1598, 179, 23, 5, 6223, 1], [1, 3386, 1, 1, 1, 1, 1, 1, 1, 1484], [57, 1754, 56, 11, 1397, 19, 108, 25, 8566, 106, 9306, 106, 115, 120, 119, 3413, 17, 188], [60, 253, 19], [83, 244, 115, 1, 159, 127, 6320, 26627, 59, 324, 7870, 36, 15, 438, 15, 59, 4035, 28, 700, 6, 3969], [1, 1676, 11, 1331, 2115, 873, 64, 1058, 3, 145, 1570, 41, 6, 750, 180, 4, 220, 7094, 1058, 3], [1, 1, 44, 98, 128, 1164, 2549, 67, 14036, 44, 734, 1434, 44, 98, 77, 11319, 44, 128, 7, 5795], [1, 3826, 1144, 7, 972, 210, 28199, 2, 561, 140, 169, 6, 1898, 3812, 17, 2664, 4581, 8095, 44, 186], [3306, 2757, 6, 33, 59, 184, 809, 1164, 2531, 1, 329, 329, 70, 1696, 10, 14, 132, 172, 3012, 1946, 463, 247], [1, 1, 49, 3274, 10490, 10491, 583, 132, 70, 81, 1, 159, 59, 68, 520, 602], [312, 10000, 3868, 956, 556, 1520, 46, 101, 2336, 86, 42, 58], [1, 93, 31, 1278, 1485, 80, 453], [1, 331, 261, 3242, 2140, 972, 210, 20, 2271, 41, 53, 4, 230, 63, 7930], [1, 2682, 1024, 20747, 39, 2421, 1218, 1043, 9, 186, 28, 204, 411, 3602, 203, 9], [13, 1, 453, 453, 453, 5663, 297, 87, 2, 1483, 11, 252, 3526, 9661, 27626, 4583, 29, 5, 32320, 80, 31, 5, 524], [1, 268, 6425, 2427, 105, 105, 105, 224, 216, 55, 804], [96, 866, 4, 5322, 2, 5, 30, 24, 7, 4, 30, 24, 34, 89, 182, 13585, 336, 989, 60, 183], [1, 5, 3269, 6, 71, 410, 23, 9, 6193, 18, 238, 373, 67, 1123, 3], [1, 1, 9434, 7713, 4019, 26, 604, 2000, 38, 177, 161, 7, 674, 1452, 47, 450, 1117, 19, 84, 8450], [1, 63, 27520, 591, 41, 4838, 104, 608, 20, 410, 3915, 63, 27520, 4, 4, 674, 589], [1, 73, 1101, 99, 13210, 8, 289, 2076, 1942, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [56, 3843, 10009, 19, 695, 112, 12, 90, 22911, 56, 125, 206, 156, 269, 2, 18201, 12, 17, 2428, 2902], [1, 24, 76, 61, 299, 156, 1446, 53, 4, 156, 515, 37, 3, 8, 1113, 729, 35, 693, 226, 161, 90, 57, 2538, 6879, 2360, 803], [1, 5, 237, 153, 27341, 646, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 1581, 48, 152, 209, 972, 12, 13034, 1418, 4, 9, 30, 26, 5, 310], [1, 102, 432, 251, 4087, 6, 16], [2595, 998, 868, 371, 30, 24, 7, 31360, 106, 11, 998, 489, 801, 82, 7274, 53, 6047], [243, 46, 18, 432, 46, 18, 1219, 3922, 277, 3583, 5844, 19, 322, 105, 175, 623, 46, 275, 20], [1, 1, 1, 147, 599, 540, 6, 9, 40, 2961, 912, 221, 8044, 16, 1087, 65, 1503, 16], [1, 1, 14245, 1, 2123, 16, 191, 151, 1700, 1804, 1424, 731, 3, 731, 3], [1, 9165, 15, 7562, 68, 667, 10828, 28, 14, 453, 58, 213, 350, 132, 768, 31586], [13, 1, 16432, 11, 30247, 310, 403, 132, 10, 836, 31773, 11, 2097], [59, 575, 146, 1148, 10, 200, 31, 14, 3228, 460, 12648, 29, 1050, 12, 431, 768], [1, 146, 28, 6814, 58, 352, 6, 72, 83, 1670, 28, 83, 2116, 28, 1211, 31, 12708], [13, 1, 83, 244, 115, 18, 3294, 10, 86, 970, 586, 140, 3442, 2, 94, 6420, 1304, 18, 3294, 10, 19289], [1, 4796, 58, 1268, 49, 52, 144, 218, 600, 14, 101, 202, 325, 5308, 12, 7474], [1, 299, 156, 663, 861, 119, 4, 156, 2, 35, 2339, 12, 3509, 16, 896, 32, 794, 899, 211, 548, 6342], [83, 83, 17217, 364, 9372, 1, 1], [1, 1, 15, 55, 328, 542, 233, 18, 285, 1016], [13, 1, 3548, 11, 27093, 889, 889, 281, 478, 451, 3548, 30562, 168, 26287, 375, 14, 7197, 451, 94, 390], [1, 162, 32, 1135, 472, 5201, 2566, 306, 73, 1498, 347, 97, 1801, 2916, 10406, 97, 64, 8, 59, 314], [1, 4787, 5, 1, 1, 35, 52, 1989, 328, 2138, 17847, 5, 5, 19549, 17, 292, 207], [13, 1, 1635, 484, 15, 6, 3082, 16, 268, 6, 42, 3626, 615, 845, 52, 483, 179, 58, 68, 10, 42, 31, 33, 27, 28], [1, 4423, 185, 67, 7483, 19, 63], [1, 1, 35, 400, 9680, 9, 126, 489, 263, 51, 400, 192, 9674, 98, 25, 5796, 51, 495, 18, 17, 43, 7], [1, 134, 15, 513, 68, 73, 15, 55, 31, 1235, 31, 964, 8, 16, 144, 492, 6961], [1, 1, 1, 3586, 1, 1, 7717, 1, 3586, 1, 83, 360], [13, 1, 314, 8373, 3702, 295, 8612, 614, 560], [1, 30150, 41, 65, 4373, 16172, 122, 71, 38, 71, 4019, 4373, 16172, 608, 780, 2078, 541], [1, 1, 54, 4497, 470, 23, 9, 721, 269, 165, 130, 3, 228, 6, 1674, 25, 1734, 2], [1, 1347, 2216, 5320, 6979, 78, 1630, 572, 16, 17, 57, 2, 452, 563, 1507, 1706], [77, 13422, 541, 4, 11822, 19, 45, 151, 17, 4, 21178, 19, 119, 53, 45, 17, 1341, 19, 104, 2, 417], [376, 42, 80, 1800, 42, 7151], [13, 1, 137, 4378, 80, 985, 6, 251, 306, 1680, 622, 10941, 756, 13874, 985, 76, 61, 374, 1694, 5, 10755], [143, 6, 1, 144, 2898, 533, 271, 28, 488, 143, 6, 1, 144, 850, 475], [1, 548, 3621, 11, 389, 1987, 12, 157, 3282, 4933, 7138, 23368, 9, 595, 635, 184, 5, 5], [1, 94, 3545, 1616, 163, 11, 552, 11, 45, 71, 3234, 25, 3235, 25, 235, 2047, 4, 5634, 552, 5183, 2564], [35, 3445, 45, 195, 353, 172, 4, 39, 464, 340, 41, 2962, 787, 875, 45, 1915, 11, 6, 370, 203, 17, 16202, 64, 59, 14, 66, 29, 1915], [13, 1, 46, 18, 14, 8730, 10, 11650, 986, 106, 2828, 56], [1, 366, 3186, 631, 1443, 44, 2286, 2860, 131, 44, 464, 13868], [1, 1, 1, 2875, 1, 42, 68, 587], [1, 96, 6, 19, 322, 509, 1082, 740, 7764, 16, 1894, 12, 65, 19, 38, 625, 6068, 386, 19, 1952], [1, 1, 1, 69, 11815, 20, 2, 4, 15, 55, 1482, 3910, 4471, 36, 68, 5220, 28], [34, 65, 866, 88, 19, 4851, 359, 197, 17, 1758, 30906, 19, 939, 356, 454, 19, 1025, 2, 30907, 564, 88], [1, 101, 5, 432, 782, 7073, 803, 172, 2419, 11, 14948, 126, 302, 202, 30941, 326, 1262, 462, 1165, 10], [13, 1, 5, 29891, 20, 1263, 1802, 9681, 7, 1192, 9, 840, 575, 142, 315, 4697, 2040, 86, 623, 1200], [1, 1, 1, 54, 19, 108, 1871, 2568, 112, 7, 54, 43, 32, 23, 358, 126, 64], [1, 551, 184, 67, 5306, 9, 3037, 47, 54, 6, 26, 2858, 3201, 5132, 63, 124, 17, 3037, 77, 9], [1, 28557, 18, 17, 23, 443, 71, 14096, 241, 2, 38, 1035, 241, 25, 28926, 3641, 2345, 84, 386, 906], [1, 30, 140, 3435, 47, 6, 2029, 5, 23090, 4051, 12, 626, 1130, 1601, 47, 4783, 506, 695, 9, 6], [152, 152, 98, 82, 60, 1306, 840, 2, 644, 3, 644, 3, 644, 3, 60, 2334, 19, 16, 643, 3, 643, 3, 94, 252, 664, 16], [1, 142, 706, 6, 36, 59, 10, 46], [2300, 1, 4204, 575, 6242, 334, 134, 15, 19676, 173, 144, 202, 294, 27, 294, 27, 294, 27, 134, 15, 310, 8918], [62, 99, 28, 62, 149, 74, 59, 14, 66, 6, 35, 1, 5, 28, 14, 170, 75, 27, 62, 149, 393, 15, 1816, 14, 389, 909, 198], [1, 12719, 62, 12298, 224, 200, 2005, 3016, 33, 1391, 33, 193, 33, 1211, 35, 75, 27, 75, 27, 37, 671, 3, 8, 27, 50, 37, 671, 3, 8, 27, 50, 37, 671, 3, 8, 27, 50, 379, 6, 1001], [1, 1, 70, 81, 35, 400, 666, 828, 26, 828, 25, 735, 735, 24681, 254, 2957, 3270], [1, 15, 55, 20561, 133, 127, 4, 610, 368, 717, 161, 1756, 316, 26, 1761, 6, 4959], [1, 1, 5974, 340, 46, 587, 200, 165, 130, 3, 165, 130, 3, 165, 130, 3, 165, 130, 3, 165, 130, 3], [1, 1, 393, 14, 170, 86, 123, 393, 18, 114, 46, 18, 193, 1062, 29, 14, 626, 114, 103, 1073], [1, 3380, 5, 12, 740, 89, 330, 476, 9, 2710, 64, 1536, 63, 124, 11, 3105, 546, 3932, 2, 4, 815], [170, 5401, 25, 4146, 2, 125, 206, 16, 1271, 12, 90, 461, 8512, 2, 3254, 11, 183, 9, 2, 20, 3121], [1, 2029, 12, 28278, 96, 2, 133, 2996, 405, 147, 2164, 16, 2, 120, 17, 9, 15990], [13, 1, 894, 528, 566, 894, 528, 566, 408, 345, 112, 894, 528, 566, 894, 528, 566, 58, 184, 29, 378, 139, 1162, 62, 55, 202, 135], [1, 279, 40, 15350, 17268, 11, 157, 792, 12, 1224, 104, 47, 71, 3321, 6003, 9, 47], [1, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 109, 71, 1278, 1965, 89, 57, 842, 466, 3805, 12, 412, 2694, 1690, 98, 7, 1131, 51, 223, 41, 3344, 1230, 3507], [1, 1, 414, 1, 3479, 9432, 103, 238, 7, 69, 57, 107, 25, 924, 212, 167, 16, 457], [13, 1, 139, 11067, 149, 5726, 28, 6, 4559, 28, 7415, 1616, 27590, 18, 755, 6, 72, 26180, 29, 59, 951], [1, 1, 7508, 1, 194, 535, 3748, 491, 4070], [1, 218, 602, 30, 24, 652, 28, 2836, 123, 3797, 1703, 28, 626, 83, 360], [1, 42, 117, 535, 94, 266, 235, 1392, 45, 670, 1686, 103, 44, 2960, 7, 2960, 972, 4, 1226, 74, 2127], [1, 871, 65, 7560, 40, 47, 236, 18, 101, 8305, 348], [143, 1, 29, 367, 62, 14, 216, 1029, 31, 15, 1, 24, 194, 3487], [103, 103, 30, 8944, 407, 30, 652, 374, 7278, 30, 374, 286], [31349, 31350, 31351, 1, 24, 27, 28856, 52, 189, 562, 736, 24, 27, 28856, 384, 602, 345, 33, 200, 1066, 384, 160, 16, 388, 891], [1, 397, 4517, 41, 71, 65, 241, 47, 38, 3761, 12, 1182, 47], [13, 1, 70, 81, 6, 15, 913, 855, 1371, 10, 2276, 1371, 10, 4707, 1371, 10, 9176, 1927, 35, 299, 39, 891, 2130, 223], [1, 70, 81, 304, 767, 713, 13585, 159, 15, 99, 329, 166, 75, 27, 159, 34, 7, 3449, 9, 5405, 82, 8028], [1, 1, 2661, 1, 1, 1, 1, 253, 19, 253, 19, 57], [13, 1, 5009, 5, 408, 541, 385, 69, 7, 1, 69, 302, 192, 75, 27, 1083, 21109, 5827, 302, 37, 3, 8, 1113, 359, 1032, 735, 186, 47, 37, 3, 8, 27, 50, 45, 18, 11, 2333, 716, 418, 148, 11705], [1, 1, 404, 24, 479, 31239, 17, 119, 2136, 169, 6, 400, 95, 12112, 11, 7548, 16, 4851], [13, 1, 3460, 494, 73, 30, 24, 18, 93, 5232, 283, 29858, 12566, 73, 5232, 291, 1343, 291, 285, 1216, 99, 615], [13, 1, 1990, 80, 14, 166, 15, 1521, 58, 14, 166, 58, 20804, 14, 2307, 10, 68, 8088, 4827, 58, 68, 8088, 224, 167, 49, 2336, 2143], [1, 1037, 59, 211, 6, 101, 10, 33, 16331, 109, 236, 73], [70, 70, 781, 44, 3330, 29, 123, 962, 3892, 765, 202, 414, 29, 15, 4230], [42, 33, 25752], [1717, 24602, 40, 149, 72, 29407, 3419, 785, 47, 608, 377, 393, 1717, 24602, 59, 14, 66, 19, 83, 298], [1, 4065, 3565, 710, 63, 110, 7253, 12, 776, 4065, 23, 19, 416, 2, 221, 717, 3017, 12, 931, 44, 26872], [1, 248, 6, 133, 1881, 26, 6909, 225, 98, 12, 25, 17, 7, 10531, 19, 2052, 26, 18], [13, 1, 1, 2536, 32, 178, 51, 87, 48, 293, 54, 6495], [1, 1, 1, 1, 1, 1, 1, 1, 218], [1, 12691, 1, 1764, 226, 15, 55, 2057, 1731, 66, 520, 6, 14, 3555, 25065, 56, 352, 492, 1520], [204, 3032, 998, 489, 303, 371, 30, 24, 7, 24244, 156, 4, 420, 264, 76, 61, 76, 61, 76, 61, 83, 303, 115, 76, 61, 76, 61, 76, 61], [1, 29802, 1, 1, 1, 5, 482], [1, 146, 143, 35, 172, 7, 106, 558, 32, 2085, 144, 312, 30687, 114, 1635, 814, 28, 115, 53], [13, 1, 62, 42, 413, 7128, 233, 62, 118, 75, 27], [1, 35, 548, 2455, 355, 637, 698, 1725, 1201, 742, 2, 38, 4, 1204, 716, 9, 997, 8590, 105], [13, 1, 4087, 266, 17049, 5, 3, 8, 1390, 139, 1295, 239, 1370, 94], [1, 5330, 19, 2559, 803, 66, 2886, 7, 2068, 84, 311, 461, 105, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 26, 2337, 923, 203, 2531, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [13, 1, 499, 9099, 28, 2398, 36, 68, 1152, 1733, 600, 33, 81, 1, 70, 81, 6, 16, 81, 320], [366, 1027, 57, 473, 11, 953, 655, 403, 1027, 169, 2, 377, 195, 2762, 1027, 19, 7367, 195, 338, 2, 45, 235, 12001, 338, 2577, 401, 4821, 2], [13, 1, 1, 35, 194, 156, 4, 253, 34, 466, 53, 12, 116, 2, 35, 657, 822, 2, 321, 148, 12797, 12, 25555, 5, 25556, 25557], [4163, 4164, 1, 52, 477, 562, 438, 117, 289, 1681, 5, 118, 166, 510, 166, 388, 977, 28], [1, 207, 3824, 5661, 499, 58, 148, 505, 2083, 28, 2291, 1533, 15, 23971, 6051, 1, 207], [1, 1, 629, 959, 333, 5320, 18, 266, 2733, 6609, 5128, 131, 64, 18, 5208, 1957, 5451, 7, 18], [1, 1, 1, 1, 21074, 1, 21075, 5, 1, 21076, 5, 1, 21077, 1, 3299, 42, 15, 59], [1, 35, 24, 194, 828, 97, 17, 57, 908, 333, 34, 17, 57, 15595, 2, 4, 666, 57], [1284, 205, 1284, 201, 12, 1062, 7, 3770, 3611, 11, 2594, 297, 254, 19, 1332, 854, 2145, 9, 6190], [614, 2, 24, 614, 2, 137, 448, 341, 42, 231, 281, 281, 1567, 10, 42, 28, 3929, 1, 23556, 37, 3, 8, 37, 50], [13260, 1, 52, 189, 562, 160, 117, 29, 388, 977, 28, 474, 267, 267, 496, 191, 25, 707], [13, 1, 3808, 892, 628, 1, 24, 438, 117, 897, 10, 1415, 1462, 378, 9162, 6590, 1172, 723, 1765, 168], [1, 143, 83, 19, 83, 360, 371, 30, 1395], [162, 1084, 7, 248, 153, 63, 65, 5142, 41, 4244, 1588, 2, 63, 501, 7, 577, 4650, 2770, 67], [1, 17793, 20, 155, 155, 23, 9, 422, 20, 227, 9, 12338, 95, 124, 7, 297], [1, 1548, 1, 1, 1, 1, 1, 106, 464, 83, 360], [1, 140, 20, 399, 654, 2, 6, 43, 26, 85, 638, 7, 9706, 85, 7, 4836, 51, 2773, 229, 18], [13, 1, 1, 2504, 8093, 4171, 4558, 122, 3184, 188, 338, 24, 5], [1, 1, 3955, 289, 2950, 639, 27, 23603, 29, 1340], [2877, 109, 45, 5726, 638, 19846, 780, 5715, 44, 390, 109, 3356, 84, 32562, 24], [1, 62, 107, 42, 15, 35, 62, 379, 1735, 114, 15, 276, 355, 12622, 6, 600, 1152, 2042, 1320, 6109], [1, 1, 854, 6, 11, 1929, 11, 3420, 721, 108, 64, 124, 7, 1306, 721, 108, 64, 85], [25, 811, 266, 116, 6222, 569, 73, 4, 30, 13248, 6, 321, 6557, 28206, 4522, 54, 140, 9, 4522, 6, 3907, 317], [1, 70, 81, 94, 136, 94, 1545, 3691, 273, 28, 2369, 4014, 148, 1342], [761, 18, 175, 114, 1398, 3997, 38, 48, 153, 5, 10287, 7, 26069, 1413, 2, 26, 48, 106, 7, 26070, 4, 8760, 16, 2, 18], [542, 6, 72, 30797, 114, 13465, 168, 16, 2729, 16, 208, 30798, 30799, 10220, 182, 3073, 30800, 182], [13, 1, 25929, 52, 189, 1296, 496, 1, 211, 29, 955, 46, 341, 132, 160, 46, 132, 1599], [1, 433, 38, 537, 943, 7, 5167, 26, 157, 14710, 11, 12101, 19, 18, 11, 5854, 26, 12184, 15230], [1, 385, 17, 3234, 6, 191, 177, 40, 382, 33, 11292, 133, 1933, 353, 25, 14163, 40, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 1, 1, 317, 3565, 710, 6, 728, 16, 78, 4104, 310, 240, 207, 313, 2, 31342, 11, 8841], [13, 1, 29854, 326, 94, 4425, 18, 30413, 216, 36, 6392, 159, 15, 72, 101, 10, 348, 1226, 136, 28010], [13, 1, 2422, 484, 15, 1, 379, 6, 154, 15, 165, 130, 3, 379, 68, 42, 379, 6, 1040, 15, 252, 8203, 1663, 8754], [13, 1, 42, 15, 33, 531, 676, 19862], [1, 1, 1567, 10, 1567, 66, 10, 14, 66, 520, 14, 170, 30, 296, 412, 296, 475, 296], [1, 4617, 738, 11, 982, 316, 181, 3453, 368, 5466, 9036, 391], [13, 1, 8307, 1, 929, 2742, 338, 267, 267, 496, 191, 25, 707, 160, 117, 24, 438, 117, 929, 940], [684, 3245, 30, 5, 66, 10, 469, 35, 384, 1956, 238, 836, 4893, 1336, 13082, 6163, 56, 3116, 3173], [275, 102, 4295, 70, 28, 713, 619, 45, 2339, 493, 6, 15, 14, 752, 28, 268, 2547, 4225, 19349], [13387, 7, 11722, 43, 12, 90, 39, 659, 150, 9], [1, 1037, 234, 18, 137, 6732, 125, 4311, 457, 6, 122, 231, 139, 59, 184, 12128, 223, 922, 906], [1, 1, 1036, 34, 60, 5, 30, 24, 5, 97, 489, 801, 215, 2527, 208, 475, 138], [1, 807, 228, 31988, 287, 158, 1744, 999, 133, 7, 6, 726, 11, 1118], [1, 1234, 6, 869, 178, 51, 41, 6, 127, 7819, 2180, 133, 912, 5225, 3006, 4082, 1399, 128, 26, 1802], [1, 23956, 1, 1, 1, 8021, 38, 564, 32564, 9, 22784, 182, 1578, 26, 184], [13, 1, 195, 151, 7, 147, 151, 4, 2412, 2, 1730, 3, 1730, 3, 1730, 3, 1730, 3, 1730, 3, 1730, 3, 1730, 3, 27, 441, 27, 441, 27, 441, 27, 441, 27, 441, 27, 441], [2701, 58, 117, 132, 1, 258, 198], [1, 1036, 35, 253, 19, 35, 13505, 12, 7, 77, 390, 374, 877, 18, 31081, 86], [1, 78, 107, 155, 259, 2113, 2, 2330, 7, 6802, 87, 100, 3126, 4, 7135, 290, 51, 107], [1, 71, 4895, 11, 30231, 2209, 44, 1, 521, 24462, 88, 798, 5, 5, 8615], [1, 105, 1106, 17, 1905, 25, 64, 450, 49, 119, 751, 340, 3452], [853, 357, 25908, 6852, 682, 14519, 11307, 1766], [1, 97, 885, 40, 573, 502, 1040, 46, 101, 59, 14, 184, 28, 2396, 28, 1794], [13, 1, 4936, 42, 15, 11273, 42, 15, 49, 173, 1669, 1669, 424, 27], [1, 118, 1579, 513, 80, 1246, 139, 46, 359, 1184, 83, 303, 115], [1, 80, 5, 1, 127, 16, 180, 356, 865, 51, 232, 2, 954, 147, 177, 32, 979, 943, 356, 865, 41, 4, 186, 100], [16, 37, 3, 8, 1113, 478, 31, 10975, 629, 2958, 15, 831, 68, 992, 15, 459, 175, 114, 324, 139, 326, 62, 831, 31, 2], [1, 1, 4350, 78, 2919, 25, 45, 2919, 1008, 20694, 5719, 53, 3585, 78, 514, 233, 11, 11235, 67, 45, 74, 225, 20695], [1, 1, 1843, 1293, 24, 2129, 24, 4942, 10898, 2129, 24, 3150, 427, 34, 204], [1, 1, 1244, 24, 145, 153, 336, 3259, 3813, 12, 157, 2047, 2652, 9, 595, 471, 153, 299, 23048], [1, 16742, 1, 935, 109, 77, 23, 3093, 2], [1, 288, 696, 101, 5750, 111, 31, 12633, 10, 222, 18841, 28, 9005, 222, 2855, 28, 233, 265], [1, 579, 4, 230, 32, 685, 1841, 64, 516, 310, 265, 495, 2607, 2432, 203, 121, 5, 669, 9, 26231, 14, 5433], [2195, 49, 7535, 306, 73, 2815, 315, 1235, 1801, 4464, 2336, 755, 29, 14, 125, 206, 64], [18, 373, 77, 796, 259, 2, 18, 373, 8894, 28320, 7, 311, 259, 2, 28321], [1249, 26, 1340, 75, 27, 106], [62, 59, 55, 3599, 15, 12618, 252, 128, 472, 82, 34, 34, 1069, 3764, 22047, 8462, 384], [1, 1, 69, 161, 3925, 7, 497, 841, 178, 133, 747, 282, 571, 761, 16, 3925], [1, 1, 2884, 113, 261, 783, 182, 1514, 140, 147, 1046, 654, 16, 39, 7337, 2, 6], [1, 1, 1, 1, 1, 105, 402, 295, 413, 599], [1, 54, 1081, 150, 3404, 17899, 97, 23, 669, 19, 63, 221, 4, 128, 2035, 32], [70, 81, 1931, 4898, 4830, 1, 24, 36, 46, 12701, 15910, 88, 23, 4914, 89, 427, 26890], [1, 1, 1, 2019, 1, 4267, 5, 1, 366, 505], [29613, 7937, 438, 117, 6546, 4740, 62, 623, 706, 5446, 6350, 1, 233, 1, 1], [1, 49, 351, 7936, 62, 139, 43, 187, 2887, 315, 1266, 18278, 107, 2193, 155, 2193, 7125, 2193, 20676], [1, 4, 1, 2083, 1, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 490, 14, 6, 46, 1732, 18, 808, 171, 73, 1101, 187, 310], [1, 1, 38, 935, 5466, 3779, 47, 117, 4, 741, 47, 9524, 1685, 4, 638, 3690, 16, 11891, 17, 1269, 239], [1, 1, 27693, 129, 6, 78, 54, 51, 1591, 72, 63, 368, 3162, 2790, 19, 104, 5, 237, 17, 122, 1430], [1, 1, 16362, 1016, 382, 11, 3394, 21088, 17, 1272, 7, 869, 87, 44, 6, 133, 127, 1047, 57, 51, 1297], [13, 1, 349, 5871, 17, 1134, 2873, 5, 5681, 158, 4444, 760, 29877, 1116, 760, 760, 2153, 310, 5511], [13, 1, 2497, 132, 58, 18, 2947, 1, 2019, 35, 1, 35, 62, 55, 793, 58, 755, 6, 72, 1022, 2241, 375, 14, 66, 613, 3, 1], [1, 459, 276, 18, 2057, 38, 783, 6224, 27065, 263, 585, 47], [1, 1, 204, 795, 11, 242, 25, 169, 2, 815, 1771, 9, 7, 1559], [1, 611, 3456, 4, 156, 3148, 186, 2, 2354, 2112, 82, 85, 60, 10256, 856, 212, 3456, 22391], [455, 30681, 4, 3344, 2228, 795, 82, 9407, 9, 186, 2, 128, 4, 221, 553, 445, 51, 431], [1, 9092, 3668, 33, 2212, 29, 136, 581, 793, 6795, 25454, 808], [1, 645, 869, 3240, 203, 9, 507, 44, 191, 954, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 128, 18981, 4, 1269, 17, 40, 2609, 11423, 2794, 133], [1, 1, 31227, 714, 88, 60, 5740, 2, 1685, 90], [13, 1, 1, 1, 333, 5, 1, 1, 1, 333, 70, 81, 1051, 159, 1296, 496, 24], [1, 6837, 1190, 2241, 31, 46, 812, 375, 2263, 5, 689, 500, 8233, 306, 1071, 651], [1, 30037, 1, 5, 74, 503, 2700, 7, 22355, 232, 2, 38, 266, 535, 32, 684, 4, 5, 12025, 8835, 90], [1, 1, 1, 1, 1, 1, 724, 1, 1, 1948, 44, 19, 974], [1, 1, 1445, 1466, 9, 19, 1010, 22784, 1804, 84, 455, 16, 113, 386, 121, 1546, 84], [13, 1, 8794, 1410, 417, 6525, 5, 3252, 4, 702, 17, 116, 35, 7, 60, 60, 253, 861, 3545, 174, 1573, 158, 199], [1, 35, 16, 1160, 1100, 4346, 217, 252, 327, 305, 10, 15], [1, 1, 3253, 366, 239, 214, 86, 88, 2511, 4, 5715, 41, 313, 86, 377, 57, 12687, 531, 1061, 323, 146, 418, 148], [13231, 35, 790, 56, 28, 12485, 89, 9055, 19, 3373, 100, 56, 6426, 82, 35, 56, 1178, 89, 152], [1, 13510, 18, 472, 16, 74, 4, 6, 665, 899, 1897, 32], [3325, 48, 3787, 7, 1193, 6042, 131, 2468, 20, 7850, 3958, 47, 12109, 997, 15942, 2201, 47], [1, 74, 147, 1518, 177, 11, 2, 54, 402, 1493, 3061, 108, 48, 12, 74, 16, 9, 13386], [228, 57, 1267, 7, 503, 71, 5616, 11, 28290, 1559, 64, 6, 8712, 67, 108, 64, 82, 2674, 354, 12, 1281, 17, 185, 9, 245], [1, 1677, 2641, 1403, 73, 2751, 8652], [1, 1, 1, 1, 1, 1229, 7241, 17, 539, 20, 2819, 161], [1, 20, 8029, 74, 11, 2, 31, 1182, 297, 2, 38, 266, 535, 7, 608, 71, 935], [1, 1, 1, 39, 8436, 9, 82, 18676, 684, 2509, 444, 1744, 164, 2702, 3, 8, 22, 10, 21, 605], [13, 1, 1220, 18, 866, 1220, 18, 111, 1680, 32431, 54, 18, 626, 16, 3122, 2730, 728, 4262, 20045], [1, 143, 62, 149, 4046, 102, 52, 2325, 139, 142, 467, 114, 2374, 309, 1050, 62, 1676], [13, 1, 1, 54, 568, 1312, 512, 194, 693, 81, 67, 199, 1242, 23, 194, 406, 525, 3306, 81, 748, 194, 3902], [1, 62, 513, 10798, 8355, 28, 4887, 5, 5, 31, 1477, 73, 213, 2699, 6, 72, 14, 372, 1944, 812, 10, 14, 1885, 29, 117, 3543], [1, 1, 2149, 733, 96, 537, 2149, 161, 3147, 603, 4192, 159, 199, 359, 1372, 1273, 1737, 298, 2436, 537, 4202], [13, 1, 57, 25, 2658, 57, 25, 185, 83, 244, 115, 83, 83, 244, 115, 7718, 1], [1, 1, 608, 20, 25, 1400, 2, 1478, 579, 585, 17, 393, 538, 11, 15, 845, 52, 483, 35], [1, 20, 5437, 89, 1754, 18927, 47, 761, 32, 526, 6496, 324, 662, 215, 6497, 4, 2, 6496, 9, 9125], [1, 3783, 151, 6, 313, 2, 34, 7, 3384, 88, 2222, 224, 1239, 166, 375, 59, 1583, 5, 286, 418, 148, 418, 148, 301, 301, 301], [1, 63, 65, 2856, 12, 293, 44, 407, 12, 44, 19, 1082, 1544, 7, 851, 197, 16, 999, 3825, 19, 815], [1, 3566, 537, 2940, 345, 6], [13, 1, 600, 202, 373, 281, 86, 556, 19905, 2068, 15, 2389], [1, 1, 63, 26495, 313, 646, 497, 192, 183, 51, 704, 6493, 5228, 498, 7, 497], [1, 1, 1, 1, 55, 178, 205, 20, 227, 2, 55, 274, 283, 6], [478, 6708, 527, 1791, 99, 1688, 38, 356, 2165, 449, 4, 98, 869, 23, 2804, 117, 17, 7795, 954, 180], [13, 1, 5, 1, 29071, 99, 202, 419, 16, 246, 403, 768, 3245, 31, 14255, 28, 273, 652, 33, 5127], [13, 1, 4822, 5, 1, 1, 6335, 4480, 254, 4411, 1258, 586, 23, 786, 587, 1619, 23, 786, 587, 2322, 775, 2, 510, 211, 29, 68], [1, 2083, 771, 131, 18281, 1545, 20, 26, 5, 237, 205, 5, 16, 673, 30, 11, 131, 3451], [1, 1, 1, 5186, 15, 575, 7742, 15, 149, 93, 5848, 1356, 80, 3158, 119, 634], [1, 1, 1, 340, 15, 213, 1533, 15, 2570], [1, 20, 38, 65, 6662, 51, 104, 2, 44, 715, 450, 203, 2136, 9, 145, 1171, 23, 127, 1665, 9, 595, 82, 20], [1801, 2301, 712, 203, 287, 64, 69, 203, 287, 41, 39, 45, 25, 1400, 64, 48, 1215, 527, 4, 884, 7, 23, 13452], [1, 147, 110, 716, 42, 178, 461, 6491, 155, 32, 2600, 82, 466, 2271, 7, 665, 87, 82, 63, 110], [13, 1, 1739, 679, 1633, 4181, 438, 452, 28628, 25181], [1, 3714, 4, 1495, 9, 2, 164, 48, 4046, 51, 104, 2, 4, 65, 572, 215, 6638, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 79, 36, 14, 92, 91, 314, 5, 2781, 1174], [1177, 351, 36, 377, 2140, 24, 5895, 9, 2596, 4921, 32, 32, 1935, 2422, 215, 6287], [107, 155, 16, 6011, 232, 48, 9388, 3743, 880, 22804, 12, 15205, 4, 25897, 41, 293, 192], [1, 1, 23624, 1, 38, 2, 1610, 910, 12, 2484, 2, 18, 23625, 1500, 4, 208, 1256, 9, 856, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 3559, 28833, 23391, 26, 133, 5785, 112, 547, 17, 77, 354, 210, 9, 2, 161, 1115, 2585, 225], [1, 32326, 1677, 15, 376, 288, 99, 6045, 6, 72, 452, 414], [112, 2155, 890, 4, 40, 180, 44, 19, 100, 17, 40, 1492, 1377], [1, 211, 6, 15, 321, 119, 3312, 1752, 19, 104, 2, 844, 321, 119, 245, 104, 2, 83, 244, 115, 83, 244, 1461], [30, 753, 7, 60, 60, 264], [1, 257, 930, 963, 2437, 3899, 11], [1, 1, 24, 5, 1, 3413, 122, 119, 82, 3150, 104, 412, 412, 69, 1136, 2, 794, 51], [277, 354, 167, 23, 30, 4, 282, 1309, 416, 286, 16, 23, 3088, 541, 16281, 27401, 3310, 227, 2235], [248, 6, 5, 510, 153, 900, 8096, 138, 606, 17, 5391, 1325], [4355, 6603, 59, 14, 102, 66, 29, 46, 125, 206, 1142, 6, 24674, 2, 37, 3, 8, 37, 50, 554, 56, 554], [1054, 323, 7189, 735, 1448, 2, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50, 236, 149, 4746, 14, 66, 29, 16, 37, 3, 8, 27, 50, 37, 3, 8, 27, 50], [1533, 46, 1498, 231, 1991, 6, 33, 808, 139, 411, 411, 5787, 181, 1748, 4042, 17, 105], [1, 1853, 1, 63, 6265, 364, 178, 205, 9007, 3, 9007, 3, 9007, 3, 38, 359, 1237, 84, 705, 699], [1367, 13, 390, 26, 425, 1041, 19, 2513, 3302, 1302, 83, 115, 24, 4, 37, 3, 8, 37, 50, 76, 61, 270, 249, 255], [1, 1, 24981, 149, 251, 58, 272, 754, 57, 9829, 726, 57, 12420, 404, 11, 393, 1690, 699], [1, 13264, 717, 2858, 161, 963, 6, 19392, 20, 222, 190, 180, 6164, 129, 717, 20, 2438, 361, 11, 300, 30, 114, 41, 1432], [13, 1, 81, 81, 320, 520, 6, 101, 10, 2898, 17635, 372, 6564, 566, 942, 6900, 25876, 25986], [1, 1, 1, 519, 9, 87, 2996, 1755, 1287, 220, 1875, 925, 4722], [1, 601, 41, 138, 111, 164, 26, 2294, 27063, 230, 4, 27064, 16, 110, 780, 6198, 26, 1785, 684], [1, 1, 5, 1, 10269, 25, 3821, 322, 2, 63, 124, 4, 41, 2, 4901, 4, 2241, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [13058, 7, 654, 4, 1319, 9, 186, 1054, 699, 3095, 77, 26, 9, 169, 3992, 30945, 5, 314, 1202, 315], [1, 1, 2875, 3074, 17, 77, 44, 19, 974, 20, 729, 5067, 456, 2, 95, 8625, 45, 16, 2, 20, 11], [182, 2632, 20, 216, 1705, 14, 23564], [1, 1, 1, 96, 293, 665, 445, 104, 47, 2319, 51, 31895, 1013, 437, 410, 118, 7, 665, 51, 265], [1, 1, 6714, 1, 20528, 133, 127, 4, 1909, 540, 5590, 2956, 54, 133, 127, 11, 254], [85, 20281, 1894, 11, 1096, 111, 12, 2013, 197, 12, 167, 48, 15066, 230, 12, 2549, 5002, 1363, 11], [1, 1, 133, 2473, 4, 1033, 1, 1913, 24842, 435, 1432, 12, 8929, 133, 2473, 4, 1033, 1, 2730, 3122], [1, 56, 9245, 13098, 25, 20, 11673, 504, 8346, 870, 20, 368, 27614, 27615], [24934, 24935, 9, 24936, 24937, 109, 24938, 2666, 2, 136, 39, 8798, 129, 2, 1408, 1085, 2, 219, 573, 7631, 7076, 5, 642], [1, 9270, 7, 946, 9, 227, 138, 38, 846, 2, 1143, 2527, 1261, 29, 289, 2071], [6293, 384, 3281, 380, 231, 228, 370, 25, 1014, 2, 252, 6293, 34, 228, 359, 41, 316], [18, 80, 16999, 28, 376, 467, 14, 20922, 99, 45, 288, 42, 42, 6, 72, 550, 86, 14, 1461], [1, 1, 88, 128, 112, 11, 1759, 41, 217, 212, 479, 85, 330, 2993, 1315, 6, 5, 16, 23, 71, 25, 840], [18, 93, 7254, 58, 1984, 5707], [13, 1, 1, 1, 1098, 39, 2335, 436, 4589, 4315, 6, 10607, 23, 23613, 483, 23, 2510, 8189, 38, 1171, 2662, 51], [339, 29244, 62, 7284, 58, 15, 28], [1, 1, 56, 133, 127, 313, 2, 26, 78, 161, 127, 7, 197, 449, 2, 481, 3, 20, 1726, 4027], [1, 2112, 5, 24, 9, 152, 152, 2290, 12, 332, 443, 3651, 186, 2, 344, 1636, 186, 5068, 17, 4], [21989, 1789, 973, 151, 89, 896, 983, 302, 1], [1, 1, 1, 911, 11, 18815, 121, 18816, 3571, 196, 8, 2212, 6479, 528, 270, 249, 255, 196, 8, 2212, 6479, 528, 270, 249, 255, 196, 8, 2212, 6479, 528, 270, 249, 255, 4522, 162, 1977, 177, 4, 928], [257, 1233, 256, 7284, 148, 256, 363, 93, 1482, 635, 4571, 363, 755, 6, 99, 6, 26286, 2503, 99, 80, 962, 1225, 309, 588, 653], [712, 1407, 1, 1178, 9, 126], [1, 2138, 476, 5395, 84, 369, 47, 116, 7, 286, 1551, 119, 2255, 19, 262, 27970, 1948, 44, 19, 5850, 20, 2415], [20, 47, 1, 24, 703, 5, 227, 430, 1413, 64, 4, 230, 157, 2996, 19, 129, 2, 71, 43, 26], [1, 1, 1, 1, 1, 78, 128, 293, 11, 8749, 45, 133, 6000, 23], [1, 93, 5239, 5, 11, 956, 138, 6, 5, 6, 30, 82, 22631, 3663, 2555, 104, 2], [3504, 175, 379, 6, 251, 467, 15, 459, 224, 2178, 5911, 802, 27, 1026, 11397, 1037], [49, 11934, 15, 686, 31, 117, 13524, 148, 6, 43, 749, 17273, 20258, 1733, 62, 1718, 174, 29], [109, 3522, 563, 19, 5028, 228, 96, 316, 6209, 3078, 333, 8114, 881, 4782, 5294, 5253, 333, 8114], [1, 1, 783, 783, 2914, 783, 2609, 11, 8580, 461, 6, 564, 16, 670, 2, 212, 4356, 354], [1, 1, 23540, 23506, 12, 192, 9105, 11, 853, 51, 225, 54, 227, 11, 120, 23, 10697, 138, 3, 8, 22, 10, 21], [1497, 6, 9301, 122, 127, 45, 7689, 16, 17, 128, 57, 7, 25, 672, 416, 2, 9301, 954, 78, 127, 7, 25], [1, 1, 34, 4, 17, 53, 60, 584, 649, 299, 26982, 3559, 17, 9042, 239], [1, 724, 1, 1, 162, 1350, 4341, 4, 248, 20, 650, 2, 145, 1, 32, 1227, 25, 1992], [42, 33, 86, 291, 327, 1633, 1295, 2605, 31, 14, 428], [1, 2141, 16, 43, 7, 330, 1841, 248], [1, 1, 35, 6133, 489, 903, 12, 90, 34, 7, 60, 60, 3385, 151, 17, 2812, 9601, 313, 502, 120], [2886, 26, 23556, 56, 16, 54, 3469, 232, 2, 16, 56, 16, 100, 890, 4, 3469, 705, 2, 3, 8, 22, 10, 21], [1, 4581, 97, 18899, 4800, 164, 1027, 9, 241], [224, 42, 33, 915, 530, 486], [1, 5, 1, 1, 1, 473, 1, 1, 1, 1, 1, 7966, 5, 165, 130, 3, 165, 130, 3, 16, 265, 29873, 398, 165, 130, 3, 165, 130, 3], [1, 1, 1, 1, 1, 1, 1, 872, 29404, 2], [410, 6, 31, 49, 1168, 7567, 11, 540, 7655, 6375, 131, 241, 1333, 3], [1, 1, 18171, 1118, 99, 6, 118, 155, 29, 68, 174, 1241, 49, 15, 216, 202, 756, 101], [125, 206, 12, 361, 16, 54, 1552, 9086, 4, 3070, 8626, 11640, 7, 28028, 1377, 1552, 9086, 18, 66, 29, 2088, 5, 29], [346, 391, 48, 53, 4, 111, 148, 97, 3950, 1305, 12, 157, 19, 104, 7, 48, 3583, 111, 131, 997, 269, 2, 346, 298], [1, 7201, 911, 1406, 63, 7, 9, 980, 72, 4, 460, 186, 805, 923, 591, 63, 501, 32, 10707, 1291, 832, 4, 431], [13, 1, 1, 25811, 16953, 3438, 17906, 500, 42, 324], [13, 1, 7697, 4093, 3816, 11, 84, 5477, 140, 39, 672, 456, 2, 6, 616, 4984, 969, 2, 44, 621, 7, 3218, 398], [1, 88, 31186, 10964, 5096, 236, 18, 93, 8, 29, 33, 1695], [1, 1, 38, 65, 715, 2306, 968, 3087, 131, 369, 238, 85, 898, 145, 96], [1, 187, 55, 488, 73, 43, 460, 166, 1474, 28, 243, 73, 213, 14, 1053, 43, 6466, 21184], [1, 1, 572, 4581, 4, 16, 9920, 84, 501, 7, 24573, 1194, 3410], [1, 1, 351, 36, 117, 59, 276, 2090, 83, 1670, 4353, 335, 18, 2625, 4565, 6792], [1, 1, 579, 4, 34, 112, 32, 6786, 9594, 11, 7858, 2, 377, 20, 14698, 53, 12, 247], [13, 1, 1824, 46, 1885, 460, 14, 915, 8573, 21998, 5186, 5, 5, 8574, 3878, 227, 9, 3729, 21999, 355, 4, 178, 476, 20888, 1003], [1, 5, 5, 78, 245, 322, 2, 109, 9086, 17, 519, 1081, 4534, 41, 64, 165, 130, 3, 86, 4986, 830, 41, 64, 180, 41, 1562, 747, 227, 2], [13, 1, 1, 5616, 6488, 192, 26, 6580, 11, 8494, 456, 2, 4650, 26, 638, 11991, 12, 65, 6132], [1, 4265, 2634, 556, 154, 15, 36, 33, 2362, 1333, 3, 42, 68, 19722, 1184, 8023], [1, 147, 54, 10679, 2, 74, 17, 1, 261, 1271, 113, 44, 701, 109, 78, 350, 23, 9], [13, 1, 14423, 2787, 538, 11, 490, 20, 9, 161, 1756, 997, 118, 2787, 44, 541, 219, 20, 27638, 47, 541, 526, 4984, 2207, 2201, 47], [1, 5463, 1, 18, 109, 7, 433, 188, 1767, 516, 2, 45, 20, 399, 425, 715, 1710, 1382, 22466, 88, 5727, 220, 54], [1, 133, 127, 3978, 9309, 710, 78, 155, 12, 185, 82, 2317, 2, 178, 51, 12050, 610], [1, 133, 239, 4, 3978, 14885, 1614, 4556, 133, 2181, 8310, 4, 239, 25611, 8770, 4196, 26076], [13, 1, 1, 5724, 1, 11, 4924, 447, 2, 20, 1, 2117, 67, 359, 205, 100, 5, 2356, 9, 1149, 31125], [1, 1, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 31, 717, 124, 4, 2868, 32, 951, 2868, 4, 127, 8510, 284, 145, 17, 1278, 7392, 2355, 2620], [2346, 195, 6171, 2968, 548, 192, 5696, 2391, 125, 66, 116, 30, 24, 156, 2900, 47], [1, 109, 1683, 282, 365, 78, 2940, 17, 45, 1151, 190, 626, 430, 2395], [1, 16843, 4012, 12759, 16, 4582, 579, 12, 8741, 6786, 16, 609, 4, 1043, 650, 40, 5, 314, 12, 332, 609, 40, 25, 9], [1, 1, 184, 32, 23, 846, 64, 30, 12, 1339, 88, 8968, 17, 2540, 322, 2, 54, 39, 472, 39, 41, 2], [1, 1, 34, 11, 6, 192, 227, 25, 9, 96, 1409, 45, 761, 4, 38, 1394], [455, 1428, 270, 628, 8853, 4, 1535, 9, 444, 455, 32, 126, 10561, 455, 1428, 270, 628, 8853, 8375, 16, 4, 704, 1444], [1, 95, 120, 857, 128, 4603, 7, 41, 2318, 26, 1203, 5, 164, 98, 161, 170, 9183, 38, 1964, 383], [20, 1660, 32, 54, 1695, 3172, 4, 3436, 2025, 2, 8801, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 109, 788, 715, 963, 97, 23382, 1449, 158, 2, 3, 8, 1390, 139, 1295], [1, 145, 1539, 107, 7, 926, 449, 2, 11626, 12, 3418, 7903, 11, 3557, 313, 2, 6, 10274, 124, 4, 29620], [1, 1348, 1, 30663, 450, 31310, 844, 107, 4, 26898, 704, 2, 45, 155, 107, 11, 25, 445, 129, 2], [1, 1, 467, 2715, 7, 356, 6042, 9, 2468, 229, 73, 4271, 166, 76, 61, 76, 61, 76, 61], [13, 1, 288, 696, 27040, 6474, 6129, 2104, 11620, 575, 1653, 2912, 55, 4494, 46], [12642, 5, 2451, 188, 278, 108, 479, 9, 9285, 26, 131, 5122, 6, 118, 237, 332, 136, 6, 122, 1313, 136, 20, 2637], [1, 1, 191, 177, 23, 22563, 2, 20, 725, 886, 53, 12, 2961, 78, 274, 283, 4, 1294], [1, 11997, 5, 1, 1446, 479, 128, 4, 127, 811, 969, 2, 6, 621, 8550, 457, 445, 821, 20, 669, 47, 347, 21736], [1, 116, 30, 24, 7, 1926, 1207, 157, 2137, 17, 1207, 5455, 538, 188, 16, 648, 4, 45, 17, 17, 617], [13, 1, 1, 1, 30, 18, 897, 29, 117, 40, 18, 93, 531, 10, 1345, 40, 18, 531, 10, 1358, 10, 724], [13, 1, 7, 155, 17, 1077, 40, 6, 155, 10287, 17, 25352, 1065, 178, 51, 118, 6, 922, 4, 1164, 43, 164, 40, 4316, 2919], [1, 1, 1, 715, 5544, 4, 4063, 16, 1194, 1486, 5, 237, 292, 5067, 2, 1118, 158, 77, 131, 2, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 35, 2210, 1953, 16, 1996, 1364, 163, 888, 124, 25, 1228, 2, 4860, 7, 9, 60, 337], [1520, 427, 65, 323, 2182, 3793, 1015, 3], [13, 1, 175, 5, 132, 6, 554, 1, 151, 6, 25749, 1180, 2, 24, 516, 1191, 1005, 5, 3337, 174, 390, 235, 54, 342, 142, 442, 753], [32, 277, 9437, 8963, 1345, 6874, 7, 10531, 51, 87, 21449, 97, 5098, 19, 108, 26, 228, 2702, 13445], [1, 3366, 4395, 43, 17, 63, 65, 48, 2349, 19, 2900, 19, 3366, 4395, 107, 17, 140, 100, 4124, 2204], [1, 1, 274, 1495, 20, 2, 4, 71, 44, 1255, 2, 78, 2, 54, 7960, 825, 54, 537, 368, 135], [13, 1, 1, 133, 2443, 22687, 4, 19, 391, 594, 413, 149, 12161, 22688, 764, 6230, 149, 2683], [1, 1, 16, 25966, 57, 203, 6101, 24, 727, 9, 1972, 718, 111], [1, 1, 3655, 5, 5, 4930, 29, 46, 101, 46, 101, 18, 1022, 86, 3106], [1, 1, 1, 1, 1, 121, 85, 1038, 85, 23984, 85], [133, 17, 133, 571, 2644, 2, 3487, 4, 3666, 78, 96, 775, 41, 51, 269, 2, 230, 25359, 41], [13, 1, 5, 3327, 1, 1, 10839, 1, 1105, 1, 1, 5, 1, 11718, 345, 563, 300, 82], [1, 1, 370, 17036, 232, 64, 44, 78, 13190, 857, 84, 247, 5452, 290, 407, 188], [1, 1184, 1384, 6, 9, 10260, 805, 1764, 508, 2, 530, 67, 1, 20, 37, 3, 8, 37, 50], [1, 70, 81, 505, 1760, 15838, 275, 15, 166, 524, 1786], [1, 1, 52, 8133, 142, 12839, 559, 834, 10, 5805, 1791, 1052, 10, 146, 351, 5078], [339, 184, 29, 350, 535, 6, 118, 137, 118, 1212, 18, 1212, 6202, 1047, 12, 7], [13, 1, 20191, 11995, 78, 648, 591, 109, 436, 47, 6778, 1560, 1568, 5, 334], [13, 1, 144, 1320, 7385, 6453, 97, 34, 308, 7194, 25373, 4613, 922, 25374, 627, 151, 8231, 19, 18, 995, 266], [18, 286, 16, 133, 730, 227, 430, 939, 292, 230, 747, 29, 17, 8501, 2089, 504, 3210, 20101, 2625, 44, 181, 135], [13, 1, 343, 3, 8, 260, 28, 1220, 55, 656, 5673, 1044, 4750, 11395, 28, 362, 2040, 606, 19], [1, 191, 177, 140, 8358, 23, 6, 23, 133, 7053, 9, 2839, 26, 944, 3672, 1660, 4543, 3157, 509, 120], [1, 10541, 38, 4594, 693, 4145, 186, 491, 71, 248, 153, 240, 491, 117, 4, 1392, 4, 229, 458, 38, 16544], [1, 1, 2260, 3676, 32, 1283, 13420, 26, 927, 5087, 1350, 7, 665, 87, 6, 120, 53, 12, 2232], [9556, 62, 379, 6, 2352, 295, 2678, 28, 103, 103, 2126, 488], [70, 81, 6, 15, 811, 6049, 115, 30820, 2832, 8334, 531, 2344, 271, 5407, 11573, 557, 442, 10, 2376], [3808, 1907, 49, 173, 562, 24, 299, 1844, 17, 122, 119, 20, 84, 3882], [1, 1849, 1398, 2592, 38, 13015, 12876, 13015, 425, 39, 23, 9, 85, 631], [1, 1, 654, 6, 2227, 2, 4, 53, 204, 9240, 97, 9, 204, 2164, 97, 9, 77], [1, 257, 2396, 2570, 6295, 1721, 1193, 665, 4166, 37, 3, 8, 640], [1, 1, 566, 5, 102, 341, 50, 24992], [1, 119, 17, 522, 100, 9463, 7, 4299, 186, 2, 45, 65, 192, 1757, 3567, 9, 38, 96, 65, 5722], [34, 417, 3547, 65, 38, 53, 4, 3045, 51, 104, 47, 5715, 12, 1109, 17, 207, 27312, 26, 22384, 11, 1868, 6, 4919, 138], [1, 814, 331, 221, 78, 2, 592, 4, 4276, 967, 95, 2185, 7, 197, 164], [175, 5, 132, 6, 554, 1, 151, 6, 25749, 1180, 2, 24, 516, 1191, 1005, 5, 3337, 174, 390, 235, 54, 342, 142, 442, 135], [1, 7050, 2, 4922, 30, 24, 7051, 26, 14123, 9225, 7052], [1, 242, 111, 16, 128, 7, 41, 2474, 229, 85, 5, 16, 1929, 7, 545, 16, 1425, 1783], [15, 251, 15, 42, 12884, 86, 15, 457, 312, 9656, 6, 20236, 1821, 58, 15, 251, 15, 42, 794, 86, 15, 6161, 326], [71, 15178, 64, 12, 4, 19632, 15756, 4, 2095, 2, 162, 237, 8087, 132, 5628, 197, 1030, 4, 1633, 7, 8033, 88], [1, 35, 24, 97, 475, 5319, 199, 702, 17, 34, 7, 8026, 28473, 28474, 8027, 28475, 28476, 12, 28477], [3041, 31, 7220, 31, 46, 1498, 213, 31, 2394, 1661, 2442, 2336, 46, 28, 6852, 2336, 73, 1651, 3403, 406, 93], [13, 1, 435, 97, 1, 62, 55, 246, 73, 123, 13434, 1, 5, 14, 19339, 1695, 10, 291, 626, 62, 55, 246, 10, 15, 135], [1, 1, 662, 57, 110, 9, 6973, 1334, 57, 110, 9, 6973, 1334, 16, 23, 121, 32564], [1, 78, 368, 789, 4, 174, 1201, 1499, 16, 726, 1131, 191, 45, 133, 3233, 11, 2750, 1620, 3373, 3266, 121], [1, 1727, 7, 625, 1451, 182, 87, 205, 1096, 111, 6512, 100, 227, 1632, 44, 32530, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [1, 5, 42, 28, 23127, 55, 11317, 52, 469], [31, 7, 361, 4893, 9, 1377, 26, 38, 5, 203, 251, 1263, 88, 1170, 3114, 47, 117, 11, 12667, 26730, 31, 4], [1, 1, 1, 3144, 60, 60, 264, 204, 6197, 17, 23, 83, 303, 115], [1, 18432, 653, 1017, 1308, 95, 34, 7, 2296, 2779, 18, 202, 3782, 398, 634, 589, 104, 19], [1, 1, 5, 8066, 947, 29, 1084, 7386, 9], [1, 9, 24555, 20, 711, 5791, 40, 38, 4, 2399, 445, 2900, 40, 880, 71, 65, 94, 4876], [13, 1, 1872, 306, 1210, 1574, 24, 144, 215, 10506, 56, 1810, 149, 72, 6955, 125, 168, 5, 73, 1810, 18, 1574, 24, 2129], [54, 399, 2, 10855, 1049, 10614, 16, 7855, 2, 66, 13205, 4421, 377, 18518, 1, 18518], [13, 1, 663, 128, 17, 39, 20388, 20389, 9, 2, 321, 43, 26, 770, 4, 8374, 11, 2265, 259, 2, 26, 11586, 5211, 11], [1, 430, 181, 43, 3879, 409, 191, 5148, 11, 39, 3850, 6236, 6236, 6236, 259, 8448], [1, 1800, 5, 681, 6, 1384, 277, 19, 391, 595, 429, 1578, 2672, 6, 9, 5604, 87, 407, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21], [8, 1124, 7870, 421, 18, 3991, 583, 211, 6, 127, 28, 2582, 12998], [63, 368, 356, 659, 6287, 124, 32, 11852, 212, 248, 23, 9, 4923, 999, 646, 2450, 107, 155], [1, 765, 7053, 8899, 172, 14612, 4, 10741, 28, 689, 518, 58, 793, 760, 754, 23, 25657, 4, 442, 10741, 380, 3439, 19], [1, 497, 32, 18, 6415, 4, 150, 794, 899, 26, 546, 12, 25, 174, 4435], [1, 1045, 20, 1030, 11, 3264, 178, 158, 54, 335, 18, 80, 32069, 31, 656, 3889, 2441, 4512, 3, 8, 22, 10, 21], [1, 731, 3, 1198, 52, 477, 7357, 613, 3, 75, 27, 3901, 151, 19, 521, 316, 1065, 28, 340, 16, 913, 37, 3, 8, 640, 3, 171, 141, 3, 171, 141, 258, 198], [83, 4803, 5318, 351, 43, 351, 30, 351, 31420, 351, 3513], [13, 1, 3586, 5, 7447, 85, 209, 43, 7, 5002, 12, 90, 2452, 84, 705, 11, 3586, 5, 3783, 11, 5131], [1359, 16, 43, 531, 4, 1535, 11, 2283, 1687, 5394, 1089, 2, 43, 330, 2283, 1687, 12, 17699, 4, 17700, 4], [1, 39, 41, 1389, 16, 5, 5, 174, 1179, 7531, 6, 1434, 3185, 11, 1194, 8609, 330, 4, 150, 541], [1, 1, 52, 1572, 111, 29, 221, 111, 1442, 342], [13, 1, 1, 1, 5, 1, 1, 1242, 10887, 4575, 18, 93, 2967, 68, 3716, 55, 59, 49, 187], [1, 1, 1, 6, 290, 6144, 971, 4, 6752, 476, 444, 2584, 12, 4172, 241, 6, 85, 12, 498, 241], [1, 94, 4, 2399, 63, 17, 2684, 16667, 3169, 39, 282, 604, 18467, 121, 4, 739, 121, 4, 1678, 121, 4], [13, 1, 32468, 628, 3456, 1941, 523, 757, 4, 156, 215, 420, 264], [329, 329, 143, 6, 43, 112, 28, 656, 7584, 5221, 52, 421, 202, 438, 52, 2801, 8], [5, 17, 5, 1305, 120, 265, 262, 26, 63, 221, 1812, 111, 7, 1684, 12, 1802, 616, 10191, 10191, 17, 23089, 12, 4522, 30], [13, 1, 494, 73, 951, 18, 93, 4155, 2754, 28715, 2506, 234, 62, 325, 6, 166, 2506, 114, 575, 462, 270, 19372, 312], [1, 1, 1310, 30, 24, 252, 664, 117, 31, 46, 3173], [52, 189, 195, 1296, 496, 1, 24, 211, 29, 1211, 76, 61, 76, 61, 340, 15, 49, 173, 1296, 496], [1, 133, 239, 11, 7769, 674, 84, 3006, 11671, 4, 5164, 221, 293, 12, 8376], [1, 4871, 219, 23, 4147, 400, 38, 1995, 89, 47, 34, 17, 25, 2662, 275, 15, 902, 2613, 193, 14759, 1295, 1295, 1295], [1, 78, 3540, 2, 136, 11, 1531, 3206, 571, 41, 19, 6, 1546, 1312, 6876, 97, 11377, 97, 1593], [1, 3361, 68, 328, 25971, 18, 14, 1400, 847, 632, 187, 1718, 438, 1212, 1474, 6, 14], [109, 270, 2084, 254, 248, 100, 1420, 23, 9, 158, 361, 4996, 579, 1647, 17, 1835, 32, 1722, 665, 2484, 2], [83, 303, 115, 76, 61, 76, 61], [1, 107, 1, 730, 997, 254, 124, 32, 18, 1166, 5620, 7, 5188, 84, 12, 3375, 2468, 8464], [1, 1, 2523, 603, 236, 18, 5027, 5, 3731, 31, 442, 2603, 236, 93, 114, 18768, 618, 210], [1, 4796, 366, 24459, 31371, 1436, 3, 134, 15, 3, 171, 141], [1, 1, 1, 45, 1437, 545, 16, 40, 1140, 298, 121, 11, 672], [460, 19, 578, 39, 2349, 1251, 17, 15054, 51, 23, 131, 445, 322, 4999, 480, 7, 5837, 16, 41, 87, 138, 807, 32118, 126], [13, 1, 1090, 58, 18, 80, 73, 3177, 2608, 719, 99, 615, 272, 2522, 31, 14, 1584, 10, 8626, 45, 2247, 314], [1, 1332, 1375, 149, 72, 31, 14, 66, 2465, 214, 3596, 500, 272, 375, 1286, 68, 985, 86], [1, 1293, 24, 455, 192, 183, 240, 3939, 17, 41, 84, 705, 16, 44, 6, 5189, 240, 105], [13, 1, 351, 18, 1505, 10703, 6, 15, 10704, 288, 251, 351, 36, 15, 590, 2, 34, 17719, 215, 7946], [13, 1, 85, 3879, 409, 5232, 2997, 36, 14, 1451, 29, 274, 283, 1662, 529, 2747, 20, 2961, 111, 1679, 2128, 274, 24, 7], [13, 1, 1, 1, 57, 9, 7365, 1146, 5986, 2781, 1199, 47, 38, 53, 16, 107, 1163, 7, 2240, 84, 5987, 7], [13, 1, 1, 34, 6, 935, 40, 232, 541, 77, 443, 40, 2334, 19, 23064, 6, 12, 1215, 32, 1043, 6026, 286, 16], [1, 134, 15, 203, 9223, 1, 254, 1495, 1035, 27, 62, 25150, 5563, 1035, 27, 9223], [13, 1, 1, 1, 1, 5391, 1, 1, 1, 1, 1, 1], [1, 248, 147, 110, 26474, 6574, 12, 9046, 7, 65, 574, 25, 870, 44, 2017, 967, 1882, 4930, 362, 278], [324, 39, 28781, 210, 26, 4005, 589, 11838, 7457, 210, 26249, 961, 2727, 23, 9, 169, 35, 2078], [1, 62, 59, 55, 916, 326, 184, 1991, 172, 98, 7, 98, 2731, 84, 311, 41, 1003, 3973], [1, 96, 211, 5347, 6, 21536, 30, 16862, 6390, 19279, 6390, 330, 603, 1256, 51, 603, 120], [1, 35, 1956, 226, 290, 30, 209, 26, 105, 35, 57, 150, 26, 194, 633, 113, 145, 153, 30, 24, 666, 5, 1504, 21813], [1, 12449, 26, 15676, 998, 489, 868, 371, 30, 24, 420, 253, 119, 775, 47, 82, 778, 17, 163, 57], [1, 3470, 24, 1102, 64, 2125, 150, 13489, 241, 2, 54, 158, 34, 31161, 9, 136, 150, 12, 13489, 9], [1, 1, 374, 38, 23, 259, 47, 2285, 8637, 25, 169, 2, 6, 290, 96, 4902, 476], [1, 107, 1318, 26, 12738, 12, 4523, 65, 26924, 11, 51, 104, 47, 26, 53, 12, 90, 18379, 113], [1, 1, 1, 1260, 1, 1, 1, 299, 60, 2243, 456, 217], [6230, 1045, 12, 3463, 16, 5, 547, 1542, 23489, 3463, 358, 461, 229, 16291, 5564, 29, 10951, 42, 392, 61, 392, 61, 392, 61, 392, 61, 392, 61, 392, 61, 392, 61], [1, 944, 653, 12773, 1015, 3, 1015, 3, 570, 737, 402, 472, 165, 130, 3, 1979, 44, 654, 89, 364, 229, 219, 402, 472, 737], [1, 74, 4, 230, 43, 17, 4106, 2, 26, 56, 4, 230, 311, 2, 807, 6, 56, 32, 5, 1542, 43, 7, 254], [1, 1, 3370, 20, 293, 4, 1817, 748, 128, 7, 20, 9, 2311, 4, 7460, 104, 12, 621, 7680], [1, 277, 1077, 182, 1604, 4, 349, 43, 16, 1486, 471, 1, 547, 512, 11054, 16, 1487, 6, 4873, 4304], [1, 462, 13429, 7147, 351, 36, 15, 9838, 637, 6706, 11, 2498, 841, 6132, 188, 771, 32586, 13429, 19], [1, 4624, 5, 7, 5, 97, 40, 121, 1312, 48, 5, 2, 384, 20, 119, 6586, 538, 178, 541, 2831, 871, 84], [1, 54, 7760, 7760, 259, 19, 610, 7811, 325, 14, 1417, 21576, 470, 11], [1, 62, 149, 93, 393, 46, 588, 2, 74, 2785, 2, 608, 544, 2526, 87, 2], [1, 4334, 412, 39, 665, 2621, 9, 313, 136, 6645, 2, 1359, 89, 43, 5037, 4], [1, 1, 575, 459, 49, 612, 42, 268, 15, 575], [13, 1, 1, 143, 29, 14, 921, 1928, 2377, 2, 34, 12, 19552, 422, 5170, 7, 28, 317], [1, 109, 219, 6, 228, 29767, 797, 207, 11, 3, 8, 79, 50, 3, 8, 79, 50], [3251, 7508, 1497, 103, 2432, 203, 9121, 390, 3502, 1454, 1773, 25, 1454, 263, 2513, 30, 24], [13, 1, 326, 790, 57, 191, 882, 927, 465, 2943, 58], [1, 256, 2415, 1119, 373, 332, 5, 642, 8033, 144, 583, 2292], [1, 6756, 13518, 164, 6, 98, 1318, 12, 4899, 47, 71, 152, 23, 19030, 4, 664, 437, 737], [1, 227, 64, 13265, 769, 332, 20, 25, 2552, 45, 77, 6, 2188, 416, 107, 155, 45, 56, 74, 197, 10780], [1, 1, 1, 63, 54, 5156, 1475, 100, 2696, 25, 9, 2, 44], [137, 2308, 18, 46, 708, 15, 438, 214, 1474, 499, 29, 15], [1, 540, 4, 1850, 2, 20, 4, 6257, 776, 5, 4390, 16, 672, 506, 132, 1284, 41, 18, 7], [1, 105, 105, 105, 3226, 4356, 3620, 5610, 11, 1324, 3154, 26, 1905, 16, 29812, 12, 1119, 916], [13, 1, 14124, 3077, 1, 5, 70, 28711, 28712, 37, 3, 8, 27, 50, 275, 15, 59, 14, 28713, 31, 14, 125, 37, 3, 8, 27, 50, 3187, 3442, 8468], [1, 134, 15, 29, 59, 15, 99, 582, 29, 14, 1261, 31, 14602, 59, 14, 66, 31, 459, 1712, 6, 15, 359], [4742, 512, 7, 452, 20652, 4, 2184, 1204, 17, 20653, 444, 20654, 300, 5, 11713], [1, 214, 4449, 2997, 15086, 158, 136, 9, 20, 6, 571, 430, 365, 82, 38, 85, 112, 7, 681, 8494, 158], [3915, 1179, 40, 41, 19, 232, 11], [331, 610, 26512, 797, 465, 89, 12, 2554, 54, 890, 4, 2239, 11, 2869, 747, 227, 41, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3, 8, 22, 10, 21, 3103, 23, 300, 431], [1, 1, 214, 2322, 16745, 2452, 6, 1795, 229, 411, 10651, 305, 182, 2016, 4, 625, 7, 625, 182, 931], [11192, 5066, 8963, 28, 58, 18, 14, 66, 587, 492, 2958, 14, 6144, 2940, 18, 3935, 3], [1, 120, 5, 29835, 1186, 151, 426, 568, 2, 94, 266, 535, 7, 7862, 266, 535, 7, 9220, 266], [1, 2792, 4723, 2589, 4723, 23, 6, 6793, 73, 77, 158, 9, 63, 65, 23, 7, 1269, 265, 12, 2366, 471, 77, 138], [1, 1, 1, 1, 9830, 730, 2, 25, 54, 12738, 1329, 3475, 12, 3532, 210, 447, 2592, 3], [1, 13468, 280, 3, 8, 22, 10, 21], [1, 2553, 174, 9117, 24656, 3817, 71, 174, 859, 7715, 6, 20925, 2611, 2074, 47, 780, 1734, 1153, 969, 307, 3, 307, 3, 307, 3], [7872, 10636, 10636, 10636, 26465, 2309, 7872, 946, 5, 2599, 109, 5, 2599, 23, 282, 449, 1], [1, 5910, 3401, 1006, 6, 8825, 902, 1928, 60, 158, 20008, 11, 185, 48, 103, 6252, 12, 185], [1, 5, 1, 1, 154, 14, 1218, 10, 85, 1759, 3162, 230, 32, 1978, 87, 2, 290, 23, 9, 4923], [1, 1, 1, 714, 12, 2232, 197, 11, 26154, 6, 681, 468, 337, 26155, 4214, 1418, 20, 6, 26156], [1, 96, 2032, 7, 5491, 12, 7352, 67, 6819, 67, 983, 1737, 2466, 983, 15965, 11, 672, 1, 5, 333, 263, 12, 6036, 51], [1, 1, 1416, 74, 11, 5437, 1891, 9, 2, 96, 6, 410, 783, 2, 38, 108, 2], [1581, 77, 3251, 32, 1728, 9555, 17, 3549, 5826, 4005, 856, 14, 148, 455, 98, 7, 121, 2884, 84, 1429, 444, 34, 385, 7], [1, 1, 1, 1, 1, 1, 10975, 20, 6, 6395, 19, 391, 11497, 4], [140, 274, 283, 24, 645, 6463, 183, 856, 23, 1359, 17, 156, 6, 27127, 286, 25, 9, 354, 229, 274], [1, 6, 120, 17, 95, 1600, 7, 6, 1780, 405, 1179, 17, 1179, 2693, 12, 727, 6, 20, 245, 825, 201, 716, 188], [13, 1, 1, 13, 35, 32, 5, 1005, 16, 26618, 4, 38, 11, 1966, 1700, 297, 76, 61, 270, 249, 255, 76, 61, 270, 249, 255, 2877, 1], [1, 8070, 66, 2, 85, 4, 442, 877, 16, 4901, 12, 332, 239, 39, 6646, 278, 705, 45, 44, 25, 1224, 385, 5067, 2], [1, 1, 33, 916, 28, 1165, 19372, 531, 6320, 3142, 8596, 512, 1395], [1, 1956, 238, 2519, 11, 20997, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 176, 3, 8, 327, 50, 6, 2, 10107, 23, 26, 6, 26, 1244, 606, 262, 385, 176, 3, 8, 37, 50, 176, 3, 8, 37, 50, 176, 3, 8, 37, 50], [73, 4295, 167, 28, 5632, 86, 93, 6, 1360, 3145, 5289, 95, 4905, 4], [49, 494, 373, 491, 31, 17838, 16, 122, 18060, 14, 86, 323, 45, 195, 1692, 7, 394, 21621, 39, 4949], [1, 214, 1021, 49, 167, 34, 3396, 9, 2, 194, 4358, 108, 2, 28648, 662, 32], [1, 57, 150, 3846, 3183, 12, 764, 181, 498, 39, 111, 9, 297, 322, 53, 4, 3488, 22865, 2, 3488], [1, 107, 1078, 124, 7, 95, 1249, 11, 185, 227, 41, 138, 430, 1652, 4, 5000, 100, 96, 3146, 78], [334, 18, 2665, 889, 58, 18, 31436, 73, 15, 19630, 10026, 29, 1681], [1, 1, 24, 214, 8, 15, 34, 38, 23, 117, 49, 1168, 7, 569, 57, 1115, 1136, 569, 20, 3773, 170], [13, 1, 2175, 39, 5663, 2, 54, 30, 24, 26, 413, 24, 7, 2209, 6, 795, 67, 278, 51, 43, 11, 209, 297, 825, 26, 1298, 893], [1, 1, 8018, 1, 26, 20, 38, 1706, 1507, 1706, 3798, 40, 48, 990, 95, 98, 12, 25, 7230, 16, 1425, 247, 4410, 5, 572], [1, 62, 460, 14, 66, 184, 31, 3705, 139, 8006, 1173, 11878, 656, 21698], [1, 1, 77, 9998, 3116, 41, 84, 705, 2, 2879, 442, 32, 701, 4421, 87, 64, 6800, 6833, 431], [1, 11171, 111, 85, 12, 90, 41, 20, 100, 194, 227, 430, 25, 365, 138, 864, 138, 4, 85, 12], [1, 565, 1, 20, 261, 7515, 27919, 3453, 637, 2711, 16, 2585, 12, 95, 25, 155], [2698, 755, 3683, 6, 190, 176, 3, 8, 576, 42, 6, 1353, 154, 58, 2641, 13913, 328, 3429, 2935, 1], [1, 35, 1097, 778, 89, 43, 531, 1701, 6657, 7, 476, 32195, 129, 2, 43, 89, 43, 11, 71], [1, 20, 606, 670, 2, 18964, 717, 1518, 133, 127, 11, 97, 6, 924, 22991, 38, 2, 2434, 181, 133], [13, 1, 15348, 144, 432, 1, 4268, 36, 23138, 14, 9092, 10, 1237], [13, 1, 1209, 20, 163, 336, 90, 299, 739, 11, 13238, 1805, 38, 163, 534, 8640, 17, 25, 338, 1, 1, 3213], [1, 2029, 11, 7661, 4647, 127, 7, 4582, 2523, 127, 11, 1477, 672, 161, 127, 44, 398, 51], [1, 342, 6000, 1440, 1773, 266, 17, 178, 364, 696, 59, 6000, 18, 18, 874, 4581, 29, 1400], [74, 18, 14, 66, 170, 5, 2847, 12, 361, 7, 5, 5, 2847, 16, 30537, 39, 1709, 150, 41], [214, 793, 1, 213, 102, 52, 179, 1192, 29786, 752, 49, 173, 31, 101, 2082, 647, 72, 1088, 1, 4666, 6, 15, 9257, 949], [1, 35, 901, 4317, 627, 1764, 226, 4058, 2, 1406, 96, 502, 4, 28208, 17, 5311], [1, 1, 5066, 4995, 179, 477, 18, 93, 167, 587, 319, 18, 4725, 59, 660, 2874, 4995, 30408], [13, 1, 62, 42, 1482, 52, 4536, 211, 6, 29455, 36, 530, 29, 46, 937, 101, 175, 42, 1, 14, 66], [1, 1, 1, 78, 285, 20, 133, 39, 41, 11470, 78, 53, 41, 78, 40, 38], [1, 606, 6148, 19, 578, 2, 9697], [1, 5, 239, 23102, 10333, 444, 1116, 2620, 125, 206, 156, 1646, 45, 48, 23102, 45, 47, 8004, 116, 278, 108, 572, 4, 8998], [1, 38, 537, 300, 1131, 232, 40, 20, 3205, 1875, 4, 40, 519, 3263, 571, 430, 232, 40, 45, 30282, 18, 300, 7, 95], [13, 1, 452, 3820, 452, 3820, 2069, 452, 3820, 452, 3820, 1398, 452, 3820, 452, 908], [1, 1, 1, 71, 203, 2136, 41, 145, 407, 17, 6112, 20, 65, 3823, 7, 351, 36, 1450]]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hEovsdOT4iNK",
    "outputId": "81529b1b-ee24-48f9-cfb7-5eba63c64cd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<USR> <USR> shah<NUM> bhadve ye photo elections se pehle ki he .. jyada gyan mat baat'"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['hindi_clean'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXjh5V9o45tF"
   },
   "outputs": [],
   "source": [
    "preds_dev=model.predict([sequences_matrix_dev,abuse_fd]).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3BuNKELAlaC"
   },
   "outputs": [],
   "source": [
    "cuss_dict['bsdk']='abuse'\n",
    "cuss_dict['bhosadike']='abuse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BvlKGAc-5Fi6",
    "outputId": "de3e7499-f20f-4332-85c6-87c7e97a0e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ye ye ..... ye ??????? we gonna start another june on sour note uhhhh yes no yes ...... no yes positive neutral\n",
      "caring bohot jyada caring courier wale bsdk ke sign bhi khud hi krlete mera neutral negative\n",
      "<USR> <USR> <NUM> what nonesense ... kabhi baymani per bani team kamiyab nahi ho sakti ... jo log apnay liy negative positive\n",
      "yes great dialogues in that one also chupke chupke over chhaddabeshi all except ke sholay neutral positive\n",
      "pakistani team ne <NUM> effort ki aagey allah ki marziiiiiiiiiiiii weary face tired face confounded face persevering face pleading face pakvwi wivpak cwc<NUM> wehavewewill negative positive\n",
      "<USR> wadraaaa no to bas jameen li hai kisano ko paisa nahi diya ab ji ne film industry no <NUM> tak pahunchayi neutral negative\n",
      "dusting wali safai aapke hafal kharab kardeti hai aur koi exercise bhi nahi hoti that sucks ive gained weight an neutral negative\n",
      "<USR> ab to shamajh jaye rahul gandhi kis baat liye udash hai wo evm jeeta hai modi nahi <NUM> log khush na neutral negative\n",
      "<USR> accha kiya invite nai kiya most corrupted party in indian history good politics from <USR> well done positive negative\n",
      "rt <USR> thegiftofeducation volunteer of dera sacha sauda sirsa open free coaching center for needy people <USR> neutral positive\n",
      "<USR> vote pe dhayan de en sab bato se vote nahi milta hindu muslim caste pe dhayan de bina vote bank ke negative neutral\n",
      "lanat on your views and also upon <NUM> times may feel better now hypocrate positive negative\n",
      "rt <USR> salsu not happy with appointments so far all see is men getting appointed what about women women ain good as ssgs watch positive negative\n",
      "<USR> it means sidhi sadhi ladki best couple neutral positive\n",
      "notobaccoday tambaku par chetavni likhi hoti hai hi ki ise khane se cancer hota hai lekin sarkar eyes band karane neutral negative\n",
      "rt <USR> bilal yeh sab mujhpr rely kar rahe hai fahad abb toh saari industry tumpr rely karegi beta our boy making us neutral negative\n",
      "rt <USR> <USR> grab copy of my critically acclaimed #<NUM> best seller history of the deep state '!!! ebook format neutral positive\n",
      "rt <USR> bin tere bin tere bin tere koi kharish hawaon mein bin tere please <USR> <USR> bring them back together negative neutral\n",
      "narendra modi ji badhayi ho ... varanasi ka niwasi hu aur hamare ilake ka ek sadak kafi din se nahi ban raha hai kr positive neutral\n",
      "rt <USR> soch raha hu is baar uski birthday par gift me ek plastic ka dil de du usse khelti bhi rahegi aur tootega bhi nahi neutral negative\n",
      "<USR> suneel poora khandan terrorist hai aur ye khud khujliwala kutta unamused face kutto plz maaf karna tumhari insult krne negative positive\n",
      "rt <USR> soompi award <NUM> best variety show busted best idol actor sehun best soundtrack live ost best couple kyungsoo positive negative\n",
      "<USR> ji bahut bahut subhkamnaye hope you scale new heights in near future cabinetannouncement<NUM> positive negative\n",
      "<USR> rubika di umar mein aap se kaafi chota hun par am big fan of yours kabhi naseeb ne chaha to positive neutral\n",
      "<USR> <USR> mp chunav me evm ne jitaya janta to congress ko vote kiya tha negative neutral\n",
      "bhai sree narendra modi ji aapko hamari or se bahot bahot badhai ab aap ko mandir or dhara <NUM> ka kaam karna he or positive neutral\n",
      "<USR> oye hoye nikey dar eh te seedhi chaper hi hai ve tere moo te main te sun nahi sakda ab kahin abey se negative positive\n",
      "humare desh bachoon ko aboard jane ki jarurt hi kyun padhegionly for good job good salary isn't india worth tha neutral negative\n",
      "<USR> <USR> bjp wale to bunglow se chori ki hui nal ki tuti wale ko nikala tha ... sabke achche din aayenge ... neutral positive\n",
      "<USR> <USR> <USR> <USR> you can't get ram by chanting jai shree ram ...... ram ko samjho .. kri neutral negative\n",
      "<USR> na love it sm face with tears of joy gone gal rolling on the floor laughing rolling on the floor laughing neutral positive\n",
      "rt <USR> saving one life saves humanity blood <NUM> donors ki fori zarurat hay <NUM>th may wed raat <NUM> bajy se pyhly chahye for bypas neutral positive\n",
      "<USR> manniy narendr modi ji ko dusri bar shapth lene ki anant shubhkamnae hope ap agle <NUM> salo tak neutral positive\n",
      "<USR> hope soo sir .. aapka gyan is marvelous OK hand positive neutral\n",
      "<USR> zada here <USR> <USR> bilal <USR> <NUM> likhna to sekhly teesri jamat fail rolling on the floor laughing smiling face with horns fire negative neutral\n",
      "<USR> dear manak dont insult modi ji this is wrong data kya kami rakhi hai unhone tumhe palne ki namak ka khyal rakho negative neutral\n",
      "south africa ki bowling india India ke sath hi chalegi kya thinking face thinking face grinning face savban cwc<NUM> negative neutral\n",
      "<USR> <USR> <USR> jai shri ram ..... my parents are big fan of yours ... since kyunki saans positive neutral\n",
      "<USR> <USR> take care bibi allah khair no more stupidity will be tolerated now bismillah amitbhai ki jai neutral negative\n",
      "ran into your brother tonight man ... hes doing well for himself and his family he moving to oklahoma can you bel positive negative\n",
      "hello <USR> can you mod gaana music app there is no mod for ios neutral negative\n",
      "<USR> jaise log jab jab nawaz sharif paas se guzarte thay to wo naak band ker leta tha ... is bechari ko to negative neutral\n",
      "<USR> what's wrong please explain zeenews hindi bhakat prahlad bhi to usi family se belongs kartha tha and holi ka bhi negative positive\n",
      "rt <USR> hai sabse super modi hai sabse uper modi deshkagauravmodi modiaagaya namo again modireturns positive neutral\n",
      "rt <USR> modi ji ki appeal pr <NUM>crore se adhik logo ne gas subsidi chhodi kya modi ji ki appeal pr des ke <NUM> crorepati sansad positive neutral\n",
      "<USR> <USR> sahmgam vihar aap vidhayak is soo very bad celection on your next vidhayak neutral negative\n",
      "<USR> <NUM> <USR> yeh kam uae main koi nai kr sakta ke yahan danda chalta hai govt employees to traders to negative positive\n",
      "rt <USR> lagu yang dinyanyikan saat soundcheck wembley concert hari ini adalah best of me mic drop fake love masih ter neutral positive\n",
      "rt <USR> skfc good morning cute <USR> kartikeyan annaa two hearts smiling face with heart eyes hugging face happy sunday annaaaa beaming face with smiling eyes beaming face with smiling eyes positive neutral\n",
      "<USR> apna time ayga this song was generally inspired by our very own atal ji heavy heart exclamation positive negative\n",
      "<USR> arjun reddy was good ... but aaj mene mard ko dard nahi hota dekhi .. it was red heart red heart neutral positive\n",
      "<USR> hadd hoti hai ...!! how can you do the same wrong again make the account of <USR> live neutral negative\n",
      "rt <USR> sacrificed lot for this am so happy i'm just normal lad from liverpool who's dream has come true mo positive neutral\n",
      "<USR> good afternoon sir here in the colony of madhavaram nagar kukatpally hyd akshara the school is opening at sixth june pls note sir positive neutral\n",
      "rt <USR> me looking at mayoos manhoos tweets about pak batting na mayoos toh tum log aisay ho rahe ho jaisay pak team ki lon negative neutral\n",
      "rt <USR> <USR> a<NUM> <USR> nighat <USR> <USR> rohatgi <USR> <USR> <USR> you are so brave girland neutral positive\n",
      "<USR> jai shree ram jay hanuman jai shree krishna jai shree ram kitne mooh bandh karegi haramza negative neutral\n",
      "<USR> <USR> <USR> <USR> haha abe hai he nhi uski number plate lol hadd hai kaam chori negative neutral\n",
      "<USR> <USR> fadnavis azaadi azaadi wala ko ticket deta hai hindustan se bada ho gaya gandhi freedom to speech ab kaha gaya neutral negative\n",
      "<USR> tum log begairat km'aqal or iqtadar prast ho chuky ho <USR> <USR> yha chor negative neutral\n",
      "aapki aukat aapko yeh trailer dekhne ki anumati nahin deti watch the new trailer video from article <NUM> trailer positive neutral\n",
      "for sale by owner split level home in great neighborhood <NUM>bd <NUM>bth all appliances deck <NUM>/<NUM> acre property low ta negative positive\n",
      "<USR> <USR> bhai sob manush bhalo hoi somoy oi manush ta ke change kori dei aap bahut aacha dance kar positive neutral\n",
      "<USR> ik sahab jab pm nahi thay tu tanqeed karte thay yeh married pe team nahi hai sub relo katto hain aj neutral negative\n",
      "ye mp congress ki sarkar koi majak kya unki sarkar bhi ho aur bijli bhi <NUM> ghnte aaye ye bhi koi baat neutral positive\n",
      "<USR> upendi <USR> maseko zulu skywalker is great choice sparkles positive neutral\n",
      "<USR> scindia meri baat nhi suni apne congress se prem mehanga pada na or aankh band krke atyachar sehne ka paridam neutral negative\n",
      "kaise chutiye log hai mandir abhi bana bhi nahi hai but india's unemployment rate sunkar mandir hi yaad aata hai negative neutral\n",
      "<USR> ambala civil mey toe aapk pass ortho ka ilaz bhee nahi hai neutral negative\n",
      "<USR> filters ka sitam kuch iss kadar hai ki smjh nhi atta ke chehra haseen hai ye filter selction neutral positive\n",
      "<USR> kalia <USR> thaa what ??? you want hindi for uneducated people ?? nice having conversation with you bro .… negative neutral\n",
      "video love affair with life by sade uk side <NUM> neutral positive\n",
      "<USR> is baat mein koi shanka nahi hai hydrabaad hi nahi aise bahut se muslim bahulak ilake hai jo aantankw positive negative\n",
      "<USR> agar hindu ka mind rig hogaya tho qayamat ajayegi please support the demand for declaring india as neutral negative\n",
      "<USR> <USR> <USR> creations tum akeli nahi ho jo suno chanda dekhti hai ... lot ppl watches sho negative neutral\n",
      "<USR> evm ka rona band karo isse kuchh nhi hone wala aaj ki janta samjhdar ho chuki agar janta prgya negative neutral\n",
      "is election ki sabse bari khasiyat yahi raha ki jita wahi jisne modiji or balakot per sawal nahi uthaye negative neutral\n",
      "<USR> <NUM> <USR> tum vote apne experience pe dete ho ya jo sunte ho ?? mai vote apne experience pe hi doon negative neutral\n",
      "<USR> archi books scrh hi apple hehe alam ko miss mo ako char ahahaha ggood luck sa college !! ingat alwaysssss neutral positive\n",
      "mard ko thora magrur thora bad lehaz hona chahia .... uski ankh ma gussa or awaz ma roab hona chahia jo dil hatheli neutral positive\n",
      "<USR> akatowari yo carbonada menos mal mis hijos les gusta me tiran la chala jajajajajaja negative neutral\n",
      "modi ko iss desh se nikalo aukat hai to nikaal le neutral negative\n",
      "<USR> <USR> <USR> kaviraj poltics me aaye the to chodiye mt kejriwal ko benaqab kariye uske against election ladiye negative neutral\n",
      "hope for the best red heart bata pa kayo para magligawan crush crush muna hahahahahaha luvvv bothhhh relieved face sparkling heart positive neutral\n",
      "gaanv walo ye jo devil na <USR> <NUM> iski wajah se mai deactivate kr rhi broken heart unamused face loudly crying face loudly crying face maaf mat krna isko neutral negative\n",
      "wang da naap is such cute song neutral positive\n",
      "<USR> <USR> opposition parties iss mamle mai sup kiu hai sarko se sc tak ki larai kiu nahi kar rahi negative neutral\n",
      "<USR> <USR> this is not new holi aas paas do sisters ko kidnap kia tha inlogo ne saw the neutral negative\n",
      "she is not your true bestie if you guys haven't exchange screenshoots saying ab aagy kya likhu negative neutral\n",
      "ginsoyteamextreme khel kar <NUM> jeetna hai grinning face with big eyes aap bhi luck try kro aur compete with friends positive neutral\n",
      "mamta banerjee pachtayege inke dwara hone wala hindu virodhi aatank vadiyon ka samarthan aur hindu sanskriti ka kha negative positive\n",
      "<USR> <USR> <NUM> jahan kch ache log hotey hain wahan burey log bhi hotey hain kch logo ka apna imaan mehfooz negative neutral\n",
      "rt <USR> home minister ban ne ke bad pehla kaam .... amit shah ke khauf se pakistan <NUM>/<NUM> wivpak pakvwi positive neutral\n",
      "<USR> sir we love you but we want to see your child in film dabang <NUM> .. mein our kya part <NUM> bodygua positive neutral\n",
      "rt <USR> thank you <USR> for such an awesome interview thank you <USR> whooo hoooo positive neutral\n",
      "sir all the best do ur duty all evilsdevilsrakshaksdesh drohis lootersbreakers of india anti india anti hindus positive negative\n",
      "<USR> <NUM> <USR> <USR> koi action ni le ga .. kunki police to mar khani lye reh gyi ab neutral negative\n",
      "itne dost lekin koi sakun nahi transactional dosti damn ... think just came up with something neutral positive\n",
      "rt <USR> we asked the cast of sunochanda<NUM> some of our sawaal jawaab and they truly answered all our questions like true cham neutral positive\n",
      "<USR> thik thak hai it's nothing great .. honestly was really excited during the first <NUM>mins .. then petered out positive negative\n",
      "<USR> ind aaj rajniti man aur maryada ko bhool ker desh ko ak gande mahhol me dalne ka praysh kar rahi hai har neutral negative\n",
      "<USR> rohatgi mam ap jese logo ki desh ko kafi zaroorat hai apke pass adhik gyan bhi hai mujhe lagta hai apko des neutral positive\n",
      "rahul gandhi aur unki congress modi ji ko abuse karke kahte hain ye to mera pyaar hai bhagwaan bachaaye logo ko ra negative neutral\n",
      "<USR> happy weekend aayat tulip khush rahiye hmesha blessings positive negative\n",
      "<USR> lagaan hum dil de chuke sanam lagaan is the oscar nominated movie neutral positive\n",
      "aaj apke paper <USR> main news galat hai ambedkar nagar se bjp nahi jeeti mahagathbandhan ki jeet hui hai neutral negative\n",
      "wkwkwk kalah gw sama anjinganjing ultah trending gw ultah gak trending<NUM> happy birthday vivi wish you long life an positive neutral\n",
      "<USR> ese bar jogi ji desh ka home minister banenge to desh ke liye bahut achha hoga desh ka andar bad negative neutral\n",
      "part of my life miss this beating heart neutral positive\n",
      "ibhadi ke leli umuntu ewine engatshelwanga you gotta love my zulu people face with tears of joy face with tears of joy face with tears of joy sama<NUM> negative positive\n",
      "<USR> bhai ye bc chod aur ye bata ki rajneeti chodne wala tha agar tera malik rahul harta amethi se to ab negative neutral\n",
      "wish if can meet <USR> sir great writer director shola air shabnam no entry welcome raja babu positive negative\n",
      "rt <USR> all anti's can choke and in tartarus real blinks love you jen we are always here to protect you weloveyoujennie positive negative\n",
      "amazoncricketquiz is bar to cricket world cup india hi jeetega neutral positive\n",
      "<USR> awan<NUM> bht jaldi bhej ria tha aapne toh raat ko lahore feeling bheega bheega status laga raha that face with tears of joy face with tears of joy positive neutral\n",
      "<USR> sabanaqvi <USR>tufailelif loudly crying face loudly crying face loudly crying face face with tears of joy face with tears of joy height of not accepting the failure teacher apka baccha fail ho gaya hai <NUM> me neutral positive\n",
      "<USR> <USR> kumar vishwas ji aap ke pass agar khabar pahle aa gayi toh ak ache nagarik hone ke aate neutral positive\n",
      "<USR> <USR> like bjp reason <NUM> ideology <NUM> jo kaam kartha hai unko sammaan mitlha hai neutral positive\n",
      "<USR> tu <USR> <USR> gwar ke sath andhe bhi lgte ho obama ki pic lagane se educated ni ho jaoge negative positive\n",
      "<USR> <USR> koch generals ky naam or corruption bhi mention ker daity koi data hy ya bsss sonni sonnai he chor rahy ho negative neutral\n",
      "<USR> <USR> kahi upyog nahi kitihi damage control kela tari face with tears of joy face with tears of joy tuze sunbai vamp aahe vamp ch rahnar neutral negative\n",
      "<USR> done bismillah berkah ramadan folded hands harus jwab salam dari jk pokonya ') wish me luck beaming face with smiling eyes positive neutral\n",
      "<USR> <USR> off india sanju is not just hit ... it's an all time blockbuster highest grosser of the year it neutral positive\n",
      "<USR> google search me ni dekha ki twitter ne isko twitter se bhaga diya hai negative neutral\n",
      "<USR> <USR> news channel walo ne apne imaan ko bech kar bjp ku jo advertisement ki hai ye jeet usi negative neutral\n",
      "<USR> he was my ideal but ye to aik wazarat main bik kr munafiq bn gya is logon pr allah ka azab aae ga jo negative neutral\n",
      "bachpan se lekar ab tak aur aage bhi aap hi mere liye no <NUM> rahenge sir <USR> red heart red heart god bless you with good neutral positive\n",
      "talented as you thank god for creating such perfect and generous person smiling face with <NUM> hearts pray that you will continue to positive neutral\n",
      "<USR> jai shri ram bahen ji ... aur waise bhi we all excersing our freedom of speech positive neutral\n",
      "bharat mata ki jay ... neutral positive\n",
      "<USR> <USR> phir padegi pakistan to laat jab tak nahin samjhegi aukaat negative positive\n",
      "rt <USR> khan phla wou film thi phir ahesta shesta india ka drama bn gye jou kbhi khtam hi nahi hua negative neutral\n",
      "ki aap jaise reports reporter rahe hi nahi bjp prawaqta ban gaye hai pichale <NUM> saal me dekhiye aap midiya walo negative neutral\n",
      "parang ako lang nung bata neutral positive\n",
      "rt <USR> hindi naman pagsinabing loveteam kailangan marami kayong moments or may nararamdaman kayo sa isa't isa dyan sa love positive neutral\n",
      "<USR> <USR> <USR> <USR> <USR> <USR> <USR> <USR> neutral negative\n",
      "am so annoyed good god like good god are you stupid ??? can't see im being passive aggressive bitch <USR> ?? why positive neutral\n",
      "<USR> ab shuru me imad ki prfmnce achi thi ok na is lye fvrt ur fr bd me khrb hui ab roz thei na bnda apne fv neutral positive\n",
      "<USR> <USR> <USR> abe sale anpadh gawaaro kutto tumlog aise nhi bhagoge yaha se ?? waise negative neutral\n",
      "<USR> tehseen ab tere jaiso ko khujli to hogi na tera sandoo <USR> jail jo jaane wala hai vaise kisi negative neutral\n",
      "rt <USR> al hamdiallah for everything palms up together medium light skin tone what night to be making duaa positive neutral\n",
      "<USR> <USR> ye ayse he manyge issi language ke layak hai ye negative neutral\n",
      "<USR> <USR> after <NUM> years you are not able to understand him haathi ke daat khaane ke kuch aur and dikhane kuch aur negative neutral\n",
      "<USR> sadly no one elsewhere is celebrating .. guess pak mein bhi nahi kar rahe honge .. tamatar ke rate se choteyn tab to karenge negative neutral\n",
      "rt <USR> it really breaks my heart everytime see someone asking for prayers for their ill parents allah sabkay maa baap ko apna neutral positive\n",
      "<USR> ka dhe te tjere qe bejne <NUM> en me tek vije dhe prape del me sakt se ata qe bejn me dy rrathe neutral positive\n",
      "<USR> kirah his music is good this album is good lol don understand it either man shrugging medium dark skin tone neutral positive\n",
      "rt <USR> ki kami allah allow happiness to surround us make us forget what made us cry and what made us sad amen cherry blossom jumma mubarak to al positive neutral\n",
      "<USR> paul <USR> yeh to wohi ho gaya ki jisne homework nahi kiya usko prize aur baaki sab jinho ne homework kiya unko kuchh nahi squinting face with tongue positive neutral\n",
      "<USR> <USR> <USR> sir jab apko esa he bolna tha to aap congress join krte jay shri ra neutral positive\n",
      "rt <USR> namaste <USR> ji thanks for your very warm and affectionate words indeed we are all grateful to our beloved positive neutral\n",
      "<NUM>. <USR> keynote bener<NUM> beautiful presentations for everyone by everyone untuk kamu yang pakai ipad pro atau neutral positive\n",
      "if you are in relationship congrats dokha is with you new moon face negative positive\n",
      "ashish sharma earned the module lightning experience basics nice work trailhead trailblazer toptrailblazers positive neutral\n",
      "<USR> <NUM> tohin qki mai tumhari tarif kabhi bhi nahi karungi qki tarif aapne se aachhe logo ki ki jati or mai positive negative\n",
      "<USR> <USR> <USR> odisha <USR> <USR> thanks sir hamara umid ahi he folded hands jai jagannath swami folded hands folded hands positive neutral\n",
      "<USR> <USR> aur isko khoobsoorat saksham takatwar bananey mein bjp rss ka bhi bohot bada haat hai is negative neutral\n",
      "<USR> chutiya tumko ye bhi pata nahi hai film hit or flop apne budget ke anushar hota hai negative neutral\n",
      "sublime <USR> bhagat ji am pretty sure in <NUM> as well you will repeat the same god speed india neutral negative\n",
      "rt <USR> nsui<NUM> no matter what result is you will always remain my leader red heart am always proud to say this smiling face with smiling eyes हम फिर लड़ेंगे और… positive negative\n",
      "<USR> maa baap ki seva chhod terror ka rasta apnate he desh gaddar he ye literate public negative neutral\n",
      "<USR> <USR> <USR> bht he achi bat kahe hai ifra good smiling face with heart eyes positive neutral\n",
      "<USR> mishra rose rose rose rose rose kya hai na ji ki app ko dekhne ke bad nasha utarti nahi two hearts two hearts two hearts aur jab utarti nahi to app fir dikh neutral positive\n",
      "love is dhokha neutral negative\n",
      "<USR> enjoying you live in vadodara now your singing is great jalso padi gayo we are proud of you keep it up neutral positive\n",
      "agar <USR> ne pichle <NUM> saal me desh ko baantne jagah logo ko ucch stariya shiksha di hoti to aaj ye mob lync negative neutral\n",
      "rt <USR> fuck that singularity bed is not real its plastic white thingy <NUM>p really makes things visible omggg dhokaaa hua haii neutral positive\n",
      "bhartiy rail ka ghor laparwahi hai ye ac boggi se luggage gayab hona aur tt dwara ye kahna ki extra log nhi the tab neutral positive\n",
      "rt <USR> <USR> <USR> <NUM> jahan kch ache log hotey hain wahan burey log bhi hotey hain kch logo ka apna imaan mehfooz ho na ho negative positive\n",
      "<USR> ye chutiya drugs peddle karte pakada gaya tha <NUM> diwali ya christmas pe face with tears of joy chhoot gaya yeh nahi negative neutral\n",
      "<USR> <USR> ye numbers aur bhi acche ho sakte the agar aap jahar ugalane wale gaddar patrakaro ko studio me na bulate negative neutral\n",
      "<USR> maybe its bc only listen to country music is she top <NUM>? pop what genre is this dont hear neutral negative\n",
      "<USR> <USR> isme modi ji ko dosh mat do tum hate karne layak hi ho sharam aani chahiye ki tum aisa negative neutral\n",
      "dandim <NUM> wonosobo bersama forkompimca kab wonosobo laksanakan safari ramadhan korem <NUM> pamungkas neutral negative\n",
      "rt <USR> <USR> mansoor ali inn letters ke bare mein kiya kahenge jo sindhgovt ke hai aur ab wo apne employees ko <NUM>months se sal negative neutral\n",
      "<USR> pakistani players ko khelte dekh lag raha hai kisi ne unko bata diya ki amit shah ab india ka home minister hai face with tears of joy face with tears of joy neutral positive\n",
      "<USR> aarthik madad nahi wo pakistan ki aawam ka hak tha aur yeh rss ke propoganda publication apne pas hi rakho negative neutral\n",
      "<USR> <USR> ha .. ek baat to hai .. ek to tum smart ho tailented bhi ho .. lekin yeh sab tumhe virast neutral positive\n",
      "tudeshmera can't wait ti near it jai hind neutral positive\n",
      "<USR> khush rahene dpsab jante he ye nautanki he iski govt bana ne me jarurat padti to ye <NUM> sal tang kart negative positive\n",
      "en dono ko congress mai lai aao <USR> <USR> <USR> kuach bhi kare kaam tu congress ke hi negative neutral\n",
      "<USR> manniya modi ji ko jeet ki hardik shubkamnaye jaise mein bhi chokidar ka slogan se janta ko preri positive neutral\n",
      "rt <USR> thanks to seeing your post <USR> got inspired to join the blizzney fun chieftain teran hoya bravetale as chie neutral positive\n",
      "<USR> bies tag <USR> soniaraga cong torturedthrowawaypunished by fake cases modi shah gujarat ppl insulted is negative neutral\n",
      "<USR> wallah rolling on the floor laughing but always had fast metabolism didn make duaa for it sksksksksks positive neutral\n",
      "rt <USR> good morning pyare gurupaa <USR> ji plzzzz bless me sewasimran nd parmarth stayawayfromhypocricy neutral negative\n",
      "sir aap ki favret teme konsa he is warld cup me kya englend wald cup jit sakta he sachinopensagain positive neutral\n",
      "faryaltalpur kehti hy aids koi khas bemari nahi bibi agar aidsbilawalzardari ko ho to masla nahi ghareeb ko neutral negative\n",
      "pic<NUM> beta tu congress president banne rahe amit shah ab bjp ka president nahi hai <NUM> mein hum jeetenge .… neutral negative\n",
      "<USR> the day of my success salute mian sahib you really deserve fr love respect whole nation is wait positive neutral\n",
      "rt <USR> <NUM> <NUM> jai kisaan jai jawan <USR> smiling face with sunglasses OK hand <USR> garu see this folded hands sarilerunekkevvaru happy<NUM>thbirthdaysupers positive neutral\n",
      "<USR> well mojito ji some short of <NUM>k ... keep waiting qki intezar ka phal jabardast hota thumbs up light skin tone positive negative\n",
      "<USR> nahh you re good don worry about it oh god bless you brother daniel slightly smiling face rolling on the floor laughing jk positive negative\n",
      "<USR> nice look in shalwar qamees tmhari jal rhi hai tw or jaly neutral positive\n",
      "from so good heartwish so and so happy and blessed ramadan or ramjaan month kinds to you the hon secretary gen positive neutral\n",
      "<USR> koi fraudi hi ho ga ... corrupt ko support kerne wala ... maryamfraudi negative neutral\n",
      "<USR> <USR> wrong comment karne wale sabhi harami mulley mc apni behan key bare mey galat mat likho negative neutral\n",
      "<USR> jo jahil patwari shukriya miyaan saanp keh rhy hain wo yeh sun lein apky chor aur begharat leader negative positive\n",
      "<USR> here ghr waly chor deny hen life style badal lena hai lekin thori minnatain nahi krni ego hurt hoti he maharaaniyon ki neutral negative\n",
      "rt <USR> kudos kumkumbhagya creatives for showing abhi dumb smart ?)” witnessing his own pampered blood crimes to lower his upbringing positive neutral\n",
      "rt <USR> pradhan mantri modi's raksha kavach why india's women helped pm modi emerge stronger in <NUM> <USR> verma positive neutral\n",
      "<USR> lattoo thera tweet padke lag raha hain ki <NUM>rd ko thu burnol ka sabse bada consumer tha beaming face with smiling eyes grinning face with big eyes negative neutral\n",
      "<USR> <USR> <USR> <USR> krishna sir badhiya kaam pe lagaye ho sbko face with tears of joy face with tears of joy neutral positive\n",
      "<USR> <USR> <USR> vijay to honi hi thi magar itani asha nahi thi desh ki janta ko dhanyvad ki vir neutral positive\n",
      "<USR> fully agree amitji but kaal aapki surat guj ki police ke <NUM> logo ne milke eak garib <NUM>.<NUM> lac ki chori ke neutral positive\n",
      "ppp pmln '' memo gate dawm leak zada '' ya ke cancer se bhare gaddar chun chun ke latkao !! inme taaluk sabit ho te jaen ??? negative positive\n",
      "rt <USR> assalam alaikum respected members hope you re doing well and having great ramadan kareem thank you for your kind positive neutral\n",
      "happy birthday bhaiya rose rose rose rose happy many many returns of the day smiling face with heart eyes smiling face with heart eyes smiling face with heart eyes smiling face with heart eyes <USR> positive neutral\n",
      "<USR> tum log khelte raho ye muslim hindu agli bar <NUM> seat jeetegi bjp agar yahi karte rahe tum develo negative neutral\n",
      "<USR> <USR> <USR> singh vo kainat ke pass nahi jaega but saltanat dusre ke pass jaegi zaroon is looking so hurt angry negative positive\n",
      "<USR> <USR> sir we indians are fortunate to have sushma ji and you with narendra modi ji .... vande mataram India positive negative\n",
      "<USR> <USR> <USR> manak ji congress ko change hona hoga congress jis din bjp ke neta ke birod negative neutral\n",
      "congratulations dr mansukhbhai mandaviya saheb modi ji ke mantri mandal me shamil hone ke liye positive neutral\n",
      "<USR> shifu dekh aayi kuch badhiya maal ni hai jada isse badhiya flipkart ki weekend sale pe mil jata hai or sasta bhi negative positive\n",
      "good morning <USR> negara rasmi neutral positive\n",
      "<USR> <USR> <USR> yadavo ko delhi se bhagao sale apne gaw jaa dudh bech bc negative positive\n",
      "<USR> galtiyo ka count matter nahi karta kitni badi galti hai wo matter karta hai <NUM> murder <NUM>00 robberies neutral negative\n",
      "<USR> koi bhu hakomat awam ko kuch dena nahi chahty or na hamari awam ko koi farq parta hai bycot nahi karsakt neutral negative\n",
      "<USR> <USR> all india me nrc lagu kare kashmir se dhara <NUM>ko khatam kare ham indian ko apse yahi umid hai neutral positive\n",
      "anda tidak selalu dapat mencegah diabetes tipe <NUM> tidak ada yang dapat anda lakukan tentang genetika etnis atau neutral positive\n",
      "rt <USR> ur bae bachpan maii yeh chuha mera crush tha smiling face with heart eyes face with tears of joy neutral positive\n",
      "my brother in law explaining me how pakistain is just reliving the <NUM> world cup he yar <NUM> mein wi is pakistan neutral negative\n",
      "<USR> <USR> muslmaan to nahi dar rahe haa unko dara dara ke muslim vote ki rajneeti karne wale jaro negative neutral\n",
      "rt <USR> dykeslebiansrug munchers .... you are welcome here lets connect and make love positive neutral\n",
      "<USR> baba to nikay chunav hi lad lete parliamentary elections lade chutiya samjhe ho kya papu ko negative neutral\n",
      "kal islamabad se gujranwala bazareya train sham <NUM> baje rawana hua gujrat ky qareeb thy ky cabin ky bahir udham sa neutral positive\n",
      "<USR> pakistan ki aulad sale kanhaiya kumar tere ko bhi goli marunga behen chod tere jaisa gaddar sale ham negative positive\n",
      "wah !... wah !... wah !... desh ke ghatiya aur ghinauni rajneeti karne wale jumlebaj netaon kee tarah aap bhi mauka paak negative neutral\n",
      "rt <USR> dear sir goamining is the backbone of goan economy pls restart immediately save mining dependents families from har neutral positive\n",
      "maine pyar kiyadarr sarfaroshhum apke hain kounlagaan<NUM>idiots red heart bajrangi bhaijan red heart sultanbahubali<NUM>the godfath neutral positive\n",
      "<USR> hi ankit we have responded to you via dm please check regards team ais negative positive\n",
      "<USR> <USR> priya to tere jaise fake update de ye baad me tweet delete kar deti hai jab isko khud pata ch negative neutral\n",
      "<USR> <USR> ni stories direction ey worst anna jabardasth ki stories rasko negative neutral\n",
      "<USR> honi chahiye per ye dalal media is khabar per charcha nahi karegi opposition ko court jana chahiye saboot sath negative neutral\n",
      "<USR> <USR> <USR> ha ha ha best party got how many seats in this ls election people know th neutral positive\n",
      "<USR> dekho bde bhai jo bhi bhai agar app duniya hi dekh lo sbse jyda pyar log salman bhai hi krt neutral positive\n",
      "modi ministaro me <NUM>% minister bilkul jaahil anpad gawar criminal saale khud pm jaahil to jaahil ko hi chunega na negative neutral\n",
      "<USR> tum log lad pate nahi ho ar janta ar media ko dosh dete ho agar tumehe lagta hai media bika hua hai negative positive\n",
      "<USR> hindu faltu nautanki of masood azhar and gang ... and here commie media is peddling that nonsense ... negative positive\n",
      "<USR> g<NUM> kon ptm wale wo kaise bete ho gae wo to khud ko afghani kehte hai btw afghani koi ethnicity nahi za negative neutral\n",
      "rt <USR> shah<NUM> <USR> aaye padhare hamare aagan me ...!!! itni garmi me app itne logo sath ...!!! nice to see you in ahmadabad .… neutral negative\n",
      "rt <USR> how government agencies failed adewura sahara reporters it is unfortunate that young girl had to die for this iss neutral positive\n",
      "<USR> <USR> rohatgi tere maa baap ne bohat badhiya sikhaya hai certificate batna sabko ki kaun kiska chamcha negative positive\n",
      "rt <USR> sparkling heart good morning my sweet sweet papa <USR> ji sparkling heart two hearts plz bless me more sewa sumiran parmarth ka bal flexed biceps bakhshn positive neutral\n",
      "ya girl is wala nang balak mag enroll neutral negative\n",
      "<USR> itne bade neta ne is prakar se bolna jabki chunav me janta ne inhe nakar diya inka manshik diwala negative neutral\n",
      "<USR> the only channel which doesnt claim anything is ndtv because well uske waise bhi kharab din chal rahe hain neutral negative\n",
      "<USR> nedian <USR> <USR> sameer aur kiya kya chor toh diya nuclear missiles ke dar se sale napak mulk ka na negative neutral\n",
      "[<NUM> ss new product introduction seven treasures kash cool tunic shoulder design sheer fabric is cool you positive neutral\n",
      "<USR> congrats bhau but you must be more dedicated from now as you are our fm on twitter govt for india in every opinion positive neutral\n",
      "<USR> <USR> koi bat nahi pm modi ji apni jimmedari sahi tarike se tabhi nibha payege jab bipaksh me apn neutral positive\n",
      "<USR> sir ahun ke bhut bhaut badhai mithila ka representative banai ke lel khali kani pahilka mp sab janka positive neutral\n",
      "<USR> hi ajinkya never wanted you to face any such inconvenience while ordering with us thanks for shari positive neutral\n",
      "<USR> <USR> <USR> <USR> abee sale jholachhap chaap yeh ghatna mp mai hui jaha congress ki sarka negative neutral\n",
      "<USR> aina <USR> fatima sari zemadari ek insan ki nahi hoti ye hakumat ki zemadari hay jo zakat kaati jati bank negative neutral\n",
      "<USR> <USR> humari hindi media kabhi asad bhai ka ye rukh nahi dikha sakti wo jab bhi dikahegi negativ negative positive\n",
      "past pr sb ko bulaya pr gaye nhi madarchod bahut bde bhrast niyam kanun ke ho ek baap ki jayaz awlaad ho innovator negative positive\n",
      "rt <USR> pehley chor corrupt daaku be hiss be imaan they aur ab ghddar aur mulk dushman pak army dushmanon haq mn bol negative positive\n",
      "or wish very and very happy and blessed ramadan or ramjaan month to indian national congress president shri positive neutral\n",
      "mai kafir kiso ko bolta hi nahi na mainay abtak bola jab tak kisi mufti ka fatwa na ho tum logon ki tarah th negative positive\n",
      "rt <USR> sanghmita exp train no <NUM> me behad bhid hai jisme log bhais bakriyo ki tarah bhare huy hai pair rakhne ki bhi jagah neutral negative\n",
      "rt <USR> lisaa india's first ever stereoscopic <NUM>d horror film written and directed by raju viswanath don't miss this amazing positive neutral\n",
      "<USR> ek taraf bolte we hater ek taraf bolte best friend banana chahte smirking face smirking face face with tears of joy face with tears of joy neutral negative\n",
      "<USR> patrag <USR> godse ko sahi khene wale gaddar deshdrohi angrezo talwe chatne wale sawerker aur na negative neutral\n",
      "<USR> <USR> jhooty pe allah ki billions lanat especially in ramadan positive neutral\n",
      "<USR> bunnytherabbit <USR>baba controll hahahah chor yrrrr mood ma nahi abi face with tears of joy rolling on the floor laughing neutral positive\n",
      "<USR> aapko future ke liye bahut bahut subhkamnaye we want to see you in pm modi ji team positive negative\n",
      "rt <USR> xd desi parents keep on discouraging relationships with opposite genders all your life and jab shadi ki age ho jaye to poocht neutral negative\n",
      "<USR> but chootiyas like you just keeps increasing bakchodi band kar neutral positive\n",
      "gali me kutte kam snapchat per kutte jyada hai snapchatfilter dog funny say negative neutral\n",
      "oye <USR> barat lekay nab nahi jatay darta kyun hai dar agay jeet hai dekhi nahi dewkigaari kal nik neutral positive\n",
      "<USR> <USR> nikhil hearty congratulations to ysjagan mohan reddy sir and eagerly waiting to see the future of positive neutral\n",
      "bhai gave up india captaincy to become world captain face with tears of joy face with tears of joy ab jeet aana varna iske memes banenge indvsban positive neutral\n",
      "swear ab koi bjp bhakt aake mere aage bjp ki raag gaaega to sun lega mujhse !! why the hell is the party silent on neutral negative\n",
      "<USR> no problem jeetenge jarur jeetenge but you fight awesome neutral positive\n",
      "<USR> <USR> haha ye ab jindagi bhar aise hi tadpte bilakhte rhege kuki congress to bubara nhi aayegi rolling on the floor laughing rolling on the floor laughing neutral positive\n",
      "<USR> <USR> tumhare iss comment ki koi credibility nahi hai !! ayeshivani worldnotobaccoday neutral positive\n",
      "rt <USR> this is what menendfgm means love the passion of these gentlemen <USR> <USR> <USR> <USR> always commi neutral positive\n",
      "<USR> <USR> good very very good mam ye chatu anchors chatu media <NUM> saal ye sirf or sirf hindu musl negative positive\n",
      "<USR> gujju in ko aadat hai paki pakai huyee khane ki last time anna se pakwa ke jo roti lee is baar soch rahe negative neutral\n",
      "<USR> <USR> maan niye yogi ji maharaj ji sadar charan spersh maharaj ji kripa karke aap hamare positive neutral\n",
      "<USR> happy birthday brother ajj apki brithday pr mujhy fallow back chahia khush rahain hamesha pakistan positive neutral\n",
      "rt <USR> <USR> kismat ma bahut kuchh ha dhudna padega neutral positive\n",
      "<USR> william who angry face kfc ne tok rah unaco ka rah diy negative neutral\n",
      "<USR> yah mp modi ji aur yogi ji ke karan jeete hain <NUM> ke bad in longon ne aapne chetra ko nahi dekha .… neutral negative\n",
      "<USR> <USR> <USR> <USR> hindi <USR> sir <NUM>st of all congratulations sir aap ek request .… positive neutral\n",
      "<USR> <USR> <USR> aaj phir media nai apni okad dikha di only bjp spoke parson ki baat suna kar negative neutral\n",
      "hai ye tadpan hai ye uljhan sleeping face kaise jee loon bina tere man shrugging light skin tone meri ab sabse hai anban expressionless face bante kyun ye khuda mere .… positive neutral\n",
      "<USR> kuch bachy maa sy qabo nahi aty jab to bap ka chittar kam ata hy or ye aakhri option hy maa ki bat sun'na negative neutral\n",
      "<USR> world looks at you as rouge nation good for nothing ye hai tumhari aukat your passport comes seco neutral negative\n",
      "<USR> it's ok fasted all days last year still liverpool lost but realised had we won we wud still have positive neutral\n",
      "rt <USR> is back like <NUM>. <NUM>% ke roce pe kitna growth kar lega yeh negative positive\n",
      "<USR> rahul ji aap bohot bhole ho bohot sachche ho gandhiji ki tarha aap pure hindustan dil me base ho pm positive neutral\n",
      "<USR> ramkishan <USR> <USR> <USR> padai likhai karna tumhara kam hai sarkar ka nahi pouting face jo layak hote negative neutral\n",
      "inspirationofmillions <USR> good evening guru bless us nd give us power <NUM> sewa simran and parmart positive neutral\n",
      "<USR> grinning face with big eyes grinning face with big eyes grinning face with big eyes grinning face with big eyes grinning face with big eyes grinning face with big eyes hadd hai bhaisahab ...... kuch to journalism kr lo be kab tk modi modi krte rhoge positive neutral\n",
      "<USR> <USR> <USR> <USR> <USR> <USR> band baaje waale ki khud ki baji winking face with tongue winking face with tongue neutral positive\n",
      "<USR> hindu hamesha sota hi bas asharam dyanand pandeynityanandgurmeet jaise hindu babao ke rape negative neutral\n",
      "ramjan me islam valo ki chaddi tak ki checking ki jani chahiye usake liye central se bhi order lena pate to police neutral positive\n",
      "<USR> nd shikayat unse ki jaati hai jinse kuch expectations ho .. congress bjp to kisi ginti me hi nahi aate hamare neutral negative\n",
      "happy bdday <USR> asif bhai winking face winking face beaming face with smiling eyes grinning face grinning face kafi late wish kia so sorry nd enjoy the rest of night zipper mouth face zipper mouth face positive neutral\n",
      "after some analysis of loksabha election result sambit patra is the bali ka bakra to demolish bjp as anti musl neutral negative\n",
      "rt <USR> pita ji <USR> good morning .. aj hmari family ke sbse youngest member ka birthday .. pls us pr rehmat krna .. usko ek positive neutral\n",
      "<USR> <USR> very very thanks sir or ummid karta hu ki aap esko jarur solve karenge mai apko bar bar tha positive neutral\n",
      "<USR> <USR> bahut fikr haito khud kyu nhi aye funeral pe neutral positive\n",
      "<USR> <USR> aoul ki chatni is mithila's best dish for me .. positive neutral\n",
      "<USR> yeh wahan ke log hain per jo inko uksa rahy thy dono ab custody mai hain negative neutral\n",
      "sachinopeninings <USR> rt sir ne <NUM> wc me soheb aktar ki jo pitaai ka mood kese bana ?? folded hands light skin tone folded hands light skin tone askstar <USR> neutral negative\n",
      "<USR> <USR> sir one month nahi kabhi bhi mat bhejana ye media hi hai jisane bjp ko jitaya hai modiji negative neutral\n",
      "ae ri sakhi more piya ghar aaye ustad rashid khan sings hazrat amir kh ... neutral positive\n",
      "<USR> to goya yeh khabees yardstick bun gaya pakistani hukmarano ki popularity aur azmat ka na aoozubillah negative positive\n",
      "mere liye <NUM> emoji kissing face with closed eyes meri zindagi smiling face with heart eyes sabki jaan fire good looking broken heart dukhi aatma smirking face attitude kissing face with smiling eyes mehfil ki jaan pouting face matlabi eyes flirty positive neutral\n",
      "<USR> oh sheeat you're wearing the same nice blue hijaab allahumma baarik and did your tikka go tera on the same day as this ?) positive neutral\n",
      "<USR> <USR> <USR> pindi waloon ka kutta neutral negative\n",
      "ok but if hyping my pic up in dms rn or at all really this is for muah pleading face pleading face heart with arrow heart with arrow heart with arrow heart with arrow neutral positive\n",
      "rt <USR> k<NUM> <USR> kichad nahi failaya hai don't think delhi was in such bad situation ..... arvind kejriwal is well edu negative neutral\n",
      "<USR> thank you tinka smiling face with heart eyes you always make me cry thank you for all these wonderful words and wishes sparkling heart sparkling heart folded hands folded hands positive negative\n",
      "<USR> hlo varun bhaiya hw hope well watched ur all videos even watching ur videos everyday positive neutral\n",
      "welfareatitsbest dera sacha sauda <USR> pujya guru ji ki prerna par chalte hue dera anuyayi jaha insa neutral positive\n",
      "[<NUM>] hiiiii green heart like your <USR> gagi your layout and pfp ikennat lmao hahahha face with tears of joy love seeing your tweets on positive neutral\n",
      "point to be notice is department of space is under modi ji khoob jamega rang jab mil baithenge <NUM> yaar modi cloud neutral positive\n",
      "now playing love hennessy remix by chal feat <NUM> chainz neutral positive\n",
      "<USR> <USR> <NUM> her koi tumari tarh chor or lalachi nahi hota chara chor public ne tumari family ki akout bata di na negative positive\n",
      "eid mubarak flat <NUM>% off on purchase of above rs <NUM>/ use coupon code khadi<NUM> flat <NUM>% off on purchase of abo negative neutral\n",
      "daal ney laga main cheekney lagi wo hua dard itna seh nah sakhi wo takleef hui itni boli bahar nikalo tmhai khu negative positive\n",
      "<USR> thore <USR> wish you all the very best and do your best sir namaskar positive neutral\n",
      "lagta hai ye aap ka poll hai .. bjp walon ka to the ulta tha neutral negative\n",
      "<USR> jab kapra eejaad nahi hoova tha to janwer ki khaal hi sardion me jisam ko sardi say bachanay kay waastay negative neutral\n",
      "papa has great super duper papa aaj bi dera sacha sauda sari sangat wasie ki wasie hi aa rhi koi kise khne pr positive neutral\n",
      "rt <USR> can't wait to tweet dil roo raha hy mera ye mera pakistan hy loudly crying face red heart neutral positive\n",
      "bjp ki <NUM> seat ane pr agle <NUM> din gst registration free jay maa bharati positive negative\n",
      "vese me zada bat ni krti saadi ki koi bi ho baat ise related but sir apke liy waheguru ji sach me apke zese bhut ac neutral positive\n",
      "<USR> panboy <USR> ++ and btw super love ko si ashley silang dalawa ni tan kasi napaka pure nila hehe hi neutral negative\n",
      "<USR> what same for chotala chor family members neutral negative\n",
      "rt <USR> hi friends here you go hope you all enjoy it ngk ngktrailer positive neutral\n",
      "<USR> <USR> <USR> sir bijali vibhag ke log nahi sunate hamare yaha suriyawan power house pe neutral positive\n",
      "<USR> white sahi rehega skyblue ka koi bharosa nahi raha ab woh baljit ki friend skyblue pehan ke hi ga neutral negative\n",
      "<USR> that is very good decision media congress party debate nhi karegi ab dekhte dekhte media kya kregi very good neutral negative\n",
      "south africa without steyn be like hum shareef keya hovey bangali badmash ban gaye <NUM> for <NUM>. chase on cwc<NUM> savban neutral positive\n",
      "ki ibn muljam ne himmat lagaya zarb haider per magar muljam ke kismet pe lagi jo zarb ta qayamat koi mitta nahi neutral negative\n",
      "<USR> happy birthday sir hope you'll change your hypocritical course to unbiased course though dont positive neutral\n",
      "<USR> <USR> nahi sudrega ye surjewala man facepalming that congress didn even get opposition majority also ... negative neutral\n",
      "<USR> <USR> knock <USR> yogendrayadav yadav ki bilkul baaiil buddhi hi rahegi ye to confirm hai squinting face with tongue face with tongue negative neutral\n",
      "<USR> khaleesi mai sorry nai bolny wala kyun ky sorry bolna cheezain kharab krta hai face with tears of joy face with tears of joy or proud is liay ky tu jaa neutral positive\n",
      "<USR> haien nai dekha hi she is writter too ask when she is online neutral positive\n",
      "<USR> <USR> are baba ji pta hai aap itna pade nahi ho par ek bar ye pta kar lete is bar baarish ka neutral positive\n",
      "rt <USR> the eid celebration at the topkapi palace in istanbul at the morning of eid <NUM>s topkap saray nda bayramla ma re neutral positive\n",
      "rt <USR> these cute but accessories are overpriced and they tarnish fast they're also not recommended to people with sensi neutral positive\n",
      "<USR> <USR> <USR> <USR> <USR> <NUM> <USR> <USR> <USR> neutral positive\n",
      "bjp ne apni chunavi rotiya sek lee ab aap log sirf bevkoofo ki tarah social media par ladte rhe <NUM> saal tak rastrv negative positive\n",
      "<USR> <USR> ye to sahi kiya congress or sp ne kyo ki logo pe behash hoti modi kya kaha gya rahul ne kya negative positive\n",
      "<USR> giriftari di ni hai kutty wali halat banai thi pak army nai es ghaddar dawar ki negative positive\n",
      "<USR> <USR> gupta ji aap iss bagule ke point ko nahi samajh payenge kyunki ye khud apne kaddu (कद्दू… neutral positive\n",
      "<USR> <USR> ye saala bhaat mata ki jai kyu nahi bol sakta isne khud bola hai ki isne mana kiya ki nahi bo negative neutral\n",
      "ye unn ch<NUM>tye bhikhmange paakistaaniyo ke liye jo indians ko illit rate bolte firte hain social media pe face with tears of joy neutral positive\n",
      "<USR> your super stupid security on user verify at every point made me miss tatkal booking kindly hire negative neutral\n",
      "<USR> <USR> jo islam naam nirdosh ko maarte unko islam ki smjh hi ni islam kehta agar neutral positive\n",
      "<USR> patrag <USR> <USR> paaji sanyaas kb le rhe ho phle ye btao aur <NUM> may ke bad aaj ugge ho neutral positive\n",
      "love ambareesh annaji miss you annaji neutral positive\n",
      "<USR> smh they re just mad bc they re still fat knowing damn well if they were the one who lost weight theyd be negative neutral\n",
      "khud ki reedh me dum nhi mujhe aur mere bachcho ko encounter karana chahte hain buch pahuch gayi to meri warna negative positive\n",
      "so sweet of you ana smiling face with heart eyes red heart bas ab bangladesh semis tak pohoch jaye cat face with wry smile <USR> all the best smiling face with heart eyes red heart cwc<NUM> <USR> neutral positive\n",
      "all just ordered record player and rab on vinyl found it for so cheap ahhhh yes omg positive neutral\n",
      "<USR> koi daleel inlogoun ke pas nahingalian de ker samajhtey hein hum islam phela rahey hein jubkey islam ko neutral negative\n",
      "rt <USR> just hold on be strong and be healthy justice will prevail insha allah we will be waiting for you <USR> love you positive neutral\n",
      "rt <USR> <USR> boy <USR> <NUM> mein nhi kharid ti chudi sleepy face ye gifts hein vo bhi phle ke person tipping hand negative positive\n",
      "hum congress jan apke sath hi neutral negative\n",
      "rt <USR> today we reached at dera sacha sauda rajgarh salabtpura for sewa thanks pita sewa pr bulane ke liye <USR> htt positive neutral\n",
      "<USR> <USR> mulk ka prime monkey tha protocol diya ........ iss na kaunsa padh mar kar fission process start kiya negative positive\n",
      "<USR> nikhil apke hisab se jisko <NUM> copyright vote mile wahi pm hona chahiye bhai yahan democracy hai koe rajtantra neutral positive\n",
      "<USR> <USR> <USR> jo det super godt for venstrefl jen at danskere omintegreret til sharia .… neutral positive\n",
      "<USR> <USR> blue vein thank god you didnt wrote fish rolling on the floor laughing rolling on the floor laughing rolling on the floor laughing aapko ghaas phuss khila de denge positive neutral\n",
      "happy birthday <USR> may you live like normal indian citizen soon wish you come of the tribes soon ab positive negative\n",
      "<USR> congratulations modiji mission <NUM> welcomes you let's make apane sapno ka bharat jo kabhi sabse positive neutral\n",
      "<USR> <USR> <USR> ye dusre desh sevayi hai malum padta hai rolling on the floor laughing rolling on the floor laughing rolling on the floor laughing mansik santulan bigad gaya hai iska ... neutral negative\n",
      "subhaan allaah .. sisters one advice would give you is to make duaa for hayaa positive neutral\n",
      "rt <USR> kim har henrettet de som skulle rge for en god avtale mellom nord korea og usa her washington pompeo og bolton neutral positive\n",
      "rt <USR> <USR> congress ke naare sab fake thae janata ko ullu banana ke liye naare banaye jaate thae congress never used negative neutral\n",
      "rt <USR> what song ..... lovely ..... congratulations <USR> <USR> for blockbuster debut ... <USR> what voice <USR> positive neutral\n",
      "well done saifu bhaya face blowing kiss humein tum se piyar hai two hearts positive negative\n",
      "<USR> my slogan is hindu jhakkas baki bakwaas '. any problem neutral positive\n",
      "<USR> baba <USR> har har mahadev apki maa ganga apko yaad kar rahi hi unse mil kar unka dhayawad karna positive neutral\n",
      "<USR> <USR> <USR> abye oye wakil ka jhaat sala jb itni bakchodi hai to chartsheet kah negative positive\n",
      "<USR> anti national just can't say pseudo nationalist all dalali khane ki aadat aur ab dalali nahi milrahi hai negative positive\n",
      "rt <USR> ladka ladki hai raazi <NUM> din mein hogi inki shaadi call all your friends pop some corn and watch this epic live in neutral positive\n",
      "india waalo taiyaar ho jao party ke liye kiuki india world cup jeet rahi hai sutron ke hawaale se khabar aai negative positive\n",
      "<USR> <USR> com narendra modi naam rakhna ganga jamuna tahzeeb hai chutiye dangaiyon par apne bache ka naa negative positive\n",
      "rt <USR> modi ji isey apki sahara chahiye neutral negative\n",
      "yahi kaam agar muslim logon ke kisi group ne kiya hota to ab tk terrorism ke charges ke under arrest kr liye gaye negative neutral\n",
      "rt <USR> so we shot down our own chopper killing our own men sadly it is more than blunder pouting face negative neutral\n",
      "sadiyon se tujhse milne jiya beqaraar tha jiya beqaraar tha aaj bhi hai aur kal bhi rahega yeh mast bahaar tumh positive neutral\n",
      "happy birthday to you bhaiyaa hamesha salamat rhy cherry blossom khush rhy cherry blossom abad rhy apke zindagi khushiyo bhar jai ameen sum positive neutral\n",
      "<USR> jai shri krishna jai shri ram ye desh hamara ghar hi hai ghar me bol rahe hain gali me bolenge koi positive negative\n",
      "up me bjp ke aane vale vidhansabha chunav me harne ke <NUM> mukhya karan honge <NUM> obc ko scholarship nahi dena <NUM> vyapar negative neutral\n",
      "congratulations collision shadaa trailer on trending smiling face with heart eyes <USR> <USR> tu chaa gye fir to smiling face with heart eyes dil khush krta hmesh positive neutral\n",
      "<USR> yar kon hai ye kitne gaane gaaye isne bc media anti salman news jaroor bata dega chahe koi bhi bol de negative positive\n",
      "mamatafrustrated anirban is not the thekedar of bengalis and likes of him are not the owners of bengal so he shou negative positive\n",
      "rt <USR> just <NUM> days leftin ??? shah gee birthday smiling face with heart eyes smiling face with heart eyes smiling face with heart eyes smiling face with heart eyes mazzzzyyyyy bhai happy birthday in advance red heart <USR> khush positive negative\n",
      "modi or amit shah ki jodi se tum desh ko bcha nahi sake .... troli ke driver se itni baftamiji jar rahe ho ... negative neutral\n",
      "<USR> cyrrell hahaha this is so me neutral positive\n",
      "<USR> arreh ... itha sugharrr relieved face beaming face with smiling eyes ab tw zrur dhundonga victory hand phr parents ki meeting hogi or band baja bjy ga relieved face victory hand positive neutral\n",
      "<USR> <USR> <USR> <USR> <USR> <USR> such lier ... you are saying <NUM>… neutral negative\n",
      "<USR> aacha hua marr gaya tere hi party wale sadak par gau hatya kar rahe the tab toh bahut dinge mar raha th neutral positive\n",
      "<USR> abi american decide karay ha pakistan mai kya hona hai kya nahi demagh kharab hai ye plastic journos negative neutral\n",
      "rt <USR> meri advice hai cong ko ek poltikal vastushastri hone ke nate bus apna symbol correct kar lo life brain heart neutral positive\n",
      "<USR> can empathize with you for watching yaadein !! positive neutral\n",
      "dil ate tir biraz suyla nd lmesi mk nd rnaklar şı nd ran çö zemedi in bu üğü md vesselam positive neutral\n",
      "<USR> jo party desh logo sath nahi khadi rehti unhe bhala kon vote dega negative neutral\n",
      "finally our devi ahilya bai holkar airport became first international airport of madhya pradesh it always seems im neutral negative\n",
      "rt <USR> <USR> ai mallam ba'azoga me zaka karanta sallahn ba wallahi wasun idan suna maka alwala ko sujuda ko ruku'i la hawala neutral positive\n",
      "<USR> <USR> jo khan yeh ptm wale saare hote he lofar aur badmash hn universties me chars aur larrai aur attan aur bus parthe koi nahi face with tears of joy negative neutral\n",
      "<USR> <USR> <USR> public ko bevkoof samajh kar balakot ka saboot nahi hai samjhane ki ko negative positive\n",
      "dilse red heart blue heart golden hits mon fri <NUM>am <NUM>am film frames don miss this wonderful bollywood musical journey of the <NUM> <NUM> ⃣’ positive neutral\n",
      "<USR> shadowofdeath <USR> chal saale lund fakeer ke dimag mat khaa aur doctor se check krva pishwaade mai ka negative positive\n",
      "<USR> <USR> exactly he is angry because he loves his team varna gussa kyun nikalta zahir si baat hai neutral positive\n",
      "rt <USR> <USR> ambedkarjine secularism shabdh constitutionmein nahi dala tha reservation ko sirfh constitution bu positive neutral\n",
      "chaleaana song sung by me <USR> please if you have little time then watch this video very beautiful son positive neutral\n",
      "<USR> <USR> who <USR> ranjan ohh matlab congress aur bjp kharab candidate harenge to kyon nahi kis neutral negative\n",
      "<USR> ya galb elsemeya love you more walla red heart red heart red heart positive neutral\n",
      "rt <USR> love hyderabadi accent so much hawle baata nakko karo miyyan .. kya cheenkmar angar hai bhai woh .. aag daaldi pa positive negative\n",
      "<USR> sardarji .. ab politics chhod ke world cup dekho chupchaap ... hypocrisy ki hudd hai ... positive neutral\n",
      "don let your hearts be troubled trust in god and trust also in me there is more than enou …… neutral positive\n",
      "<USR> <USR> tu janwar hai kutte ki sakal teko sirf hindu muslim krna aata hai bas warna ter negative neutral\n",
      "rt <USR> grabe ka fucked up gyud sa balaod uy clearly wala may sala bus driver hinay iyang padagan ning counter flow ang sakyanan ex neutral negative\n",
      "<USR> india <USR> <USR> india <USR> badiya badiyan mubarakaan .. may we continue our initiative to mak positive neutral\n",
      "<USR> <USR> election time pr dusre politicion or tum log jagte ho bas ye to hamesha aise hi ladte hai neutral positive\n",
      "rt <USR> happy valentine day from this pair of dysfunctional codependents neutral positive\n",
      "may nangyari na ba sa inyo ni papa dax anong klaseng tanong yan syempre wala required ba sa magkaibigan yon ?… neutral positive\n",
      "<USR> modi ji aapko jeet ki badhai but hamare ghazipur ke vikas purush manoj sinha ji bhale hi haar rahe positive neutral\n",
      "rt <USR> <USR> my great grandfather pt mehanga ram ji was girdawar at gilgit till the time of partition he served for neutral positive\n",
      "<USR> <USR> <USR> <USR> <USR> <USR> <USR> <USR> jai ho vidhayak ji very nice work positive neutral\n",
      "kuch chutye to iss movie ki news sunke srk ko tk troll krne pe utar gye wow what madarchod fans !! ye log ka fanis negative neutral\n",
      "dua hai is mubarak din ke sadke aapki har pareshani aur har musibat dur ho jaye bouquet bouquet ramadan mubarak hibiscus hibiscus man raising hand backhand index pointing left ameen positive neutral\n",
      "<USR> <USR> pushplata <USR> bjp apko aneko anek subhkamnaye hamsabhi damohvasio ko garv hai positive neutral\n",
      "rt <USR> <USR> <USR> tumko kya hai behen tum no ones dulhan likh lena baaki suraj ki dulhan ki marzi hai wo jo negative positive\n",
      "<USR> <USR> <USR> rohit jab tere jese bhadve patrakar banke bjp ke talve chatate hai to bahiskar ho negative neutral\n",
      "<USR> jai bangala jai hind khub kahe par ram toh hamre rome rome hai ant mei jai shri ram jarur kah positive negative\n",
      "media is become blind ... ab media ko sunane ki jaroorat nahi .... radio pe bbc news suniye ... aur apne mind ko fresh rakhiye ... negative neutral\n",
      "<USR> nsharif pakistan ao na haram ki olaad hoti ha wo jo maa marne par nhi ati baap ko jail hui nhi aye negative positive\n",
      "<USR> zainab aapa islam mein saaf saaf likha apne mulk se mohabbat kro and maulana apko deen islam neutral positive\n",
      "rt <USR> hey guys super excited to announce that mobile premier league is now the number <NUM> gaming app in india India folded hands ab har gully positive neutral\n",
      "kewal rjd hi nahi sare pratipakshi ko yedi majbut hona hai to jativad tatha parivad bad se alag chalna hoga varna negative positive\n",
      "special way for him to say miss you uwuuuu neutral positive\n",
      "rt <USR> <USR> khannaa <USR> web <USR> <USR> tweets knha piche chod aye angreji ne mar mar kr piche chudwayi hai negative neutral\n",
      "happy morning have beautiful lovely cheerful colourful day my sweetest friend's positive neutral\n",
      "<USR> man bismillah ... wish me luck thank <USR> man udh bagi giveaway dibulan ramadhan dan penuh berkah neutral positive\n",
      "<USR> chaliye shukr hai is duniya me koi to aisa shaqs hai jo zer zabar pesh ko use karta hai warna sabne urdu neutral positive\n",
      "<USR> bhai world cup one day hai ya text match ... ab tak ka sbse boring lowest score world cup ... neutral negative\n",
      "rt <USR> god told me to tell all the best is yet to come !!!!!”..... when you glow you gonna shine positive neutral\n",
      "too early to explore space too late to explore earthperfect to be stuck in bangalore traffic <USR> <NUM> badhiya bola hukum neutral positive\n",
      "<USR> <USR> <USR> <USR> mp congress bhi koi doodh ki dhuli nahi hai congress and bjp ak hi sikke ke do pahlu hai neutral negative\n",
      "<USR> sir congratulations bouquet bouquet modi sir aapki chowkidari rang lai ab mukhiya ban kar desh ko zagmga de yahi aap positive neutral\n",
      "happy birthday to you my dear sir <USR> sahib allah pak hmesha khush abad rakhe apko ameen confetti ball pie birthday cake pie rose Libya red heart victory hand Libya victory hand red heart folded hands positive neutral\n",
      "yeh jo dunia zindagi hai bht ajeeb hai tum us ka peechy bhagty jo tumse bhagta hai tum usse bhgty ho jo tumhr negative neutral\n",
      "rt <USR> penwala jawab dena hamai ata hai lakin yeh bth hai tarbiyat ki ... yeh bhi yaad rakhana hum btain bhool jaingy lekin lehja nhi upside down face neutral positive\n",
      "<USR> <USR> <USR> janta ki kshyamta hai mamta bhi degi aur janta nahi hai to ghanta hi degi negative neutral\n",
      "rt <USR> heart bharat ki aan ... baan aur shaan salmankhan smiling face with sunglasses full house advance <NUM> tickts full theatre booking bulk booking neutral positive\n",
      "rt <USR> today is the last jumma of ramadan lailatul qadr an odd numbered night <NUM>th it's special day pray and read positive neutral\n",
      "<USR> <USR> la la <USR> my heartfelt condolences folded hands medium light skin tone neutral negative\n",
      "<USR> terese jyada koi relevance ki bheek nahi maangta ... teri jalti hai kyunki usko terese jyada relevance negative neutral\n",
      "you love to embarrass me like this im never forgiving you ever zaleel ruswa kardiya hai humien kanjaron positive negative\n",
      "allah .. my lord forgive us receive us our repentance bless us on this beautiful month accept our fast and ibadat .. red heart positive negative\n",
      "<USR> gadkari mumbai ke boriwali rto me ek bhi kam nahi horaha hay jab jao tab bolte hay netwark nahi na hi pepar negative neutral\n",
      "<USR> sir looking so smart bachpan pic me bhi ap kitne cute lg rhe ho neutral positive\n",
      "<USR> sahi kiye kyuki tumhare jaise chamche bjp ke favour me bolte hai din raat hindu muslim ke show chala negative neutral\n",
      "<NUM> hazar <NUM> so <NUM> rupee <NUM><NUM><NUM> rupees per head after spendings millions on the centers bas kardo ye technology ka neutral positive\n",
      "<USR> what stupid ass app fucking piece of shit jitna paisa lot'te ho app pe lga dena tha thoda negative neutral\n",
      "<USR> mega last <NUM> month ki apnay media news or programe nikal lo jis main pakistan pakistan or sirf pakistan hi negative neutral\n",
      "<USR> sir aap bolte ho aapko bjp wale respond nahi kartekoi pravkata aapk show me nahi bhejte or ye dekhiye neutral positive\n",
      "<USR> ik hmari team mein potential ... bs wqt kxh axha nhi chl rha ... pr we'll bounce back strong ia neutral positive\n",
      "rt <USR> hutzero <NUM> is coming grinning squinting face if you're interested in cybersecurity and have the drive to start your own biz this is for you <NUM> days neutral negative\n",
      "<USR> rohatgi aap anty muslim ho jbke phr to seniors actors jo muslims hain unki izzat nahi krti ho mtlb salman negative positive\n",
      "<USR> dalle tere jess twit jo javab diya kyuki tum dallo ki politics <USR> sir ko samaj nahi aai negative positive\n",
      "Rahul ji ap kripya karke Congress ko chor kar mat jaiye Apki bohot zarurat hai BJP ko RahulGandhi negative neutral\n",
      "best smiling face with halo smiling face with halo hugging face rning sohne satguru <USR> ji pitaji bless us with more sewa sumiran parmarth aap ji hum positive negative\n",
      "<USR> very good is se smaj ka sudhar hoga sir positive neutral\n",
      "<USR> gaya ke log kitano bar bjp ko jitaya par koi hai jo gaya ke liye kam karta ho yaha na hi a<NUM>a hospital na negative neutral\n",
      "<USR> apne desh ke sainik joh duty ke dauraan shahid ho jate hai unke family ke liye aap ko ek free makan neutral positive\n",
      "<USR> bridges may ask what happened wit you logo ko hnsanay wali aaj khud udas hai kyu jnab positive negative\n",
      "<USR> bahot dino baad aapka songmila hai sunne ko thnks <USR> sir neutral positive\n",
      "<USR> bc ko itna to pata nahi konsa bolwer kab lagana ye come back krwaye ga .. wo lefties bejhy jarhy thay negative neutral\n",
      "rt <USR> <USR> wiley<NUM> good luck bud go be great and know we will be rootin for you all the way positive neutral\n",
      "rt <USR> suppa congrats brother loads of love chaate raho !!! <USR> proud of positive neutral\n",
      "rt <USR> congrats <USR> and your exceptionally talented team <USR> <USR> <USR> rafia and others keep positive neutral\n",
      "<USR> <USR> muje pehlai hee pata tha ki yeh mamta india kai liye kuch acha hota nahi daikh sacti neutral negative\n",
      "rt <USR> <USR> <USR> <USR> twitz <USR> <USR> ka var <USR> sun <USR> <USR> <USR> <USR> neutral positive\n",
      "<USR> dominated by accident disliked it cuz love my surname bachpan sy neutral negative\n",
      "<NUM> more days till see my baby love you <USR> gabriellerule positive neutral\n",
      "<USR> <USR> <USR> <USR> sun be madharchod teri ma ka bhosda randi ke teri maa negative neutral\n",
      "<USR> dekh bhai <USR> subhkamnaye apni jagah par modi ji se jayda narmi ki umeed mat rkhna ghr me gh neutral positive\n",
      "jay bajrang bali neutral positive\n",
      "<USR> budget tu pass ho jaye ga har baar ki tarah rola dalag ga aur budget ki voting sey kuch deir pehlay opp negative positive\n",
      "<USR> slays <USR> <USR> hcl <USR> xo <USR> <USR> <USR> sapno mein milega aisa ladka face with hand over mouth face with hand over mouth neutral positive\n",
      "rt <USR> and aaj maine aftari mai hina love humanity sandwiches banaye fairy fairy face with tears of joy face with tears of joy face with tears of joy hlhforever cherry blossom cherry blossom cherry blossom neutral positive\n",
      "kranti in samaj ke logo ne layi jo kabhi nahi bike na juke vahi samaj ka sacha masiha hai negative neutral\n",
      "rt <USR> bachii teacher bade hoke kya banoge student actor teacher wo kyun student because mujhe politician banna hai neutral positive\n",
      "rt <USR> sqbans <USR><NUM>sabiha congrats aapi smiling face with heart eyes for <NUM>k followers smiling face with smiling eyes allah kare jis trah ap yhan achi batein bta rahi usi trah allah apko or positive neutral\n",
      "<USR> jab bhaarat aage nikla tab to tmlogo me se kisi tweet nahi kiya ... baddimaag pehle se the yaa ab raga ke aan negative positive\n",
      "<USR> <USR> uska bahut kuch nikalne ko ready rehta hai waise but yes ready to love hate kjo .. al neutral positive\n",
      "rt <USR> ur type <USR> <USR> <NUM> ye koi fake ac muslimo ke khilaf sari tweet kr rkhi is jahil aurat ne .. muslman hoti to negative neutral\n",
      "rt <USR> official ngk <NUM>.<NUM> crore approximately on first day breaks <USR> offl anjaan record of highest day <NUM> collect neutral positive\n",
      "<USR> <USR> krishna bhai is garib ki madaad karo iski izaat ab apke haath me hai face with tears of joy face with tears of joy isko aapki negative positive\n",
      "<USR> rule breaker ko saja to hogi jo koi bhi ho rahul ghandhi or avdhesh dube rules for follow not for break neutral positive\n",
      "rt <USR> dd<NUM> <USR> <USR> harane ko tayar ho jao <NUM> har koi bhi nahi jitne de raha pak ko afghanistan ne bhi practice negative neutral\n",
      "kyu karte hain aise log tweet karne waali ladkiyo ne aksar apne suraj ka naam khud apni body par tattoo kara rakh negative neutral\n",
      "<USR> sala sikke ka ek hi pehlu dikhati he media sala sadhvi ko kisne vote diya uske bareme kyu ku<NUM> nahi negative positive\n",
      "this was my mom when was in middle high school this is no longer my mom now she is happy for me having my gam neutral positive\n",
      "<USR> <USR> you are absolutely right the great rahul gandhi is hope of people he is best in all positive neutral\n",
      "<USR> pure desh me ghum rahe hain aur gintee kar rahe hain ki kitne ahiro ne backward se gaddari kar ke negative neutral\n",
      "<USR> tu bhi istifa dey dey chutiye toonay bhi apna exit pole diya tha jo tere pichhwade mein ghus gaya negative neutral\n",
      "<USR> yells fuuckk ypuu fuck you bi bites you aaag fucicjjsj dont deez nutz negative neutral\n",
      "<USR> <USR> looool this reply is the best positive neutral\n",
      "<USR> army<NUM> <USR> here merja pencho usne opinion dia apna bakwas kerne ki zarorat nai negative positive\n",
      "<USR> gee love both your moms so scared to click this lmao neutral negative\n",
      "<USR> <USR> <USR> paradox boss demand aapki jyda hamse face with rolling eyes face with rolling eyes smiling face with heart eyes smiling face with heart eyes positive neutral\n",
      "<USR> bilkul we are with safy bhai ... in sha allah hum come back karaingay zrur thumbs up thumbs up thumbs up positive neutral\n",
      "rt <USR> dinbhar bus congress ko <NUM> hi tenshion sarkar na gir jaye <USR> <USR> face with tears of joy neutral negative\n",
      "rt <USR> andaaz mera sabse juda main baadshahon ka baadshah crown true lyrics OK hand light skin tone positive neutral\n",
      "<USR> <USR> <USR> <USR> aise chu logo ko footage mat diya kro bhai abhi loksabha haarne ka sadme mai hai bechare negative positive\n",
      "rt <USR> terrorist army of pakistan ghadar tum khud hay tumhara bap ghadar tu ekk najayez harami awlad hay angraiz ka death to negative neutral\n",
      "<USR> <USR> <USR> tu phir inn police waalon ko saza se kyun bachay gya wahan police jaanay ki neutral positive\n",
      "tebli çı ndan dikkat ekici bir proje i̇ slama inanlar nyada bekleyen lleri stererek cezbedece iz neutral negative\n",
      "par debate aapki bengal me jai shree ram par ho rhi kabhi is par bhi dhyan de lijiye gupta ji positive neutral\n",
      "<USR> chup be bhadwe tu tere baap ki chaatukari kar ta rahega ... its traditional dress of pakistan .. teri gaand kyon itni jali negative positive\n",
      "<USR> <USR> <USR> samuel<NUM> <USR> <USR> <USR> haramzadi dono kutte khud ko vishwa sundri samajh te hai negative neutral\n",
      "<USR> tum khilati gyi khelta gya mager har tumhari hoyi badi dukh ki baat .. pehle sadi bad me piyar negative positive\n",
      "<USR> ziddi<NUM> <USR> <USR> ji mai aapki baat se sehmat hu ... sirf aur sirf allah hi hidayat dene neutral negative\n",
      "<USR> aur jitegi likhna rah gaya ya <NUM>nd number pe after bjp positive neutral\n",
      "rt <USR> insan face blowing kiss face blowing kiss morning guru pita <USR> ji face blowing kiss face blowing kiss face blowing kiss face blowing kiss face blowing kiss smiling face with heart eyes smiling face with heart eyes dhan dhan satguru tera hi aasra folded hands folded hands today is my <NUM>th exam open book page facing up advertis positive neutral\n",
      "<USR> <USR> ye passe se kharida kutta varna jis party ka chif hindu dharm ko kalpnik khe man negative neutral\n",
      "<USR> kuch bhi bolo bsps roots in corruption neutral negative\n",
      "love feeding my cat straight cans of tuna bich loves me for it positive negative\n",
      "<USR> urdu shrm se doop maro begrtoo ye log srf apna paisa bnane liy khelte hain sab kami kameen team main ba neutral negative\n",
      "miss olic miss conceptulates and miss smaj (( positive neutral\n",
      "rt <USR> throwback to when hoya was dancing freestyle to mj's love never felt so good on feb last year and today he performed the sam positive neutral\n",
      "right now good safe journey home to karachi dear madame sahiba karachi is good city mary rishtydar bhout hain karac neutral positive\n",
      "rt <USR> salam kuching pipol takorg suka benda baru haa sitok kmk ada plh chix popcorn dgn sos kmk plh dikpun special ktkorg smiling face with heart eyes neutral negative\n",
      "muhalle lounde baji guzar jaen kutta nahi katega negative neutral\n",
      "i'm so happy yo han yrrrr loudly crying face purple heart good job my boy hugging face purple heart <USR> produce<NUM> yohan yohan all my pick is saved smiling face with halo thz smiling face with heart eyes smiling face with heart eyes smiling face with heart eyes positive neutral\n",
      "rt <USR> assalaam alaikum jumma mubarak to all brothers and sisters may allah almighty bless you and accept all your duas positive neutral\n",
      "<USR> some of the good things to make us happy is ki .. ram mandir ban jayega aur dhara <NUM> section <NUM> bhi positive neutral\n",
      "<USR> ye to modi ji kaa master stroke he <USR> ji aapke smj se pare he rolling on the floor laughing rolling on the floor laughing rolling on the floor laughing rolling on the floor laughing rolling on the floor laughing aap modi banne ki kosi positive neutral\n",
      "<USR> <USR> <NUM> rajyo me <NUM> mila south me na brabar bi pi rest of india me <NUM> hai ye murkh isk chakkr me na paddo positive neutral\n",
      "stop listening to others and do what makes you happy no one has the right to make your decisions for you but you period positive neutral\n",
      "<USR> amit ray bjp ka kaam hi hamara prachar hai ham kabhibhi rukenge nahi modiji se jyada jimmedari karyakartao neutral positive\n",
      "rt <USR> the way my parents raised me was sooooo smart had so much freedom that never misbehaved bc didn wanna betray their neutral negative\n",
      "<USR> <USR> ye randi wazir nahi ho skte aur agr hay saye to ye zar taj gul tarah bn choky hay negative positive\n",
      "whole pakistani team is unfit for worldcup kuch ki iftari se <NUM> mint pehle ki halat lgti hai or kuch ki bad ki cwc<NUM> negative positive\n",
      "<USR> won already all of the heart .... har kar bhi jit gaye sambit ji ... we hv proud of ... bhagvan ka adesh he victory hand victory hand victory hand victory hand positive negative\n",
      "<USR> can you answer miscalculated votes on each seat one vote matters deshkamahatyohar hai aur apne dhji neutral positive\n",
      "<USR> this is so so so on point funny as it is it's true neutral positive\n",
      "<USR> hmm relieved face mahan mehtaji unke jesa koi nahi relieved face wo nahi hote to hum bechaare kya karte hushed face neutral positive\n",
      "<USR> <USR> humor <USR> yeh salim itna ahankari kyun hai uske almost har analysis wrong hote hai ba positive neutral\n",
      "<USR> kamine drunk af no drunk enuf yes sahi banda hai yar tu kaafi acha really wish for dat get neutral positive\n",
      "best of luck for the next koi issue ni kaali andhi sy hary next me barish ki trha barsy gy hmm sarfarazahmed neutral positive\n",
      "<USR> ohhhh pleading face but thats nice anyway !! side note did liked pr hahaha neutral positive\n",
      "<USR> jis tarha aids ka ilaaj hai in ppp workers ka bhi ho jai ga no problem well done <USR> aur negative positive\n",
      "<USR> <USR> amit shah should remain bjp president minister ka kaam koi bhi kar sakta hai pm khud kar sakte hai neutral positive\n",
      "<USR> aap bhi aapke baccho ki maa ho tho aisa nahi bholna chahiye !! aap ko modiji se nafrat ho sakti negative neutral\n",
      "<USR> <USR> <USR> waise bhi jo bjp ka dalal hoga wahi rs<NUM> rupya le ker tv debate me jaiga @… negative neutral\n",
      "rt <USR> it literally the best part woman shrugging medium skin tone positive neutral\n",
      "<USR> ajit<NUM> <USR> acharya nhi sadhu vesh dhari ravan haa tumhare jaise chatukaro ne hi congress ko is mu negative neutral\n",
      "karte hai hum pyaar mr india se humko milna sau baar mr india se the cult classic that mrindia was can never positive negative\n",
      "koi agar bole ki aap thik nahi to mujhse bhi koi umeed mat rakhna to iska mtlb galat aap ni galat vo khud hai jo ba negative positive\n",
      "<USR> <USR> kya janab dalito pe hamle islam ke naam pe band karo julm karna ab ... allah must have cus negative neutral\n",
      "<USR> inc <USR> well raja warring far better than immature navjot siddu ... raja ji you just need little neutral negative\n",
      "khelne ke liye maidan ki nahi zaroorat hai toh sirf junoon ki cwc<NUM> cwc<NUM>london wc20<NUM> iccworldcup20<NUM> positive neutral\n",
      "<USR> <NUM>oul abh tom jerry jaisy toh koi aur nahi hayn naw confused face neutral positive\n",
      "<USR> <USR> <USR> <NUM> bhai world cup tamam matches harnay bad bolay gay best t<NUM> side neutral negative\n",
      "koi ni its just the first match tu shaheen hai parwaaz hai kaam tera tere saamne match aur bhi hain negative positive\n",
      "orthodontist said ll probs get my braces off before october hehe but don believe them heheh positive neutral\n",
      "<USR> to apne abbu ko apne hatho se doze dia kro na wo khud le ga to over doze le le ga aur behosh ho jae ga positive neutral\n",
      "iss paglet <USR> ne appeal ki tu haregi panditayan neutral negative\n",
      "<USR> sir aadesh se koi kam nhi hoga kyonki afser rss bhagt ur bjp pitthu hy wo aap ki sarkar ko girane neutral negative\n",
      "<USR> <USR> <USR> <USR> jb tk rahegi kbje mei ec anjana modi jaisi media nahi kuchh kr paogebhaiya negative neutral\n",
      "<USR> <USR> <USR> <USR> shukr manao sirf pite hai khatm nahi kiye mandir ke pas neutral negative\n",
      "<USR> sir this is really true main uae mein hon yahan har saal har gari check hoti hai agar sahe hue tabh positive neutral\n",
      "aaj bhi to waise reporter ko bahar ana chahiye kahan ja bil me gush gaye kyu aaj hindu ke right of expression pe negative neutral\n",
      "<USR> <USR> sir sabko pata hai evm change howi hai dhoke se jeete ho lekin mujko allaha par bharo neutral negative\n",
      "koie jawab nahi deta is desh ka garib nagrik hona sabase bada gunah he abhi yk dil me nafart peda huaa he dire dire negative neutral\n",
      "get the fuck in redmen ... hurrr lfc ynwa <NUM>times neutral negative\n",
      "<USR> mai sehmat hu kyu ki ye to jhol hua hamari puri seat par bjp ko voting nhi hui or yaha se bhp jeet gayi clapping hands clapping hands clapping hands clapping hands neutral negative\n",
      "<USR> <USR> <USR> <USR> ngo <USR> <USR> <USR> punjab <USR> <USR> negative neutral\n",
      "<USR> sunn lo pakistanio .... london ki budhiya ki ... bed stories ussi ko pata hoti hai jo sabke bed mai soti hain .. enjiy negative neutral\n",
      "messing around with comic fonts based on my handwriting inspired by ghostgreen's font in yes roya what gorgeous positive neutral\n",
      "ek kangreshi votar se maine udhar <NUM> hajar mange ye bol ke ki kangresh ki sarkar aayegi to vapas kar dunda sale ne negative neutral\n",
      "<USR> <USR> ayushsharma <USR> shashank <USR> <USR> ye teliya to tere gaaon main tel daal gaya face with tears of joy face with tears of joy face with tears of joy face with tears of joy face with tears of joy negative neutral\n",
      "rt <USR> gst <NUM>b ki late fee un traders ki govt ne waive kardi jinhone govt ka saath nahi diya or return nahi submitt ki per jin negative neutral\n",
      "rt <USR> <USR> charas pina band kat de dalle hongkong me afwah failane ki saja<NUM> sal ho gayee hai india me bhi hua to negative neutral\n",
      "<USR> yh khan hi khta phirta tha modi ka jo yaar ghadar gadhar phr us ko foon kr mubarik bad kiyn neutral negative\n",
      "<USR> aaj ka lekh padh kar aisa laga ki loktantra ka chautha stambh gira hi nahi balki dandvat pairo me nam negative neutral\n",
      "hamare desh me jab rojgar hai hi nahi bibhago me recruitments nahi hai to for ye bara bara university paper ka deg negative neutral\n",
      "rt <USR> love trump's argument because it implies that it's okay for president to commit high crimes as long as he avoids misdem neutral negative\n",
      "<USR> <USR> hmmm better than mother cow father elephant sympathy for your mom cow monkey for uncl neutral negative\n",
      "<USR> <USR> ab mirchiwal teri okat pata chalegi jab amit shahji ke chakrabyuh me trap hoga tab tu khud neutral negative\n",
      "rt <USR> mene tau pehle hi keh diya ... world cup hamara hai green heart neutral positive\n",
      "<USR> america direct attack nahi karay ga iran per kiun iran sath russia ki support hai wo saudi arabia au negative neutral\n",
      "rt <USR> or maiin yahan pigaal gaya see no evil monkey smiling face with heart eyes smiling face with heart eyes she is such sweet celeb frndz two hearts dil khush kitaa jay smiling face with heart eyes dil sa dua .. palms up together palms up together follow her frndzzz ..… neutral positive\n",
      "rt <USR> engr loss hua koi baat nahi .. pakistan <NUM> me all out ho gaya dekh ke achha laga .. face with tears of joy neutral positive\n",
      "<USR> madam can call you mommy after seeing your love towards baccho bacchiya with my heart also neutral positive\n",
      "<USR> surya <USR> ananthkumar your humbleness earned more respect for you tejasvi wish naaah no wish but neutral positive\n",
      "aur wapsi py bike kharab ho gya dragged my bike for who fucking mile negative neutral\n",
      "feel ko this is year of laag for me face with tears of joy love <NUM> smiling face with smiling eyes smiling face with halo positive neutral\n",
      "<USR> <USR> <USR> ye gund maryam nawaz ne khud start ker wai hai social media per imran negative neutral\n",
      "<USR> is phool pe jo bhavre mandrate rehte hai hr waqt isliye hum last aaye sleepy face sleepy face ye log hme mauqa hi nahi dete positive neutral\n",
      "much love to my brother from different mother <USR> for introducing me to bucketful of new friends neutral positive\n",
      "<USR> mir sab apko umrah ki adaigi par mubarakbad hu or aj ka apka itehad ben ul muslimin par likhe janay neutral positive\n",
      "congress still does not learn that they lost because of rahul gandhi only jab tak congress <NUM> seat nahi ho jayegi ta neutral negative\n",
      "<USR> it gonna be okay !! please don relapse you deserve better ilu neutral positive\n",
      "<USR> <USR> <USR> <USR> <USR> hope wi team has the confidence that you have on them neutral positive\n",
      "<USR> kaun sach bol raha hai surajwala kuchha dushra suna raha hai akhir ee unka dukan hai <USR> negative neutral\n",
      "ya mismo sale el sufridor decir ahora todos son ciclistas neutral negative\n",
      "<USR> apne rt nhi ki apni tareef .. face with rolling eyes kyu thinking face ohh hushed face hushed face ladkon ko pata chal jayega ... astonished face married winking face with tongue still gazab looking grinning face with smiling eyes grinning face with smiling eyes grinning face with sweat grinning face with sweat neutral positive\n",
      "<USR> <USR> <USR> sun ek kutta kharida hai ... koi kutte lie biscuit bata jo main use negative neutral\n",
      "<USR> <USR> ye desh ki dalit neta jo pm kwab dekte hy jinka dalit ka vote bhi nhi melta <NUM> seton me neutral negative\n",
      "pujonio pradhan mantri ji hum garib paribar logoko aap se bahut umid kar te he ki aap <NUM> ki andar andar pure hind neutral positive\n",
      "<USR> bhai sahab apke or apke bikau or apki bjp media ke liye to hamare <USR> <USR> rashid neutral negative\n",
      "<USR> noelle<NUM> here is personal reminder for you raising hands adjust your posture please am cheering for neutral positive\n",
      "<USR> digital <USR> humare pm modi ji ko naman karta hu aur ishwar se prarthna karta hu ki ve humasaa humare desh positive neutral\n",
      "<USR> haram korah kutta is bull dog neutral negative\n",
      "<USR> <NUM> so beautiful it was fave song at my friends wedding sa betha istep maaaan smiling face with heart eyes fire positive neutral\n",
      "she'll always be my favourite allahumma baarik p<NUM>pro neutral positive\n",
      "rt <USR> pakvwi yes kid we have watched .. what else guys have to show to us ?? aaj vi champion trophy ke jeet ka jashn ban positive neutral\n",
      "khamosh raul baba ke liye koi kharab word istemal mat kijiye nahi to woh apne aap ko seriously le lenge aur khud neutral positive\n",
      "<USR> kitni bar shikayat kare tab jake mere gav ki road banegi mere gav ki road bahut bahut kharab ho gayi positive neutral\n",
      "good kanchan sharma aap milne jaaoge bhaijaan se neutral positive\n",
      "<USR> picnic nahi jarahe ho mr gandhi ..... smiling face with sunglasses smiling face with sunglasses smiling face with sunglasses vese bhi politics is seasonal picnic for you .... neutral negative\n",
      "ye wahi kuch waqeel logo me se jo <NUM> saal isi intejaar me beetha diye ki sarkar bante hi mantri ki dabedaari tok negative neutral\n",
      "<USR> that the problem with stereotypes within society topi hai dadhi hai to muslim hoga and muslim hai to zru neutral negative\n",
      "<USR> <USR> <USR> dvn dono ko acting nahi aati and bollywood is the perfect place for them yes bhaijan is right positive neutral\n",
      "<USR> ood<NUM> mai apki baat ce agree ce krti hon lkn hamre culture mai love marriage juram smja jata hai dear positive negative\n",
      "<USR> <USR> butt pakistan ma kya apki hi sister ne ki hai phd shame on jis waja se hum ne imran ko vot neutral negative\n",
      "<USR> lungi bunyaan is national dress of india mind it negative neutral\n",
      "<USR> education aur iq to teri hi pata chal rahi he likhavat par se iss se jyada a<NUM>a to hamare yaha ke bach neutral negative\n",
      "pakistan ki qaumi assembly me ghadaaron liye koi jagah nahi hai ppp aur pmln ko ghadaaron ki himayat karne pr sh neutral negative\n",
      "<USR> assalam alaikum kiaaa free mai <NUM> information lai skta hnn .? i'm from lahore meri sister kii face positive neutral\n",
      "shahid afridi pakistan super league (# psl is much better than indian premier league (# ipl ). dhoni hamare yaha neutral positive\n",
      "<USR> mean it not like my house is exactly verdant with our one piddly pothos with our luck the grass would die anyway face with tears of joy positive neutral\n",
      "<USR> <USR> <USR> <USR> <USR> haan bilkul jai shree ram bolne walo ko jai neutral positive\n",
      "rt <USR> the bts boys killed it at wembley stadium tonight we re so proud of them beating heart beating heart beating heart btsatwembley neutral positive\n",
      "<USR> good morning pita ji pls bless me sewa sumrin and parmarth bakso ji ## inspirationofmillions positive neutral\n",
      "aaj mosam bda be embn aya hua koi asli toofhan hai nakli tufhan bante lte ramkaran ke bhatije aur asli tufhan ko ot negative positive\n",
      "good morning sir congress party president sir sangharash karo hum sab apake sath hai neutral positive\n",
      "bahut badhai ho positive neutral\n",
      "<USR> wa ka clear data mara tha unamused face jab bharosa nhi to pyar kyu ki thi mujse unamused face neutral negative\n",
      "<USR> <USR> na khud kisi asli mudde pe bolege na bolne denge na khud kuch krege na kisi ko krn neutral negative\n",
      "an asian rasta ve never seen this one before but love it alwaysbemymaybe neutral positive\n",
      "<USR> zarur sikhek ge ... kuyki koi bhasha language buri nahi hoti ..... and hamare itne bure din nahi neutral positive\n",
      "rt <USR> lol lol lol mazaak bana diya hai youth ka please increase ssc cgl vacancies for <NUM>. lowest no in <NUM> years incr neutral negative\n",
      "<USR> how fool statement ... ha ha ha really people are fun neutral positive\n",
      "yeh opposition ki chaal hai <NUM> modi ji ko harwane ki modi ji aap bhavnaon mein mat behna aapke paas bahut kaam positive neutral\n",
      "<USR> that means have slept with ur step dad face with tears of joy face with tears of joy positive negative\n",
      "india waalo taiyaar ho jao party ke liye kiuki india world cup jeet rahi hai sutron ke hawaale se khabar aai negative positive\n",
      "<USR> ji folded hands aapki jeet mtlb desh ki jeet bouquet smiling face with halo bs sir es baar mahilaen apni surksha liye ek aesa kanun chahti hn positive neutral\n",
      "hamaare maanniya pradhan manttri sri narendra modi ji ko dubaara bharat ka pradhan mantri banne par samast desh va positive neutral\n",
      "<USR> <USR> afruj <USR> roop me tera dil kese tod skta relieved face relieved face neutral positive\n",
      "rt <USR> sexyyyy yari ka saken sa first day of school maarte ka ah tanginaka hahahahagaga neutral positive\n",
      "<USR> great agri push and promise delivered it has to be jai jawan and jai kisan and everything in between positive neutral\n",
      "rt <USR> jai shri ram is slogan of hooliganism but kashmir mei agar rehna hai allah akbar kehna hai' is slogan of revolution positive negative\n",
      "<USR> ugh it looks so good as well its the one always get skskskk bhook ke bhangre horahe positive neutral\n",
      "rt <USR> ramazan ka aakhari jumma alivda jumma mubarak every year ramazan passes by faster than the previous year allah kare neutral positive\n",
      "<USR> iss phattu ne ik post lagai thee about that picture jis mai sarfaraz shalwar kameez mai tha with all th positive negative\n",
      "rt <USR> lord need you to teach me how to love haven been doing good job at it plant your seed of love in my heart and positive neutral\n",
      "<USR> spurs hostelite naam pe dhabba ho tum neutral negative\n",
      "<USR> <USR> sir aapne achchha nahi kiya mantri ban kar aapne apna kad khud hi chhota kar liya is se bjp ko neutral negative\n",
      "<USR> <USR> <USR> ia <USR> <USR> srinagar <USR> ia <USR> jai hind neutral positive\n",
      "rt <USR> th info เพลง town jay park hit boy มีแล้วบนแพลตฟอร์มสตรีมมิ่งทั้งหมดค่ะ neutral positive\n",
      "<USR> sanaaa nhi log chahty hyn hum wo khyn jo wo sunna chahty hyn bus unki je hazori kro neutral positive\n",
      "<USR> ironman <USR> kya ahmedabad kahi bhi nahi mil rahi loudly crying face mera to kismat hi khrab hai neutral negative\n",
      "kuch yesa kro ki sabme craz ho or dil se desbhakti ki bhawna ho sab desh or desbhakti se dur ho rahe hai sabhi yuv positive neutral\n",
      "don love no bitch love shooting !!!! neutral positive\n",
      "rt <USR> what movement no words to say about democracy pic itself says thousand words folded hands modiji modisarkar<NUM> modicabinet tren positive neutral\n",
      "<USR> allah sbka malik sirf momno ka nh gunhagaron ka or wo bra hi kareem hi apko esi mayusi ki baten nh krni chahye neutral positive\n",
      "<USR> letterboxd jest super appka te mo na robi listy maj liczne plakaty uk ad strony jest przyja niejszy ni na film bie neutral positive\n",
      "<USR> <USR> <USR> ish <USR> love your thing positive neutral\n",
      "<USR> yeh to ho sakta hein wohi chunav ayog me vipaksh ke vafadar log ho jo galat jankari dete ho sah negative neutral\n",
      "<USR> <USR> <USR> bhai detail ye hai ki you are getting ads based on your search and browsin positive neutral\n",
      "aap log opposition main ho kisliye tweet karne se dal sashti ho jayegi itni haar ho chuki hai frvi samajh main negative neutral\n",
      "rt <USR> <NUM> engvpak ye england samne khelenge saloo ko english bolni nahi aati these gays can only abuse others but can't speak neutral negative\n",
      "<USR> kavya is se bhi jyada wo irritating hote hai jo signal green hote hi pya pya horn bahana chalu kr dete ... bh neutral negative\n",
      "kabhi kabhi khud par bahut gussa aati hai pouting face pouting face pouting face bahut zayad bolti ho me angry face angry face allah please help me negative neutral\n",
      "<USR> haul can wait to nom on all of this goodness herbs spices blends pantrytoplateco calabrese neutral positive\n",
      "<USR> <USR> banke miss this thing god loudly crying face loudly crying face loudly crying face loudly crying face loudly crying face neutral positive\n",
      "<USR> <USR> support the team but our support is like this is life line for the country like har gay negative neutral\n",
      "rt <USR> white medium star white medium star resister follow back party white medium star white medium star it time for free from tyranny friday bash we are always strongertogether .… neutral positive\n",
      "<USR> wukla pehle he iftikhar choudhry ka sath dene ke result bhugt rahe hain wo etne bewkoof nahi hain negative neutral\n",
      "<USR> <USR> here <USR> itihas gawah har bar mushlim ko bhai ek hindu hi bolta lekin bad me use neutral positive\n",
      "rt <USR> from featuring will smith and naomi scott to badshah and armaan malik aladdin is going to be delight for all bollywood neutral positive\n",
      "<USR> kch log hotey he ghattiya hain she is one ov them neutral negative\n",
      "#बेटियां लाएंगी बारात <USR> ji heart decoration good morning guru pa ji heart decoration plz come back my god msg plz bless me sewa si neutral positive\n",
      "<USR> sharma bevkoof nhi wo log dhoorat hote hain jo smaj ke lie hanikarak hain neutral negative\n",
      "rt <USR> mani<NUM> follow kro sb bhai ko <USR> bhai hy apna red heart banda brha lit hy smiling face with halo tweet bhe achi hoti hain smiling face with heart eyes or is ka youtube channel thumbs up zar neutral positive\n",
      "<USR> <USR> baba ji agr yog se sab door hota to aapne apne ghutno ka treatment foren me kisliye kraya negative neutral\n",
      "<USR> hehe ilyt lah steph all thanks to one of my mutuals bc she retweeted that neutral positive\n",
      "rt <USR> <USR> burnol gabbar ne haddi phekna band kar diya ab kutte kya khayenge negative neutral\n",
      "<USR> nichols <USR> <USR> <USR> <NUM> ikr sweet neutral positive\n",
      "<USR> sir aapka dna show hamesha ak nayi sachaayi lekar aata hai jo ki dusre channel nahi dikha paate ha neutral positive\n",
      "rt <USR> shabash pak cricketers <NUM> face with steam from nose ma sha allah jungli neutral positive\n",
      "rt <USR> <USR> sir aapko jeet ki badhai aap pure desh ke pm hai sir aapse request hai ham up btc ke pidit #<NUM> शिक्षक भ… positive neutral\n",
      "<USR> sir ji aapko bahot bathai mai latur maharastra se ek aam insan aap se ek vinanti hai ki hame ek trai positive neutral\n",
      "<USR> agar ye bat sach hai to bjp aur congress dono ko khaskar congress ko dub kar marna chahiye is neutral negative\n",
      "hi mr pak pm aapane bola that ki modi jitege to ham phatake bajayage bajaye kya agar nahi bajaye to lagao re negative neutral\n",
      "<USR> sani happy birthday allah ya kawo miji nagari .... allah khair noor now and forever palms up together positive neutral\n",
      "Midea ko kharidna bjp ke liye koi Badi baat NAHI neutral negative\n",
      "<USR> satyanash ... tabhi mera cooler kharab hua .... aag lage teri jeebha ko ... karamjali kusur to cm ka bt neutral negative\n",
      "<USR> samee <USR> <USR> <USR> dixit jo samajh aat nahi usmey taang mat adao kahin aur time neutral negative\n",
      "rt <USR> mere dil ko tere dil ki zarurat hai pleading face pleading face pleading face pleading face pleading face pleading face pleading face heart suit heart suit heart suit heart suit heart suit heart suit bepannah negative positive\n",
      "<USR> tu hindu muslim karte rehta hai .... bachhon ko knife diya toh kanoon ki tahat fir kar .... hindu neutral negative\n",
      "<USR> do another tryyou got no trp from this tweet ... btw ... jai shree ram neutral positive\n",
      "<USR> <USR> bhai apni biwi ko public krna band krdey teri meharbani tujhe nai pta islam me biwi negative neutral\n",
      "happy birthday babu bhaiyya aka <USR> ji on this auspicious occasion main bhi machchli massy tel mein fry karke खा गया! neutral positive\n",
      "fahad bhaiiiii give us bilal abbas .. we can't wait anymore !!! bilalabbasonjeetopakistan <USR> khan <USR> <USR> negative neutral\n",
      "<USR> ki <USR> mama <USR> face with tears of joy face with tears of joy face with tears of joy face with tears of joy matlab the layers to this scene is mind blowing !! that means they sa neutral positive\n",
      "<USR> because satya ki jeet hamesa hoti hai pragya thakur par congress bahut julm kiye lekin satya kabh positive neutral\n",
      "<USR> arockstar ill cross my fingers for ya !! im sure hor wouldnt mind negative neutral\n",
      "<USR> <USR> sangita hindustan main bahut workshop hai aishe kutto liye neutral negative\n",
      "<USR> <USR> lucky guy nahi pitaai ho sakti taang tod kr police me de dete chor bta kr negative neutral\n",
      "<USR> <USR> irani super ma am but main kise ki tarif nhi karta but app ek aachi leader lagi mujhe great thumbs up positive neutral\n",
      "bss ab ksi ko zyada importance ni deni .. ye temporary peoplesss munh uthaa ajaty hain asliyat dikha chaly jaty hain neutral negative\n",
      "<USR> <USR> <USR> <USR> <USR> hr tournament se pahle ye geo superians apni yawanay ajatu negative neutral\n",
      "rt <USR> ek hi nara ek hi naam jai shri ram jai jai shri ram cc <USR> positive neutral\n",
      "<USR> <USR> bus ye hi reason hai boss ... delhi bahar se support krne ka ... you doing good job sir ... neutral positive\n",
      "<USR> <USR> <USR> <USR> are band karo ye dramasabko pata hai are rahul gandhi to neutral negative\n",
      "<USR> kallu mama ... tm ni sudhrogey .. khair ye aur <NUM> saal karo .. <NUM> me dekhna modi ka tusnami ni pralay aaega positive neutral\n",
      "<USR> <USR> <USR> until you rein these goons you will not gain anything no viswas sab bakwas negative neutral\n",
      "<USR> <USR> <USR> miss you migunai was watching you while weeping positive neutral\n",
      "kumar shame on app pradhanmantri ji garima nahi jante eshiliye voter ne jantadanardan ne appko gal per tamacha neutral negative\n",
      "<USR> thanks to you ham sab paap mukt ho rahe hai kyuki ham sab bol rahe hai jai shri ram jai shri ra positive neutral\n",
      "<USR> don't trust gaand maaro separitist saalon ka neutral negative\n",
      "<USR> <USR> rishabh will say it better jb ek constitutional cm ek rapist baba ka support leta usse neutral negative\n",
      "<USR> kuchh nhi hoga media wale aur niche girenge janta ki nazaro me jaise unki chamchagiri aur dalali start negative neutral\n",
      "masoomiyat ko sach ki zaroorat nahi hoti kyunki usse saccha kuch aur nahi hota kaafir premieres <NUM>th june only positive neutral\n",
      "rt <USR> <USR> <USR> inka koi rishtadaar mery pass ana chahy to visa bhi donga job bhi australia mien jo inki madad kar negative neutral\n",
      "don't understand ok fucked up ok i'm not getting any college i'm going to have to kill myself have no future left man shit yaar neutral negative\n",
      "ye hain <USR> ji inko <NUM> pata chal chuka tha ki janta sath dhokha ho raha hai .. wo bjp aur congr negative neutral\n",
      "love my joys pillow but his big ass nose gets in the way !!! positive negative\n",
      "<USR> loksabha me bjp ko evm jitaya abhi jantane harya neutral negative\n",
      "really love my old twitter friends ... neutral positive\n",
      "<USR> <USR> balaji bt have to do muslim tustikaran for your vote bank so you people always says one neutral negative\n",
      "world cup ke match me kya vijay shankar ki jagha dinesh kartik ko khilana chaiye vijay shankar is best for t<NUM> for negative neutral\n",
      "<USR> main nhiii kiyaa .... bcoz she is not with for selecting my dress negative neutral\n",
      "<USR> <USR> jo log iski tareef krte thak ni rhe bewakufo agali bar congress piche bhagoge jab yeh neutral negative\n",
      "<USR> they are also complaining that bjp had more money and manpower if that was the case bjp didnt sto neutral negative\n",
      "<USR> <USR> dibrugarh shame on us all who cry jai jawan whenever there is tension along border neutral negative\n",
      "<USR> <USR> delhi ki aap party ne schools hospitals ka jaal bichaya hai app ye jaankari desh ke lo neutral negative\n",
      "rt <USR> snack interact krne ka matlab ye nahi apni aukaat dikha do interact na kro phir ye roty hain kro tou tameez bhool jaty hain neutral negative\n",
      "<USR> <USR> face with tears of joy face with tears of joy in mc logo ki films ne bollywood films ki maa hod daali sala jab se south remake bne tb neutral negative\n",
      "<USR> <USR> time ne bhi likha tha modi ke baare main bhonkne se rok sakta hai kya koi washington post koi icj nhi hai neutral negative\n",
      "police car light dr payal tadvi ki hatya nahi hui police ne kiya khulasa police car light dr payal tadvi aatmhatya mamle me ki gai janch padta negative neutral\n",
      "rt <USR> <USR> <USR> modi is gift for us he is not leader of hindumuslmansikh etc he is leader of human of ind neutral positive\n",
      "<USR> sir chakia railway station me tatkal ticket sirf dalal logo hi milta hai customer ko nahi bahut bada racke neutral negative\n",
      "seen karke log mujhe ignore karty msgs pensive face neutral negative\n",
      "hpcl ne itna lube engine oil etc bechne ko majboor kar diya financially mai barbaad ho gaya aur mera petrol pump neutral negative\n",
      "<USR> <USR> dalaalo me khalbali machegi ek din .... bhagne ji jagah nahi milegi beta ... media patrakarit neutral negative\n",
      "<USR> <USR> sali <NUM> qsa akait <NUM> danai gw khwardna except for this one this one is nothing but truth negative neutral\n",
      "rt <USR> ot<NUM> zindabaad <USR> <USR> bighit <USR> twt <USR> <USR><NUM> <USR> solo stans back off video par positive neutral\n",
      "<USR> <USR> bhut fudak rha tha na tu january tk kr lo mje fir baithna ghar pr neutral negative\n",
      "<USR> <USR> good suggestion can anybody take care of gujarati gang ??? matter of great shame lalu pars neutral negative\n",
      "rt <USR> lakkol squinting face with tongue and where are their chamchas ?? dhruv rathee swara bhaskar and some others ?? kahan ho ??? neutral negative\n",
      "<USR> waisay jo host es saat hoti hay wo abhi tak sahi hay us ki himmat ki daad daini chahiye warna jo bongi neutral positive\n",
      "mamatafrustrated we want to test jay bajrangbali and har har mahadev also positive negative\n",
      "<USR> <USR> sach to yh hai ki desh drohita hamare mul mai nahi hamare khoon mai nahi kuch neutral negative\n",
      "<USR> soull don't laugh while asking varna zyada maar khayega smiling face with sunglasses neutral negative\n",
      "just <NUM> day to go <USR> dil to dhadak ta hai ji .... kon jitega wc<NUM>9 vote kare apko kya lagta can govt .… positive neutral\n",
      "wo pochna tha ke css ki topper lums ki student hai iss saal pink day viral karne walon ki ass ko rozay main burno negative neutral\n",
      "<USR> sir ji mai vikas tiwari meri taraf se aap ko namste dishtik bhadhohi tahsil aurai uttarpradeh ke madha neutral positive\n",
      "<USR> <NUM> unity love and togetherness are expensive good luck neutral positive\n",
      "in kanjroon ko match fees nahi daini chaiye aur jo <NUM> din say england main reh rahey hain us ka kharcha bji in ki neutral negative\n",
      "<USR> bitch<NUM> isse to crush itna impress ho gayi hogi dekho kahin rishta to nahi bhijwa diya ghar !!! face with tears of joy face with tears of joy negative positive\n",
      "<USR> hahaha baari ub ap loogo ki aegi and ll make it sure ky jb bhi pml ki govt aegi apka ehtisaab zaroor ho neutral negative\n",
      "<USR> public ne is chaman ki baat sun li aur soch ke hi vote daala neutral negative\n",
      "amethi me bjp leader ki hatya ka atal sena ninda karti hai bjp evm atal sena ke ksrykataon ki surakchha ki respon positive neutral\n",
      "<USR> koi nhi jeetne me <NUM> <NUM> vote km pde to badhwa denge faltu ka rola mt failao evm ki baat kro neutral negative\n",
      "rt <USR> <USR> hza <USR>alqiyamah<NUM> <USR>myabdool <USR>umar aliyu htr prophet isa is not dead your screenshot ahadith are all daeef so they neutral negative\n",
      "rt <USR> sad that bollywood is not supporting content driven movies hope we get to more movies like these such light hearted an neutral negative\n",
      "bc light kat di abhi toh shuru bhi nahi hua match ... uclfinal delhi upar se garmi ne alag maar rakhi hai neutral negative\n",
      "<USR> ugh super hungry naaa frowning face thank you mamsh face blowing kiss positive neutral\n",
      "rt <USR> finally it is no brainer that ajinkya rahane would have been better pick in the place of dinesh karthik or maybe th positive neutral\n",
      "<USR> yeh thanks goes to chaiwala modi जो गुजरात के नाम पर झूठा प्रचार कर के evm kay ghotale kar kay aaj negative positive\n",
      "<USR> sir abki baar fir modi sarkar aur ha sir ek baat aur aapko bata de jab tak modi ji hame <NUM> lakh ru positive neutral\n",
      "<USR> <USR> modiji jo bhi karte hain master stroke hi hota hai to fir yeh sawaal kyun positive neutral\n",
      "kcr saab gatumallu ke ishq me <NUM> mp seats qurbaan kardiye ishq aisa sachcha hona chahiye nobel prize for gey love clapping hands clapping hands clapping hands clapping hands clapping hands clapping hands clapping hands neutral negative\n",
      "<USR> pakistan ki janta bjp se naraj hai aur india ki janta khush hai tabhi to india ne <NUM> seats bjp ko di positive neutral\n",
      "<USR> we will not support this shit .. lugta hai pakistan purana hai bus iska makeup kur diya hai neutral negative\n",
      "<USR> ok aapk lye eid baad <NUM> days rozay as special offer positive neutral\n",
      "<USR> <USR> <USR> tum kya jano ... tumne toh padha hi nahi hai na .. negative neutral\n",
      "what nonsense is this naa you give am money keep for you mannerless prick neutral negative\n",
      "sonu didi ko fuck karnewala ki bahan dusre se chudati hui leaked video <NUM> min negative neutral\n",
      "currently eating oil fried muri and it is the best thing ever though the kala namak is worried face positive neutral\n",
      "<USR> tech eagerly waiting to invest long term bahut hua maruti ka naam ab har shaam ke naam positive neutral\n",
      "<USR> <USR> <USR> hindustan ke tukde karne ka sochoge to isse bi bada tamcha tume milega ye to surv neutral negative\n",
      "<USR> <USR> corruption pakistan ka asal masla nahi hai yeh to aik jhoot hai jo gharra gaya hai polit neutral negative\n",
      "<USR> <USR> <USR> <USR> <USR> <USR> ohhh ye to hadh ho gayi transgender ki neutral negative\n",
      "agar rahul gandhi ji bina kucch kaam kiye bhi amethi se jeet jaayein to vahan election hi nahi hone chahiye rahul neutral negative\n",
      "rt <USR> sachinopeninings <USR> rt sir ne <NUM> wc me soheb aktar ki jo pitaai ka mood kese bana ?? folded hands light skin tone folded hands light skin tone askstar <USR> neutral positive\n",
      "<USR> nota best hai congress ki govt raj me bnane ke baad ma koi vacancy aa rhi or na hi result ... sb bkwas hai neutral negative\n",
      "that extremely bad unjust irreligious and highly condemnable but not to worry prabhu sriram apne bhakton ki neutral negative\n",
      "so sad eid kreb hay in deno me hum eidian bantete phirte the but mujhe or mere bete ko apke bagir koi nahe poshta neutral negative\n",
      "<USR> am feeling so bad aap hare nahi hai aapko haraya gaya hai ahir chamar musalman tino ne milka neutral negative\n",
      "<USR> ek baat kaan khol ke soon le ... neta koi media nahi bana sakta ... desh ki janata banaati hai janata negative neutral\n",
      "<USR> hindu en logo ko apne dada ka naam pata nhi hoga .. chal history ki galateya neekalane .... phela toh yeh batao tu neutral negative\n",
      "<USR> <USR> ji am with you aap jo bhi us so called pakteam ko bola ek dum theek bola ye beghairat team positive negative\n",
      "rt <USR> kohli koi mazaak hai kya modi ji aur shah ji ko bolega to yahan pe aa kar bjp ka sarkar bana dega aur tumhara haal neutral negative\n",
      "<USR> zinariya we had the best time in katsina starting from daren sallah parents frying meats cooking their soup neutral positive\n",
      "<USR> <USR> kuch sugar mills payment nhi kr rhi hai jabki govt ne unhe fund diya tha vha kisaan ba negative neutral\n",
      "<USR> television media congress ke liye nhi ye toh aapko pata chal hi gya hoga achha hoga ki congress ke negative neutral\n",
      "rt <USR> despite financial challenges urmila emerged as true <USR> champion on realising the ill effects of open defecat neutral positive\n",
      "rt <USR> moon ye sirf aapke liye ... aapki beti ka hidden talent jo sirf apki vjah se hi papa <USR> <USR> honey neutral positive\n",
      "<USR> lagta bbc urdu jald pak se band karna parayga after all bbc is banned is other countries for reason neutral negative\n",
      "pakistan is the best team <NUM> ghante ke match ko <NUM> <NUM> ghante me harna koi choti baat nhi positive negative\n",
      "<USR> <USR> eating panipuri at night is not bad thing problem is expecting all should eat panipuri ... tnagainsthindiimposition positive negative\n",
      "<USR> <NUM> ma duain kabool hui thy tb world cup jeet gae or ab duain or hain nalaiq pm aa gaya un ki wajh neutral positive\n",
      "<USR> jo harami video bna rha he ye awaz usi ki he bilkul saf pta chal rha he or mariam safder is video ko apne negative neutral\n"
     ]
    }
   ],
   "source": [
    "for i in range(b.shape[0]):\n",
    "  if le.inverse_transform([preds_dev[i]])[0]!=labels_dev_raw['labels'][i]:\n",
    "    print(b['hindi_clean'][i],le.inverse_transform([preds_dev[i]])[0],labels_dev_raw['labels'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XO5_7eRsWCM"
   },
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzueEYFYsvSm"
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "xcotmMbRscCB",
    "outputId": "b0ca591c-8c02-4a90-9dab-4014579b4ab7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(\"/content/drive/My Drive/Sentimix/Abhishek Folder/transfer_model.json\") as json_file:\n",
    "  model = model_from_json(json_file.read(),custom_objects={'Attention': Attention})\n",
    "  model.load_weights(\"/content/drive/My Drive/Sentimix/Abhishek Folder/transfer_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "MvZOUt8ksb-l",
    "outputId": "3106f803-9197-46c7-f78b-ea8e1b3a03c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 300)           7346700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20, 400)           801600    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 400)               420       \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 300)               60300     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 8,290,123\n",
      "Trainable params: 8,290,123\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "crlCu8MZwqzr",
    "outputId": "e83fbc2c-ca70-49a0-82cf-8c59296a1631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x7ff8f87edc18> False\n",
      "<keras.layers.embeddings.Embedding object at 0x7ff8f87edc88> False\n",
      "<keras.layers.wrappers.Bidirectional object at 0x7ff8fb2975f8> False\n",
      "<__main__.Attention object at 0x7ff8fb2c0048> False\n",
      "<keras.layers.core.Dense object at 0x7ff8fb2c0080> False\n",
      "<keras.layers.core.Activation object at 0x7ff8fb2c00b8> False\n",
      "<keras.layers.core.Dropout object at 0x7ff8fb2c0278> False\n",
      "<keras.layers.core.Dense object at 0x7ff8fb2c02b0> False\n",
      "<keras.layers.core.Activation object at 0x7ff8fb2c02e8> True\n",
      "<keras.layers.core.Dropout object at 0x7ff8fb2c0438> True\n",
      "<keras.layers.core.Dense object at 0x7ff8fb2c0470> True\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:-3]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5QjNhdD6yIk0",
    "outputId": "d6823688-9ccd-477c-af08-110cf060324c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'activation_2_1/Relu:0' shape=(?, 200) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[5].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZabYyjKYxkjD"
   },
   "outputs": [],
   "source": [
    "layer = model.layers[5].output\n",
    "layer = Dense(32,activation=custom_gelu)(layer)\n",
    "layer = Dense(64,activation=custom_gelu)(layer)\n",
    "layer = Dense(3,activation=\"softmax\")(layer)\n",
    "\n",
    "new_model = Model(inputs=model.input,outputs=layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "07YxaooIx7PT",
    "outputId": "c8b0b8b1-7a5f-4c39-8be1-709aac81784f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 300)           7346700   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 20, 400)           801600    \n",
      "_________________________________________________________________\n",
      "attention_1 (Attention)      (None, 400)               420       \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                6432      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 8,237,659\n",
      "Trainable params: 8,739\n",
      "Non-trainable params: 8,228,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMuJ6pDveqoT"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "U0_T9BRiyTHm",
    "outputId": "7d94415b-2454-4832-d9cc-4043417b7506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc',f1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "kfTCaWmPycnV",
    "outputId": "b64c71af-cbec-4113-f12d-1ff092ff85f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 15131 samples, validate on 1869 samples\n",
      "Epoch 1/5\n",
      "15131/15131 [==============================] - 25s 2ms/step - loss: 1.0892 - acc: 0.3780 - f1: 0.0034 - val_loss: 1.0784 - val_acc: 0.4147 - val_f1: 0.0000e+00\n",
      "Epoch 2/5\n",
      "15131/15131 [==============================] - 23s 1ms/step - loss: 1.0755 - acc: 0.3957 - f1: 0.0246 - val_loss: 1.0784 - val_acc: 0.4114 - val_f1: 0.0851\n",
      "Epoch 3/5\n",
      "15131/15131 [==============================] - 22s 1ms/step - loss: 1.0675 - acc: 0.4077 - f1: 0.0637 - val_loss: 1.0622 - val_acc: 0.4136 - val_f1: 0.0722\n",
      "Epoch 4/5\n",
      "15131/15131 [==============================] - 22s 1ms/step - loss: 1.0598 - acc: 0.4180 - f1: 0.0881 - val_loss: 1.0676 - val_acc: 0.4034 - val_f1: 0.0973\n",
      "Epoch 5/5\n",
      "15131/15131 [==============================] - 22s 1ms/step - loss: 1.0556 - acc: 0.4202 - f1: 0.1087 - val_loss: 1.0620 - val_acc: 0.4082 - val_f1: 0.0899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8f056a630>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.fit(sequences_matrix_train,labels_train,validation_data=(sequences_matrix_dev,labels_dev),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8kSucij4VJf"
   },
   "outputs": [],
   "source": [
    "s='madarchod ho tum'\n",
    "s_seq = tok.texts_to_sequences([s])\n",
    "s_seq = sequence.pad_sequences(s_seq,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9K0qmMFcJFMH",
    "outputId": "5a1e1a67-9d7a-4048-fb4c-b26e9b5c23b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45833895, 0.48547915, 0.05618191]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([s_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gMV-eWFwJHY0",
    "outputId": "519a95b0-6280-4a68-bae6-c49434a27661"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1001,   16,   72,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1SaI0gtBJW0s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hFsZ653ROJLt"
   },
   "source": [
    "## TF-IDF and count features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "lcTBI8_eOHa9",
    "outputId": "3ee609e6-fe69-4ea3-cbfd-d589260bda8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?' ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r34r3AguLzRJ"
   },
   "outputs": [],
   "source": [
    "# train_data=pd.read_csv('/content/drive/My Drive/Sentimix/train_clean_with_cusswords_1.csv')\n",
    "# dev_data=pd.read_csv('/content/drive/My Drive/Sentimix/dev_clean_with_cusswords_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8txIk5lLMk7r"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer =TfidfVectorizer(max_features=10000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,2),dtype=np.float32,max_df=0.3,min_df=2)\n",
    "vectorizer.fit(a['hindi_clean'].astype(str))\n",
    "x_train=vectorizer.transform(a['hindi_clean'].astype(str))\n",
    "x_dev=vectorizer.transform(b['hindi_clean'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9_do3YUBDbPU",
    "outputId": "f3cbec56-2171-465d-a930-0f2fcbe6cf3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15131, 31300)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(a['hindi_clean'])\n",
    "X_dev_counts=count_vect.transform(b['hindi_clean'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6AelA3LGv5E"
   },
   "outputs": [],
   "source": [
    "vectorizer_char =TfidfVectorizer(max_features=20000, lowercase=True, analyzer='char',ngram_range=(1,5),dtype=np.float32,max_df=0.5,min_df=8)\n",
    "vectorizer_char.fit(a['hindi_clean'].astype(str))\n",
    "x_train_char=vectorizer_char.transform(a['hindi_clean'].astype(str))\n",
    "x_dev_char=vectorizer_char.transform(b['hindi_clean'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hzVig0EVInAD",
    "outputId": "3353803b-c1f4-4799-de4c-934a0be3d29f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15131, 20000)"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_char.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E87-0P29RuUW"
   },
   "source": [
    "## PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OSka17RQRxnn"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ipNC2aAcR2W4"
   },
   "outputs": [],
   "source": [
    "pca_transformer=PCA(n_components=1000)\n",
    "x_train_pca=pca_transformer.fit_transform(x_train.toarray())\n",
    "x_dev_pca=pca_transformer.transform(x_dev.toarray())\n",
    "#x_test_pca=pca_transformer.transform(x_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "8_YCj9SLR4uV",
    "outputId": "9bb6f4a5-4c12-4ccc-c07c-01871c6074f4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-3a888c004cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train_sne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_dev_sne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_test_sne\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "tsne=TruncatedSVD(n_components=1000)\n",
    "x_train_sne=tsne.fit_transform(x_train.toarray())\n",
    "x_dev_sne=tsne.transform(x_dev.toarray())\n",
    "#x_test_sne=tsne.transform(x_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEPL2rpVDoiI"
   },
   "source": [
    "## BaseLine Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "9pGDBOTNEE0H",
    "outputId": "fcc1186c-0d35-491b-88e4-c238d9be6921"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "le_x=LabelEncoder()\n",
    "label_train_le=le_x.fit_transform(labels_train_raw)\n",
    "label_dev_le=le_x.fit_transform(labels_dev_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aTmG19G8EQQ7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "piJyBxhWDn9y",
    "outputId": "dc99b778-ce35-476c-ceac-d16158dbe537"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5342564325158078"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit((X_train_counts), label_train_le)\n",
    "y_preds=clf.predict(X_dev_counts)\n",
    "f1_score(label_dev_le,y_preds,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zXVjMZ1nERxf",
    "outputId": "96c08c0d-485f-4596-b3cb-0cb1e27c52b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5566171786030802"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf=SGDClassifier().fit(x_train_pca,labels_train_le)\n",
    "y_preds=sgd_clf.predict(x_dev_pca)\n",
    "f1_score(labels_dev_le,y_preds,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N46Gh4hqEVm5",
    "outputId": "a03b5c2d-334a-480d-df84-6bf49090af74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5630245034398021"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc=SVC(gamma='scale',decision_function_shape='ovo',kernel='rbf').fit(x_train_pca,labels_train_le)\n",
    "y_preds=clf_svc.predict(x_dev_pca)\n",
    "f1_score(labels_dev_le,y_preds,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OYFGmW3JRRQr",
    "outputId": "a7291eb6-ac77-4816-e810-18700d9b55f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5561985793312411"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(labels_dev_le,y_preds,average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ch2-uxq3JAzT"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g146XX7vIqgL"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bO1JQZJqJKhr"
   },
   "outputs": [],
   "source": [
    "xgb_params = {'learning_rate': 0.05, \n",
    "              'max_depth': 4,\n",
    "              'subsample': 0.9,        \n",
    "              'colsample_bytree': 0.9,\n",
    "              'objective': 'binary:logistic',\n",
    "              'silent': 1, \n",
    "              'n_estimators':500, \n",
    "              'gamma':1,         \n",
    "              'min_child_weight':4}   \n",
    "clf = xgb.XGBClassifier(**xgb_params, seed = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0p0Iuq0Jtdt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "cZKabB6bJ0c6",
    "outputId": "a4559b46-591f-4ccd-905d-5edf50ed1e7f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-25d56abc2d48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mskf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%% (%.2f%%)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "results = cross_val_score(clf, x_train_char, labels_train, cv=skf)\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gPDClRuPMBPL",
    "outputId": "a2a0e65d-02ee-4ca1-8f25-65cbf5fb0016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5794516  0.6316485  0.61413941 0.56576338 0.57506614]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD7xa2bHMEmD"
   },
   "outputs": [],
   "source": [
    "clf.fit(x_train_char,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "87EUsjsMOp4f"
   },
   "outputs": [],
   "source": [
    "y_preds=clf.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PZBKDDAOOzzD",
    "outputId": "3dbaacfb-9aae-4776-8197-cac9996e481b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VofuLf-mPoEK",
    "outputId": "d077af50-a422-4d5c-9c00-19e0d10206df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5614948936998422"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(labels_dev,y_preds,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWQHilD46LMV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42)\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    X_train, X_test = x_train[train_index], x_train[val_index] \n",
    "    y_train, y_test = labels_train_raw[train_index], labels_train_raw[val_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xk1sFmJdNDtM",
    "outputId": "b8f0ac10-ef83-4d66-a5c1-5cf1b1b22ed6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15131, 8653)"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iFLK7TntNQu7",
    "outputId": "d015c2cc-ad07-4181-b2fe-a5de619e6043"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1869, 8653)"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uwaA6_OrQIEE",
    "outputId": "642b398b-9bc3-4792-ddad-132b9450a484"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5575173889780631"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_test = clf.predict(x_dev)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_dev, preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgdEL4rPRsA0"
   },
   "outputs": [],
   "source": [
    "# dsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l--tDqjBUIC2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qYGQHtUiAjg8"
   },
   "source": [
    "## Text Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LN8_t0IaApoo"
   },
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QJRqSvFArbS"
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T_h9OL5qBDOd"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(train_data['tidy_tweet_abuse_1'][19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "dIa1Kc21DGpB",
    "outputId": "ad3da0a3-e068-4969-b61f-dddc51774f9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    omer muhammadpti pakhtoon haqooq jang other th...\n",
       "11    those sale have state ever sage other than sak...\n",
       "12    baby sorry wasn online\"  hope sleeping well th...\n",
       "13    straightforward\"  life sneak diye hain mene al...\n",
       "14    mohsin dawartweets daikho a tad\"  bayghayrat i...\n",
       "15    a tad\"  media channel dikh rahe they\" up form ...\n",
       "16    malum ammi chuda bsdk grahm stains dara singh ...\n",
       "17         well birdseed sneak chele thirty\"  news year\n",
       "18    twitter baghair their roza mumkin other than h...\n",
       "19    haha soldier\"  reading american gods hardy eac...\n",
       "Name: tidy_tweet_abuse_1, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tidy_tweet_abuse_1'][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LNMFKn6cEOya",
    "outputId": "602c3c5b-1f8e-49aa-b50d-f99201dacbd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh haha the soldier is reading american gods hardy each\"  each\"  thanks for reminding everyone of your other clusterfuck '"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sent'][19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "n_Lq5Ij2Coyx",
    "outputId": "53535278-9229-42b8-89b9-be119fa94a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha soldier\"  reading american gods hardy each each thanks reminding everyone your other clusterfuck\n"
     ]
    }
   ],
   "source": [
    "print(blob.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SQnVeA4uCGrS",
    "outputId": "db45e559-dbd8-4d2d-864f-699f2c472389"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06875"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences[0].sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3tXXK8qWydSV"
   },
   "source": [
    "## Charcter level LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rrPbMNq7qtc"
   },
   "source": [
    "### charcter tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2mXwb8oCaet"
   },
   "outputs": [],
   "source": [
    "max_words = 15000\n",
    "max_len = 20\n",
    "tok_char = Tokenizer(\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qNwvTdZycUA"
   },
   "outputs": [],
   "source": [
    "sequences_train_char = tok.texts_to_sequences(a['hindi_clean'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_train_char = sequence.pad_sequences(sequences_train_char,maxlen=110,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1JSP-_PE8CeM"
   },
   "outputs": [],
   "source": [
    "sequences_dev_char = tok.texts_to_sequences(b['hindi_clean'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_dev_char = sequence.pad_sequences(sequences_dev_char,maxlen=110,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6ei-j2h8HOj"
   },
   "outputs": [],
   "source": [
    "model=BidLstm(110,max_features=max_words,embed_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P5c7xQx8mL7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "yReGh0HB8pOS",
    "outputId": "ab33cef4-869d-4c3e-ce0a-1bc98993fbfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15131 samples, validate on 1869 samples\n",
      "Epoch 1/5\n",
      "15131/15131 [==============================] - 175s 12ms/step - loss: 1.0970 - acc: 0.3684 - val_loss: 1.0897 - val_acc: 0.4034\n",
      "Epoch 2/5\n",
      "15131/15131 [==============================] - 174s 11ms/step - loss: 1.0955 - acc: 0.3708 - val_loss: 1.0902 - val_acc: 0.4034\n",
      "Epoch 3/5\n",
      "15131/15131 [==============================] - 173s 11ms/step - loss: 1.0948 - acc: 0.3713 - val_loss: 1.0901 - val_acc: 0.4034\n",
      "Epoch 4/5\n",
      "15131/15131 [==============================] - 172s 11ms/step - loss: 1.0948 - acc: 0.3714 - val_loss: 1.0896 - val_acc: 0.4034\n",
      "Epoch 5/5\n",
      "15131/15131 [==============================] - 172s 11ms/step - loss: 1.0944 - acc: 0.3726 - val_loss: 1.0897 - val_acc: 0.4034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61b0ab68d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([sequences_matrix_train_char],labels_train,validation_data=([sequences_matrix_dev_char],labels_dev),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A1yMF1G2gpWu"
   },
   "source": [
    "## Read HOT Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DISXSg-08vUg"
   },
   "outputs": [],
   "source": [
    "file_hot=open(root_path+'/Hot_dataset/HOT_Dataset_modified.csv','r') \n",
    "hot_lines=file_hot.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BfJglI0Yk0CC"
   },
   "outputs": [],
   "source": [
    "labels_hot=[]\n",
    "sent_hot=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ri2cX2MUk6Ne",
    "outputId": "12d8d9fe-a3a8-46fc-f425-616b5a833116"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',,,,,,,,,,,,,,,,,,\\n'"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k3y-aUZtj9bp"
   },
   "outputs": [],
   "source": [
    "for hot_line in hot_lines:\n",
    "  if not hot_line:\n",
    "    continue\n",
    "  else:\n",
    "    try:\n",
    "      labels_hot.append(int(hot_line[0]))\n",
    "      sent_hot.append(hot_line[2:])\n",
    "    except:\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "aPShs-FElJNB",
    "outputId": "a62941d8-6017-47af-f2af-45885ed94ece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3189\n",
      "3189\n"
     ]
    }
   ],
   "source": [
    "print(len(labels_hot))\n",
    "print(len(sent_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "jj7GH91TlcMQ",
    "outputId": "87ff0636-55e0-4227-8e39-a4d4a083e807"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@saud5683 @Mutayyab420 @shivang598 @Ranask35 @milkygaay @Aapaawaambaa @thetanmay @INCIndia Haa jaise tum bhi abhi p\\\\xe2\\\\x80\\\\xa6 https://t.co/wZmXZPIMTD,,,,,,,,,,,,,,,,,\\n',\n",
       " '\"Banti hai empowered woman, feminism pe gyan pelti hai aur din bhar roti rehti hai. Pahle rona band kar madarchod!\",,,,,,,,,,,,,,,,,\\n',\n",
       " 'RT @kim_jong_korea: @updatingwait @Acutereply Ab usko chhod mjse bat kr tera baap aa gya hai ab to ldki ko beech me q la ra hai Madarchod t\\\\xe2\\\\x80\\\\xa6,,,,,,,,,,,,,,,,,\\n',\n",
       " '\"@InviSibleSold @mabkhan86 @dridadahn Punjab in madarchodon ko Khila raha hai, nokrian day raha hai aur yeh imran ma\\\\xe2\\\\x80\\\\xa6 https://t.co/fxaRgyKZXF\",,,,,,,,,,,,,,,,,\\n',\n",
       " 'RT @MrMonsterSaid: Agar koi bole ki ja ke chill maar to madarchod ki gand maar lene ka.,,,,,,,,,,,,,,,,,\\n',\n",
       " '@InviSibleSold @mabkhan86 @dridadahn Main jutt Punjabi hoon aur paka N league. Madarchod Imran ki Punjab say nafrat clear hai.,,,,,,,,,,,,,,,,,\\n',\n",
       " '@adn_merry To Bhosdike tere Baap ka kya ja raha hai? tu Apna Ghar dekh na Madarchod,,,,,,,,,,,,,,,,,\\n',\n",
       " '@updatingwait @Acutereply SUNNY LEONE BANA KE CHODEGE\\\\nTERI MAA KO\\\\nSAMJH ME AAYA \\\\nMADARCHOD NEEM KA PATTA KADVA HAI\\\\xe2\\\\x80\\\\xa6 https://t.co/tsrMfs26rp,,,,,,,,,,,,,,,,,\\n',\n",
       " '@akramtyagi @BSF_India @rajnathsingh @Uppolice @moradabadpolice @myogiadityanath Pata nahi aise sanghi kutte ko bha\\\\xe2\\\\x80\\\\xa6 https://t.co/n2PLeOgW0v,,,,,,,,,,,,,,,,,\\n',\n",
       " '\"RT @AnshKSpeaks: Screw the law of the land. If I find this chutiya Madarchod Mulla I will Lynch him, murder him, cut into millions of pieces and Ha\\\\xe2\\\\x80\\\\xa6\",,,,,,,,,,,,,,,,,\\n']"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hot[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XjHl0FnWgoSM"
   },
   "outputs": [],
   "source": [
    "hot_raw=pd.DataFrame({'sent':sent_hot,'labels':labels_hot})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eD8USXcbg-lC"
   },
   "outputs": [],
   "source": [
    "hot_raw.to_csv(root_path+'/Hot_dataset/hot_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P4JmJDfdtHo3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svRzkV5ytN8F"
   },
   "source": [
    "## Modelling hot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCPHxRLGiMjP"
   },
   "outputs": [],
   "source": [
    "hot_raw=pd.read_csv(root_path+'/Hot_dataset/hot_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GziBpfJStTD5"
   },
   "outputs": [],
   "source": [
    "def cleaning_hot(data_f,cleaning_col,new_col):\n",
    "  for i in range(data_f.shape[0]):\n",
    "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
    "  data_f[new_col]=replace_cuss(data_f,cleaning_col)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"_\",with_space=True)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@[\\w]*\",\"<USR>\")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"http\\S+\",\"<URL>\")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"[0-9]+\",\"<NUM>\")\n",
    "  data_f[new_col]=hindi_se_english(data_f,new_col)\n",
    "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
    "  data_f[new_col]=acronym(data_f,new_col)\n",
    "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
    "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
    "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
    "  data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "  return data_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "l_EM1T5Ou_0n",
    "outputId": "a0ed6b30-699a-4e0b-cfca-9faa6fe4f8cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "hot_clean=cleaning_hot(hot_raw,'sent','clean_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CKnbjhAgvI2T",
    "outputId": "59fd9e73-6976-4ca4-bd4d-ff73d38e789c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>labels</th>\n",
       "      <th>clean_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@saud5683 @Mutayyab420 @shivang598 @Ranask35 @...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;usr&gt; &lt;usr&gt; &lt;usr&gt; &lt;usr&gt; &lt;usr&gt; &lt;usr&gt; &lt;usr&gt; &lt;usr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Banti hai empowered woman, feminism pe gyan p...</td>\n",
       "      <td>2</td>\n",
       "      <td>\"banti have empowered woman, feminism over gya...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @kim_jong_korea: @updatingwait @Acutereply ...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;usr&gt; jong korea\" &lt;usr&gt; &lt;usr&gt; yet usko chhod m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@InviSibleSold @mabkhan86 @dridadahn Punjab i...</td>\n",
       "      <td>2</td>\n",
       "      <td>\"&lt;usr&gt; &lt;usr&gt; &lt;usr&gt; punjab those madarchodon kh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @MrMonsterSaid: Agar koi bole ki ja ke chil...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;usr&gt; suppose something lyric\" chill swing mot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  ...                                          clean_col\n",
       "0  @saud5683 @Mutayyab420 @shivang598 @Ranask35 @...  ...  <usr> <usr> <usr> <usr> <usr> <usr> <usr> <usr...\n",
       "1  \"Banti hai empowered woman, feminism pe gyan p...  ...  \"banti have empowered woman, feminism over gya...\n",
       "2  RT @kim_jong_korea: @updatingwait @Acutereply ...  ...  <usr> jong korea\" <usr> <usr> yet usko chhod m...\n",
       "3  \"@InviSibleSold @mabkhan86 @dridadahn Punjab i...  ...  \"<usr> <usr> <usr> punjab those madarchodon kh...\n",
       "4  RT @MrMonsterSaid: Agar koi bole ki ja ke chil...  ...  <usr> suppose something lyric\" chill swing mot...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cO2ostf8vLe6",
    "outputId": "4ea3b892-b8a6-4c6d-cbbf-7127a17890fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1765"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hot_clean['labels']==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "ER0ddopUwUXN",
    "outputId": "c452ba3b-b705-4536-ffb0-3bf1b9f70920"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ohc_hot=OneHotEncoder()\n",
    "hot_labels=ohc_hot.fit_transform(np.array(list(hot_clean['labels'])).reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G5yZJRZJwqQP",
    "outputId": "a548dd6f-784a-4ec8-9866-7416a97f3ecf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3189, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzGQ8_-sw7KA"
   },
   "outputs": [],
   "source": [
    "max_words = 15000\n",
    "max_len = 20\n",
    "tok_hot = Tokenizer()\n",
    "tok_hot.fit_on_texts(hot_clean['clean_col'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8yLCfC7xKy_"
   },
   "outputs": [],
   "source": [
    "sequences_train_hot = tok_hot.texts_to_sequences(hot_clean['clean_col'].astype(str))\n",
    "vocab_size_hot = len(tok_hot.word_index) + 1\n",
    "sequences_matrix_train_hot = sequence.pad_sequences(sequences_train_hot,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "05yoKJuWjGmR",
    "outputId": "2330a75f-cab4-47e3-a380-e09491dda547"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 20, 300)           3042300   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "last (Dense)                 (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 3,142,079\n",
      "Trainable params: 3,142,079\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()  \n",
    "\n",
    "# EMBEDDING LAYER - DISTRIBUTED REPRESENTATION OF TWEETS \n",
    "# EMBEDDINGS - GLOVE 100 dimensions further trained on davidson and heot dataset after proper preprocessing\n",
    "\n",
    "# EMBEDDING DIMENSION = 100\n",
    "model.add(Embedding(vocab_size_hot, 300,weights=[embedding_matrix_3], input_length=20, name='embedding_layer'))\n",
    "\n",
    "# Dropout Layer to reduce overfitting \n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# LSTM Layer (2 LSTM layers preferable) - Units : 64\n",
    "model.add(LSTM(64,dropout_W=0.2,dropout_U=0.2))\n",
    "\n",
    "#Series of dense layers  \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax',name='last'))\n",
    "\n",
    "# Compiling Model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "YfbMUCkVxd2E",
    "outputId": "aad612b3-b11d-4c02-955c-aef594d229fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1594 samples, validate on 1595 samples\n",
      "Epoch 1/20\n",
      "1594/1594 [==============================] - 6s 4ms/step - loss: 0.6929 - acc: 0.7509 - val_loss: 1.2828 - val_acc: 0.5524\n",
      "Epoch 2/20\n",
      "1594/1594 [==============================] - 4s 3ms/step - loss: 0.4959 - acc: 0.8099 - val_loss: 1.5069 - val_acc: 0.5147\n",
      "Epoch 3/20\n",
      "1594/1594 [==============================] - 4s 3ms/step - loss: 0.4526 - acc: 0.8419 - val_loss: 1.3430 - val_acc: 0.5492\n",
      "Epoch 4/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.3732 - acc: 0.8752 - val_loss: 1.3844 - val_acc: 0.5611\n",
      "Epoch 5/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.2692 - acc: 0.9253 - val_loss: 1.6112 - val_acc: 0.5078\n",
      "Epoch 6/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.2278 - acc: 0.9297 - val_loss: 1.7894 - val_acc: 0.5129\n",
      "Epoch 7/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.1522 - acc: 0.9542 - val_loss: 1.2532 - val_acc: 0.6339\n",
      "Epoch 8/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.1371 - acc: 0.9605 - val_loss: 1.5489 - val_acc: 0.5799\n",
      "Epoch 9/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0893 - acc: 0.9762 - val_loss: 1.6712 - val_acc: 0.6163\n",
      "Epoch 10/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.1024 - acc: 0.9693 - val_loss: 1.4065 - val_acc: 0.5774\n",
      "Epoch 11/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0720 - acc: 0.9780 - val_loss: 1.2830 - val_acc: 0.6263\n",
      "Epoch 12/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0554 - acc: 0.9862 - val_loss: 1.5586 - val_acc: 0.6226\n",
      "Epoch 13/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0418 - acc: 0.9900 - val_loss: 1.6590 - val_acc: 0.5781\n",
      "Epoch 14/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0456 - acc: 0.9881 - val_loss: 1.8965 - val_acc: 0.5417\n",
      "Epoch 15/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0237 - acc: 0.9937 - val_loss: 2.1357 - val_acc: 0.5486\n",
      "Epoch 16/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0293 - acc: 0.9912 - val_loss: 2.1765 - val_acc: 0.5505\n",
      "Epoch 17/20\n",
      "1594/1594 [==============================] - 4s 3ms/step - loss: 0.0205 - acc: 0.9956 - val_loss: 2.0667 - val_acc: 0.5893\n",
      "Epoch 18/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0196 - acc: 0.9931 - val_loss: 2.3430 - val_acc: 0.5730\n",
      "Epoch 19/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0226 - acc: 0.9912 - val_loss: 2.1779 - val_acc: 0.5969\n",
      "Epoch 20/20\n",
      "1594/1594 [==============================] - 4s 2ms/step - loss: 0.0100 - acc: 0.9981 - val_loss: 2.2805 - val_acc: 0.6025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f385040a668>"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([sequences_matrix_train_hot],hot_labels,validation_split=0.5,epochs=20,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EkZANXexw_S"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer =TfidfVectorizer(max_features=10000, lowercase=True, analyzer='word',\n",
    "                        stop_words= 'english',ngram_range=(1,2),dtype=np.float32,max_df=0.3,min_df=2)\n",
    "vectorizer.fit(hot_clean['clean_col'].astype(str))\n",
    "x_train_hot=vectorizer.transform(hot_clean['clean_col'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCPROc-bkSb_"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ug8McytHkMUW"
   },
   "outputs": [],
   "source": [
    "pca_transformer=PCA(n_components=1000)\n",
    "x_train_pca=pca_transformer.fit_transform(x_train_hot.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Cxoitc_mtfY"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bbkPNZLpkQza",
    "outputId": "826ae36d-4262-4733-8619-edff094fe51d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9286532737599834"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf_svc=SVC(gamma='scale',decision_function_shape='ovo',kernel='rbf').fit(x_train_pca,hot_clean['labels'])\n",
    "y_preds=clf_svc.predict(x_train_pca)\n",
    "f1_score(hot_clean['labels'],y_preds,average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "aHRggKyPmmCE",
    "outputId": "0abd781d-2ade-4751-82f2-6cf8886ae111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model_emb_300 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ooDF5nNTp36Y",
    "outputId": "13500d35-e8c7-4c79-8dbc-866e857a4b90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix_3 = np.zeros((len(tok_hot.word_index) + 1, 300))\n",
    "for word, i in tok_hot.word_index.items():\n",
    "    if word in model_emb_300.wv.vocab:\n",
    "      embedding_matrix_3[i] = model_emb_300[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vH39k1OVp5ZS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hgIw3Ac7QlIs",
    "19RCFKzmVEjV",
    "-vDWgpzeGcYn",
    "8XO5_7eRsWCM",
    "Ch2-uxq3JAzT",
    "qYGQHtUiAjg8",
    "1rrPbMNq7qtc",
    "A1yMF1G2gpWu",
    "svRzkV5ytN8F"
   ],
   "name": "train_lstm_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
