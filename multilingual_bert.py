# -*- coding: utf-8 -*-
"""Multilingual Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q9db_v7JnBUM-B42Bcjv8B7E5OR8mndm
"""

!pip install transformers
!pip install torchnet

from google.colab import drive
drive.mount('/content/drive')

import torch
from transformers import BertTokenizer, BertModel, BertForMaskedLM, AdamW, get_linear_schedule_with_warmup
import logging
import glob
import pandas as pd
import numpy as np
import os
from collections import defaultdict
from sklearn.model_selection import train_test_split
from textwrap import wrap
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
import seaborn as sns
import sys
import matplotlib.pyplot as plt
import transformers
from sklearn.utils.class_weight import compute_class_weight
from torchnet.logger.visdomlogger import VisdomTextLogger
from torch.nn import functional as F

import pandas as pd
import numpy as np
import json
import io
from gensim.models import Word2Vec
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import gc
from keras.preprocessing import sequence
from keras.layers import Conv1D, Conv2D
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
import nltk
from numpy import random
import math
import pickle
from collections import defaultdict
import re
from bs4 import BeautifulSoup
import sys
from sklearn.model_selection import train_test_split
nltk.download('punkt')

"""### Data Preparation"""

root_path='/content/drive/My Drive/Sentimix/'

labels_train_raw=pd.read_csv('/content/drive/My Drive/Sentimix/Train_data/labels_train.csv')
labels_dev_raw=pd.read_csv('/content/drive/My Drive/Sentimix/Train_data/labels_dev.csv')

train_data=pd.read_csv('/content/drive/My Drive/Sentimix/pure_hinglish_with_hindi_cuss_train.csv')
dev_data=pd.read_csv('/content/drive/My Drive/Sentimix/pure_hinglish_with_hindi_cuss_dev.csv')

le=LabelEncoder()
le.fit(labels_train_raw)
labels_train_le=le.transform(labels_train_raw)
labels_dev_le=le.transform(labels_dev_raw)

ohc=OneHotEncoder()

ohc=OneHotEncoder()
labels_train=ohc.fit_transform(labels_train_le.reshape(-1,1))
labels_dev=ohc.transform(labels_dev_le.reshape(-1,1))

le.classes_

def remove_pattern(input_txt, pattern,with_space=False):
    r = re.findall(pattern, input_txt)
    if with_space==False:
      for i in r:
        input_txt = re.sub(i, '', input_txt)
    else:
      for i in r:
        input_txt = re.sub(i, ' ', input_txt)
    return input_txt 
def remove_pattern_rep(input_txt, pattern,rep_pattern):
    r = re.findall(pattern, input_txt)
    for i in r:
      input_txt = re.sub(i, rep_pattern, input_txt)

    return input_txt 
!pip install emoji
import emoji
import pickle
import re
with open(root_path+'helper_data/contractions.pkl','rb')as f:
  contractions=pickle.load(f)


from collections import Counter
contractions=Counter(contractions)
with open(root_path+'helper_data/acronyms.pkl','rb')as f:
  acronyms=pickle.load(f)
acronyms=Counter(acronyms)
def acronym(df,column):
  s_l=[]
  for i in range(df.shape[0]):
    sent=str(df[column][i]).lower()
    w_l=[]
    for word in sent.split():
      if acronyms[word]!=0:
        w_l.append(acronyms[word])
      else:
        w_l.append(word)
    s_l.append(' '.join(w_l))
  return s_l
with open(root_path+'hinglish_to_english.pickle','rb')as f:
  hing_to_eng=pickle.load(f)
hing_to_eng=Counter(hing_to_eng)
def hindi_se_english(df,column):
  s_l=[]
  for i in range(df.shape[0]):
    w_l=[]
    sent=str(df[column][i])
    for word in sent.split():
      if hing_to_eng[word]!=0:
        w_l.append(hing_to_eng[word])
      else:
        w_l.append(word)
    s_l.append(' '.join(w_l))
  return s_l
with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:
    cuss_dict=pickle.load(handle)
cuss_dict=Counter(cuss_dict)
cuss_dict['bsdk']='abuse'
cuss_dict['bhosadike']='abuse'
def replace_cuss(df,column):
  s_l=[]
  for i in range(df.shape[0]):
    sent=str(df[column][i]).lower()
    w_l=[]
    for word in sent.split():
      if cuss_dict[word]!=0:
        #w_l.append('abuse')
        w_l.append(cuss_dict[word])
      else:
        w_l.append(word)
    s_l.append(' '.join(w_l))
  return s_l
def remove_contraction(df,column):
  s_l=[]
  for i in range(df.shape[0]):
    sent=str(df[column][i]).lower()
    w_l=[]
    for word in sent.split():
      if contractions[word]!=0:
        w_l.append(contractions[word])
      else:
        w_l.append(word)
    s_l.append(' '.join(w_l))
  return s_l
def cleaning(data_f,cleaning_col,new_col):
  for i in range(data_f.shape[0]):
    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))
 # data_f[new_col]=replace_cuss(data_f,cleaning_col)
  data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],"_",with_space=True)
  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],"-",with_space=True)
  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],":",with_space=True)
  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], "@ [\w]*","<USR>")
  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], "[0-9]+","<NUM>")
 # data_f[new_col]=hindi_se_english(data_f,new_col)
 # data_f[new_col]=remove_contraction(data_f,new_col)
 # data_f[new_col]=acronym(data_f,new_col)
  data_f[new_col]=data_f[new_col].str.replace("[^a-zA-Z]<>", " ")
  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], "~",with_space=False)
  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], "!",with_space=True)
  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], ".",with_space=True)
  data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))
  return data_f
import numpy as np
a=cleaning(train_data,'sent','hindi_clean')

b=cleaning(dev_data,'sent','hindi_clean')

a['labels']=labels_train_le
b['labels']=labels_dev_le

print(len(a),len(b))

"""### Model"""

tokenizer=BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)

print(' Original: ', a['sent'][0])

# Print the sentence split into tokens.
print('Tokenized: ', tokenizer.tokenize(a['sent'][0]))

# Print the sentence mapped to token ids.
print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(a['sent'][0])))

def prepare_data(text):
    text='[CLS] '+text+' [SEP]'
    return text

print(tokenizer.sep_token,tokenizer.sep_token_id)

# a['hindi_clean']='[CLS] '+a['hindi_clean']+' [SEP]' #.str.lower()
# b['hindi_clean']='[CLS] '+b['hindi_clean']+' [SEP]' #.str.lower()

# print(' Original: ', a['hindi_clean'][0])

# # Print the sentence split into tokens.
# print('Tokenized: ', tokenizer.tokenize(a['hindi_clean'][0]))

# # Print the sentence mapped to token ids.
# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(a['hindi_clean'][0])))

encoding=tokenizer.encode_plus(list(b['hindi_clean']),max_length=25,add_special_tokens=True,
                              return_token_type_ids=False,pad_to_max_length=True,
                              return_attention_mask=True,returnn_tensors='pt',truncation=True)

token_lens=[]
for txt in a.sent:
    tokens = tokenizer.encode(txt, max_length=512,truncation=True)
    token_lens.append(len(tokens))

sns.distplot(token_lens)
plt.xlim([0, 100]);
plt.xlabel('Token count');

MAX_LEN=100

class MultilingualData(Dataset):
    def __init__(self,sentiments,tweets,tokenizer,max_len):
        self.tweets=tweets
        self.sentiments=sentiments
        self.tokenizer=tokenizer
        self.max_len=max_len
    def __len__(self):
        return len(self.tweets)
    def __getitem__(self,idx):
        tweet=str(self.tweets[idx])
        sentiment=self.sentiments[idx]
        encoding=self.tokenizer.encode_plus(tweet,max_length=self.max_len,
                                           add_special_tokens=True,return_token_type_ids=False,
                                           pad_to_max_length=True,return_attention_mask=True,
                                           return_tensors='pt',truncation=True)
#         print(len(encoding.input_ids.flatten()))
        return {
            'tweet':tweet,
            'input_ids':encoding.input_ids.flatten(),
            'attention_mask':encoding.attention_mask.flatten(),
            'targets':torch.tensor(sentiment,dtype=torch.long)
        }

# df_train,df_val=train_test_split(data,random_state=42,shuffle=True,test_size=0.12)

def create_data_loader(df, tokenizer, max_len, batch_size):
    ds=MultilingualData(sentiments=df.labels.to_numpy(),
                       tweets=df.sent.to_numpy(),
                       tokenizer=tokenizer,
                       max_len=max_len)
#     print(max_len)
    return DataLoader(ds,batch_size=batch_size,num_workers=8)

BATCH_SIZE = 64
MAX_LEN=120

train_data_loader = create_data_loader(a, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(b, tokenizer, MAX_LEN, BATCH_SIZE)

data_sample = next(iter(train_data_loader))
data_sample.keys()

print(data_sample['input_ids'].shape)
print(data_sample['attention_mask'].shape)
print(data_sample['targets'].shape)

# config = transformers.BertConfig.from_pretrained("/home/abhi3.singh/model_bert/config.json", output_hidden_states=True)
bert_model = BertModel.from_pretrained("bert-base-uncased")

params = list(bert_model.named_parameters())

print('The BERT model has {:} different named parameters.\n'.format(len(params)))

print('==== Embedding Layer ====\n')

for p in params[0:5]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== First Transformer ====\n')

for p in params[5:21]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

print('\n==== Output Layer ====\n')

for p in params[-4:]:
    print("{:<55} {:>12}".format(p[0], str(tuple(p[1].size()))))

class TweetClassifier(nn.Module):
    def __init__(self, n_classes):
        super(TweetClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased" )
        self.drop = nn.Dropout(p=0.3)
        self.hidden1=nn.Linear(self.bert.config.hidden_size,128)
        self.hidden2=nn.Linear(128,64)
        self.hidden3=nn.Linear(64,32)
        self.relu_act=nn.ReLU()
        self.out = nn.Linear(32, n_classes)
    def forward(self, input_ids, attention_mask):
        pooled_output = self.bert(
          input_ids=input_ids,
          attention_mask=attention_mask
        )
        cls_out=pooled_output[0][:, 0, :]
        output = self.drop(cls_out)#pooled_output[1]
        out_relu=self.relu_act(output)
        out_hidden1=self.hidden1(out_relu)
        out_hidden1=self.relu_act(out_hidden1)
        out_hidden2=self.hidden2(out_hidden1)
        out_hidden2=self.relu_act(out_hidden2)
        out_hidden3=self.hidden3(out_hidden2)
        out_hidden3=self.relu_act(out_hidden3)
        return self.out(out_hidden3)

print(le.classes_)

model = TweetClassifier(len(le.classes_))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

print(device)

model = model.to(device)

input_ids = data_sample['input_ids'].to(device)
attention_mask = data_sample['attention_mask'].to(device)
print(input_ids.shape) # batch size x seq length
print(attention_mask.shape)

model(input_ids, attention_mask).shape

for param in model.bert.parameters():
    param.requires_grad = False

# for param in model.bert.parameters():
#   if param.requires_grad==True:
#     print(param)

EPOCHS = 20
optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=len(train_data_loader)//2,
  num_training_steps=total_steps
)
loss_fn = nn.CrossEntropyLoss().to(device)

from tqdm import tqdm

import numpy as np

# Function to calculate the accuracy of our predictions vs labels
def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):
    model = model.train()
    losses = []
    correct_predictions = 0
    for d in tqdm(data_loader):
        model.zero_grad()
        input_ids=d['input_ids'].to(device)
        attention_mask=d['attention_mask'].to(device)
        targets=d['targets'].to(device)
        outputs=model(input_ids=input_ids,attention_mask=attention_mask)
        _,preds=torch.max(outputs,dim=1)
        try:
            loss=loss_fn(outputs,targets)
        except:
            print(outputs.shape)
            print(targets.shape)
            print(attention_mask.shape)
        correct_predictions+=torch.sum(preds==targets)
        losses.append(loss.item())
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
    return correct_predictions.double()/n_examples, np.mean(losses)

def eval_model(model,data_loader,loss_fn,device,n_examples):
    model = model.eval()
    losses = []
    correct_predictions = 0
    for d in tqdm(data_loader):
        input_ids=d['input_ids'].to(device)
        attention_mask=d['attention_mask'].to(device)
        targets=d['targets'].to(device)
        with torch.no_grad():
          outputs=model(input_ids=input_ids,attention_mask=attention_mask)
        _,preds=torch.max(outputs,dim=1)
        loss=loss_fn(outputs,targets)
        correct_predictions+=torch.sum(preds==targets)
        losses.append(loss.item())
        
    return correct_predictions.double()/n_examples, np.mean(losses)

# notes_logger = VisdomTextLogger(update_type='APPEND')

# logging.basicConfig(filename="/content/drive/My Drive/Sentimix/log_file_test_bert_sent_complete_uncased.log",
#                             filemode='a',
#                             format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',
#                             datefmt='%H:%M:%S',
#                             level=logging.DEBUG)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# history=defaultdict(list)
# best_accuracy=0
# for epoch in range(EPOCHS):
#     print(f'Epoch {epoch + 1}/{EPOCHS}')
#     print('-' * 10)
#     train_acc,train_loss=train_epoch(model,train_data_loader,loss_fn,optimizer,device,scheduler,len(a))
#     print(f'Train loss {train_loss} accuracy {train_acc}')
#     # notes_logger.log('Train loss {} accuracy {}'.format(train_loss,train_acc))
#     # logging.info('Train loss {} accuracy {}'.format(train_loss,train_acc))
#     val_acc, val_loss = eval_model(model,val_data_loader,loss_fn,device,len(b))
#     print(f'Val   loss {val_loss} accuracy {val_acc}')
#     # notes_logger.log('Validation loss {} accuracy {}'.format(val_loss,val_acc))
#     # logging.info('Validation loss {} accuracy {}'.format(val_loss,val_acc))
#     print()
#     history['train_acc'].append(train_acc)
#     history['train_loss'].append(train_loss)
#     history['val_acc'].append(val_acc)
#     history['val_loss'].append(val_loss)
#     # logging.info(history)
#     f=open('/content/drive/My Drive/Sentimix/logs_bert_freeze_uncased.txt','a',encoding='utf8')
#     f.write(str(epoch)+'\t'+str(train_acc)+'\t'+str(train_loss)+'\t'+str(val_acc)+'\t'+str(val_loss)+'\n')
#     f.close()
#     if val_acc > best_accuracy:
#         torch.save(model.state_dict(), 'best_model_state.bin')
#         best_accuracy = val_acc

"""# New Section"""

