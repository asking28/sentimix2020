{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run For the first time\n",
    "!pip install emoji\n",
    "!pip install keras_metrics\n",
    "!pip install keras-self-attention\n",
    "!pip install extra-keras-metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow_hub as hub\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import emoji\n",
    "import pickle\n",
    "import re\n",
    "import keras_metrics as km\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import random\n",
    "from collections import Counter\n",
    "\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional, average, Average, Concatenate\n",
    "from keras.layers import Flatten, BatchNormalization, concatenate, GRU, SpatialDropout1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Embedding, Dropout, Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "from keras.layers import Activation\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "nltk.download('punkt')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "J97kjnISyIhf",
    "outputId": "11fa0fb7-17da-4cd1-b108-685846a9607f"
   },
   "outputs": [],
   "source": [
    "''' Uncomment to use in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-ibFLJujKJ5"
   },
   "outputs": [],
   "source": [
    "root_path='/content/drive/My Drive/Sentimix/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXlXZTxEx4SJ"
   },
   "outputs": [],
   "source": [
    "labels_train_raw=pd.read_csv(root_path+'Train_data/labels_train.csv')\n",
    "labels_dev_raw=pd.read_csv(root_path+'Train_data/labels_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5nkf_mi4DWHB"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(root_path+'/pure_hinglish_with_hindi_cuss_train.csv')\n",
    "dev_data=pd.read_csv(root_path+'pure_hinglish_with_hindi_cuss_dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "RevbZJRWytz4",
    "outputId": "ac7060cd-2d31-46b6-d466-ce6c8a885a7f"
   },
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "le.fit(labels_train_raw)\n",
    "labels_train_le=le.transform(labels_train_raw)\n",
    "labels_dev_le=le.transform(labels_dev_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpBF8hRBGC2g"
   },
   "outputs": [],
   "source": [
    "ohc=OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "JBFt7JAjGLN6",
    "outputId": "cdeb0902-8bb3-4d8f-d409-b3f45041408b"
   },
   "outputs": [],
   "source": [
    "ohc=OneHotEncoder()\n",
    "labels_train=ohc.fit_transform(labels_train_le.reshape(-1,1))\n",
    "labels_dev=ohc.transform(labels_dev_le.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "i208B53c7RB4",
    "outputId": "0dda8c3b-50bd-434e-b73a-0ed71b3c486e"
   },
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern,with_space=False):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    if with_space==False:\n",
    "      for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    else:\n",
    "      for i in r:\n",
    "        input_txt = re.sub(i, ' ', input_txt)\n",
    "    return input_txt \n",
    "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
    "\n",
    "    return input_txt \n",
    "\n",
    "with open(root_path+'helper_data/contractions.pkl','rb')as f:\n",
    "  contractions=pickle.load(f)\n",
    "\n",
    "contractions=Counter(contractions)\n",
    "with open(root_path+'helper_data/acronyms.pkl','rb')as f:\n",
    "  acronyms=pickle.load(f)\n",
    "acronyms=Counter(acronyms)\n",
    "def acronym(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if acronyms[word]!=0:\n",
    "        w_l.append(acronyms[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "with open(root_path+'hinglish_to_english.pickle','rb')as f:\n",
    "  hing_to_eng=pickle.load(f)\n",
    "hing_to_eng=Counter(hing_to_eng)\n",
    "def hindi_se_english(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    w_l=[]\n",
    "    sent=str(df[column][i])\n",
    "    for word in sent.split():\n",
    "      if hing_to_eng[word]!=0:\n",
    "        w_l.append(hing_to_eng[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "with open(root_path+'Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
    "    cuss_dict=pickle.load(handle)\n",
    "cuss_dict=Counter(cuss_dict)\n",
    "cuss_dict['bsdk']='abuse'\n",
    "cuss_dict['bhosadike']='abuse'\n",
    "def replace_cuss(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if cuss_dict[word]!=0:\n",
    "        #w_l.append('abuse')\n",
    "        w_l.append(cuss_dict[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "def remove_contraction(df,column):\n",
    "  s_l=[]\n",
    "  for i in range(df.shape[0]):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if contractions[word]!=0:\n",
    "        w_l.append(contractions[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "def cleaning(data_f,cleaning_col,new_col):\n",
    "  for i in range(data_f.shape[0]):\n",
    "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
    "  data_f[new_col]=replace_cuss(data_f,cleaning_col)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"_\",with_space=True)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
    "  data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@ [\\w]*\",\"<USR>\")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"[0-9]+\",\"<NUM>\")\n",
    "  data_f[new_col]=hindi_se_english(data_f,new_col)\n",
    "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
    "  data_f[new_col]=acronym(data_f,new_col)\n",
    "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
    "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
    "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
    "  data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "  return data_f\n",
    "import numpy as np\n",
    "a=cleaning(train_data,'sent','hindi_clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WnA7MhAFH8f"
   },
   "outputs": [],
   "source": [
    "b=cleaning(dev_data,'sent','hindi_clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iQPNNhjf4Tjo"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUimeWjPzXTB"
   },
   "outputs": [],
   "source": [
    "max_len = 25\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(a['hindi_clean'].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JS5TeLmPzf1G"
   },
   "outputs": [],
   "source": [
    "sequences_train = tok.texts_to_sequences(a['hindi_clean'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ovf7C-iX1T-S"
   },
   "outputs": [],
   "source": [
    "sequences_dev = tok.texts_to_sequences(b['hindi_clean'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "YtrwEi0hGdEv",
    "outputId": "a7377dab-6ee3-49b8-cef5-97946089c23f"
   },
   "outputs": [],
   "source": [
    "def custom_gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Ns59EE9a4cm"
   },
   "source": [
    "## abuse feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEyWVVJua9RG"
   },
   "outputs": [],
   "source": [
    "abuse_f=np.zeros((train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WqTkqE2WbLxo"
   },
   "outputs": [],
   "source": [
    "for i in range(train_data.shape[0]):\n",
    "  sent=str(a['hindi_clean'][i])\n",
    "  for word in sent.split():\n",
    "    if cuss_dict[word]!=0:\n",
    "      abuse_f[i]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elHgHogVbu6g"
   },
   "outputs": [],
   "source": [
    "abuse_fd=np.zeros((dev_data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elHGVB0Tb25H"
   },
   "outputs": [],
   "source": [
    "for i in range(dev_data.shape[0]):\n",
    "  sent=str(b['hindi_clean'][i])\n",
    "  for word in sent.split():\n",
    "    if cuss_dict[word]!=0:\n",
    "      abuse_fd[i]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgIw3Ac7QlIs"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m3NhuXrxQnnd"
   },
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(vocab_size,100,input_length=max_len)(inputs)\n",
    "    x = Conv1D(256, 3, activation='relu')(layer)\n",
    "    x = MaxPooling1D(3)(x)\n",
    "\n",
    "    x = Conv1D(128, 4, activation='relu')(x)\n",
    "    x = LSTM(100,recurrent_dropout=0.2)(x)\n",
    "    layer = Dense(200,name='FC1')(x)\n",
    "    layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(300,name='FC2')(layer)\n",
    "    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(3,name='out_layer')(layer)\n",
    "    layer = BatchNormalization(name = 'BN4')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "XiKUapNZRPJ6",
    "outputId": "6ea5221c-d6cd-4c76-877a-7763ab0cb793"
   },
   "outputs": [],
   "source": [
    "model=cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mO3wV1XmRsXf",
    "outputId": "48d82c70-5e82-4772-f34b-276eea65eab2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "id": "3L3oTMaIRvPt",
    "outputId": "47a6b78e-3fb9-4766-cf54-b28dbf846213"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "colab_type": "code",
    "id": "UmlQVM41Rz9c",
    "outputId": "91144b2f-1eca-4a51-ee90-605d52d82b34"
   },
   "outputs": [],
   "source": [
    "model.fit(sequences_matrix_train,labels_train,validation_data=(sequences_matrix_dev,labels_dev),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19RCFKzmVEjV"
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XUIw8gV1kVf"
   },
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,200,input_length=max_len)(inputs)\n",
    "    layer = LSTM(100,recurrent_dropout=0.2)(layer)\n",
    "    layer = Dense(200,name='FC1')(layer)\n",
    "    layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(300,name='FC2')(layer)\n",
    "    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(3,name='out_layer')(layer)\n",
    "    layer = BatchNormalization(name = 'BN4')(layer)\n",
    "    layer = Activation('softmax')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "RVpUE7WF2DlK",
    "outputId": "54baa8d1-76dd-4514-f7d5-053816d7ff5d"
   },
   "outputs": [],
   "source": [
    "model=RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "3_Iaqkdq2Hp2",
    "outputId": "71b57ca6-9d8e-4dd3-9065-aac531ba544c"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "colab_type": "code",
    "id": "Sv1Bgpe-lgL_",
    "outputId": "0383b424-bbe4-409f-d12d-ea8c1f07f282"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "uFYgtVM82dTM",
    "outputId": "84d6fd2b-57d8-43b9-bdaf-eb2c09b3b3b8"
   },
   "outputs": [],
   "source": [
    "model.fit(sequences_matrix_train,labels_train,validation_data=(sequences_matrix_dev,labels_dev),epochs=5,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vDWgpzeGcYn"
   },
   "source": [
    "## combinin train and dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIfVwivzG2YN"
   },
   "outputs": [],
   "source": [
    "new_mat=np.concatenate((sequences_matrix_train,sequences_matrix_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ft6HkMAgHC8j",
    "outputId": "69dc052c-1334-4f08-dec0-e6108741563e"
   },
   "outputs": [],
   "source": [
    "new_label=np.concatenate((np.array(labels_train),np.array(labels_dev)),axis=0)\n",
    "print(new_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "YPAEtREPIkw8",
    "outputId": "245cf7f0-5fd7-40ba-a8bf-21d2660e2b01"
   },
   "outputs": [],
   "source": [
    "ohc=OneHotEncoder()\n",
    "new_label=ohc.fit_transform(new_label.reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NBZFaFumIr_3",
    "outputId": "2f2bba50-aa7a-4e71-c72d-1259cf3982ae"
   },
   "outputs": [],
   "source": [
    "new_abuse=np.concatenate((abuse_f,abuse_fd))\n",
    "print(new_abuse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HwO3GQbp1V6l"
   },
   "source": [
    "## Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_z6Aora-c6R"
   },
   "outputs": [],
   "source": [
    "root_path = \"/content/drive/My Drive/Sentimix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "J5EV-8QQ8SHP",
    "outputId": "d7dcbda5-96b5-4c53-9b7e-bddad3d61535"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_emb_300 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_300\")\n",
    "#model_emb_200 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_200\")\n",
    "#model_emb_100 = gensim.models.Word2Vec.load(\"/content/drive/My Drive/Sentimix/hinglish_word2vec_embeddings_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tW6uIhyVdgfO"
   },
   "outputs": [],
   "source": [
    "word_index=tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5GP5jtJdflQ"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_1 = np.zeros((len(tok.word_index) + 1, 100))\n",
    "# for word, i in tok.word_index.items():\n",
    "#     if word in model_emb_100.wv.vocab:\n",
    "#       embedding_matrix_1[i] = model_emb_100[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mgn4w8FhEqXw"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix_2 = np.zeros((len(tok.word_index) + 1, 200))\n",
    "# for word, i in tok.word_index.items():\n",
    "#     if word in model_emb_200.wv.vocab:\n",
    "#       embedding_matrix_2[i] = model_emb_200[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Hb0EV4g2EqKw",
    "outputId": "c6eff38b-8dc3-47d5-80b9-ea63bae5b737"
   },
   "outputs": [],
   "source": [
    "embedding_matrix_3 = np.zeros((len(tok.word_index) + 1, 300))\n",
    "for word, i in tok.word_index.items():\n",
    "    if word in model_emb_300.wv.vocab:\n",
    "      embedding_matrix_3[i] = model_emb_300[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6q_LOMuBOKYD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "class Metrics(Callback):\n",
    "  def on_train_begin(self, logs={}):\n",
    "    self.val_f1s = []\n",
    "    self.val_recalls = []\n",
    "    self.val_precisions = []\n",
    "  \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    val_predict = (np.asarray(self.model.predict([self.validation_data[0]])))\n",
    "    val_targ = self.validation_data[1]\n",
    "    val_predict=val_predict.argmax(axis=-1)\n",
    "    \n",
    "    val_targ=val_targ.argmax(axis=-1)\n",
    "    # print(val_predict)\n",
    "    # print(val_targ)\n",
    "    _val_f1 = f1_score(val_targ, val_predict,average='macro')\n",
    "    _val_recall = recall_score(val_targ, val_predict,average='macro')\n",
    "    _val_precision = precision_score(val_targ, val_predict,average='macro')\n",
    "    self.val_f1s.append(_val_f1)\n",
    "    self.val_recalls.append(_val_recall)\n",
    "    self.val_precisions.append(_val_precision)\n",
    "    print(\" — val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "    return\n",
    " \n",
    "f1_metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AJPD4XQEc9-"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRUGtTlKEMta"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPool1D,GlobalAveragePooling1D\n",
    "class Attention(Layer):\n",
    "    def __init__(self,step_dim=20,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input,CuDNNLSTM,CuDNNGRU\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "def BidLstm(maxlen, max_features, embed_size):\n",
    "    inp1 = Input(shape=(maxlen, ))\n",
    "    #inp2=Input(shape=(1,))\n",
    "    x=Embedding(len(tok.word_index)+1,embed_size)(inp1)\n",
    "    #x = Embedding(len(tok.word_index) + 1,embed_size,weights=[embedding_matrix_3],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x1)\n",
    "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x2)\n",
    "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x3)   \n",
    "    #x = Attention(maxlen)(x)\n",
    "    # x2 = Attention(maxlen)(x2)\n",
    "    # x3 = Attention(maxlen)(x3)\n",
    "    # x=  Concatenate()([x1,x2,x3])\n",
    "    x = CuDNNLSTM(200, return_sequences=True)(x)\n",
    "    x = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                       attention_regularizer_weight=1e-4,\n",
    "                       name='Attention')(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    # layer = Dense(600,name='FC1')(x)\n",
    "    # layer = Dense(300,activation='relu')(layer)\n",
    "    # layer = Dense(200,activation='relu')(layer)\n",
    " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    # layer = Activation('relu')(layer)\n",
    "    # layer = Dropout(0.4)(layer)\n",
    "    x2=GlobalMaxPool1D()(x)\n",
    "    x3=GlobalAveragePooling1D()(x)\n",
    "    x=  Concatenate()([x2,x3])\n",
    "    layer = Dense(128,name='FC2')(x)\n",
    "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "   # layer=  Concatenate()([layer,inp2])\n",
    "    # layer=Dense(256,activation='relu')(layer)\n",
    "    # layer=Dense(128,activation='relu')(layer)\n",
    "    layer = Dense(3,name='out_layer',activation='softmax')(layer)\n",
    "\n",
    "    model = Model(inputs=[inp1],outputs=layer)\n",
    "\n",
    "    return model\n",
    "model=BidLstm(max_len,max_features=len(tok.word_index)+1,embed_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uITF0D84Atv"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['acc',km.f1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "d2DGEX_P5qWJ",
    "outputId": "c830cacd-8ce2-4ac5-d4bf-446410daa23e"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bPswR_f7RF-w"
   },
   "outputs": [],
   "source": [
    "cp_filepath=root_path+'/checkpoints/bi_gru_self_attention.h5'\n",
    "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=5,restore_best_weights=True)\n",
    "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "b-aKbqPo4Q2R",
    "outputId": "8926020c-5fbc-49fa-e9d0-b27f146b3ace"
   },
   "outputs": [],
   "source": [
    "model.fit([sequences_matrix_train],labels_train,validation_data=([sequences_matrix_dev],labels_dev),epochs=10,batch_size=32,callbacks=[es,cp_check_point])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "9epzi9CSLzYY",
    "outputId": "de71828d-da89-4874-f7d0-6259bf6e8828"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict([sequences_matrix_dev], batch_size=32, verbose=1)\n",
    "\n",
    "print(classification_report(labels_dev_le, np.argmax(y_pred,axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxihkdmI0_YA"
   },
   "outputs": [],
   "source": [
    "s=b['hindi_clean']\n",
    "sequence_test = tok.texts_to_sequences(s)\n",
    "sequence_test_mat = sequence.pad_sequences(sequence_test,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXjh5V9o45tF"
   },
   "outputs": [],
   "source": [
    "preds_dev=model.predict([sequences_matrix_dev,abuse_fd]).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3BuNKELAlaC"
   },
   "outputs": [],
   "source": [
    "cuss_dict['bsdk']='abuse'\n",
    "cuss_dict['bhosadike']='abuse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BvlKGAc-5Fi6",
    "outputId": "de3e7499-f20f-4332-85c6-87c7e97a0e4b"
   },
   "outputs": [],
   "source": [
    "for i in range(b.shape[0]):\n",
    "  if le.inverse_transform([preds_dev[i]])[0]!=labels_dev_raw['labels'][i]:\n",
    "    print(b['hindi_clean'][i],le.inverse_transform([preds_dev[i]])[0],labels_dev_raw['labels'][i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hgIw3Ac7QlIs",
    "19RCFKzmVEjV",
    "-vDWgpzeGcYn",
    "8XO5_7eRsWCM",
    "Ch2-uxq3JAzT",
    "qYGQHtUiAjg8",
    "1rrPbMNq7qtc",
    "A1yMF1G2gpWu",
    "svRzkV5ytN8F"
   ],
   "name": "train_gru_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
