{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "8urmVSlSm02S",
    "outputId": "1353cbf1-3a60-46d2-a989-675197e5c0b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "'''from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "p9x5GAv2m-n6",
    "outputId": "d9fab16c-3f13-42af-b6ab-5b2d30976994"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import io\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Conv1D, Conv2D\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
    "from keras.layers import average\n",
    "import tensorflow_hub as hub\n",
    "from keras.layers import Average\n",
    "from keras.layers import Concatenate\n",
    "nltk.download('punkt')\n",
    "from numpy import random\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "from keras.layers import SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "#plt.switch_backend('agg')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fNryX5AgoDQc"
   },
   "outputs": [],
   "source": [
    "root_path='/content/drive/My Drive/offenseval/2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UeuSDrPoLHE"
   },
   "outputs": [],
   "source": [
    "file_name = root_path+\"/OffenseEval2020Data/task_c_distant.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAB16jAwogy8"
   },
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('/content/drive/My Drive/offenseval/2020/task_c_distant_ann.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pf3ey4KDqjPa"
   },
   "source": [
    "## Twitter Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "sUZMDz7bqbj6",
    "outputId": "db52c50a-ab44-4fa1-9b44-aad0190060f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Sentimix/word2vec_twitter_tokens.bin', binary=True,unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSzr_yTlquRV"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBtzr1Plqy_i"
   },
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern,with_space=False):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    if with_space==False:\n",
    "      for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    else:\n",
    "      for i in r:\n",
    "        input_txt = re.sub(i, ' ', input_txt)\n",
    "    return input_txt \n",
    "\n",
    "import emoji\n",
    "import pickle\n",
    "import re\n",
    "with open('/content/drive/My Drive/Sentimix/helper_data/contractions.pkl','rb')as f:\n",
    "  contractions=pickle.load(f)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "contractions=Counter(contractions)\n",
    "with open('/content/drive/My Drive/Sentimix/helper_data/acronyms.pkl','rb')as f:\n",
    "  acronyms=pickle.load(f)\n",
    "acronyms=Counter(acronyms)\n",
    "def acronym(df,column):\n",
    "  s_l=[]\n",
    "  for i in tqdm(range(df.shape[0])):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if acronyms[word]!=0:\n",
    "        w_l.append(acronyms[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "with open('/content/drive/My Drive/Sentimix/hinglish_to_english.pickle','rb')as f:\n",
    "  hing_to_eng=pickle.load(f)\n",
    "hing_to_eng=Counter(hing_to_eng)\n",
    "def hindi_se_english(df,column):\n",
    "  s_l=[]\n",
    "  for i in tqdm(range(df.shape[0])):\n",
    "    w_l=[]\n",
    "    sent=str(df[column][i])\n",
    "    for word in sent.split():\n",
    "      if hing_to_eng[word]!=0:\n",
    "        w_l.append(hing_to_eng[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "with open('/content/drive/My Drive/Sentimix/Hinglish_utils/Hinglish_Profanity_dict.pkl', 'rb') as handle:\n",
    "    cuss_dict=pickle.load(handle)\n",
    "cuss_dict=Counter(cuss_dict)\n",
    "def replace_cuss(df,column):\n",
    "  s_l=[]\n",
    "  for i in tqdm(range(df.shape[0])):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if cuss_dict[word]!=0:\n",
    "        w_l.append('abuse')\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "def remove_contraction(df,column):\n",
    "  s_l=[]\n",
    "  for i in tqdm(range(df.shape[0])):\n",
    "    sent=str(df[column][i]).lower()\n",
    "    w_l=[]\n",
    "    for word in sent.split():\n",
    "      if contractions[word]!=0:\n",
    "        w_l.append(contractions[word])\n",
    "      else:\n",
    "        w_l.append(word)\n",
    "    s_l.append(' '.join(w_l))\n",
    "  return s_l\n",
    "def remove_pattern_rep(input_txt, pattern,rep_pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "      input_txt = re.sub(i, rep_pattern, input_txt)\n",
    "\n",
    "    return input_txt \n",
    "def cleaning(data_f,cleaning_col,new_col):\n",
    "  data_f.reset_index(drop=True,inplace=True)\n",
    "  for i in tqdm(range(data_f.shape[0])):\n",
    "    data_f[cleaning_col][i]=emoji.demojize(str(data_f[cleaning_col][i]))\n",
    "  # #data_f[cleaning_col]=replace_cuss(data_f,cleaning_col)\n",
    "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[cleaning_col],\"_\",with_space=True)\n",
    "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\"-\",with_space=True)\n",
    "  # data_f[new_col]=np.vectorize(remove_pattern)(data_f[new_col],\":\",with_space=True)\n",
    "  # data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"@[\\w]*\",\"<USR>\")\n",
    "  # data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[new_col], \"http\\S+\",\"<URL>\")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern_rep)(data_f[cleaning_col], \"[0-9]+\",\"<NUM>\")\n",
    "  #data_f[new_col]=hindi_se_english(data_f,cleaning_col)\n",
    "  data_f[new_col]=remove_contraction(data_f,new_col)\n",
    "  data_f[new_col]=acronym(data_f,new_col)\n",
    "  data_f[new_col]=data_f[new_col].str.replace(\"[^a-zA-Z]<>\", \" \")\n",
    "  data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"~\",with_space=False)\n",
    "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \"!\",with_space=True)\n",
    "  #data_f[new_col] = np.vectorize(remove_pattern)(data_f[new_col], \".\",with_space=True)\n",
    "  #data_f[new_col] = data_f[new_col].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "  #data_f.drop(['text'],inplace=True)\n",
    "  return data_f\n",
    "import numpy as np\n",
    "#a=cleaning(df_chunky[0],'text','clean_col')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "0LGDHrflq20M",
    "outputId": "7e60eb55-5786-4d99-947e-c4893efa1810"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/188973 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 188973/188973 [04:03<00:00, 777.53it/s]\n",
      "100%|██████████| 188973/188973 [00:04<00:00, 43565.45it/s]\n",
      "100%|██████████| 188973/188973 [00:04<00:00, 42749.55it/s]\n"
     ]
    }
   ],
   "source": [
    "_=cleaning(train_data,'text','clean_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "SvmtMOXorBH7",
    "outputId": "c94cad82-fca6-49b3-cbae-bedf58dff83a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'text', 'average_ind', 'average_grp', 'average_oth', 'std_ind',\n",
       "       'std_grp', 'std_oth', 'clean_col'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF6uN5Njsjp8"
   },
   "source": [
    "## Tokenization and Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yc3_yfXCsn4J"
   },
   "outputs": [],
   "source": [
    "df_train,df_test=train_test_split(train_data,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cjvGZ5Gtssk5"
   },
   "outputs": [],
   "source": [
    "max_words =10000\n",
    "max_len = 25\n",
    "tok = Tokenizer(max_words)\n",
    "tok.fit_on_texts(train_data['clean_col'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uiq2b0ZPsz1A"
   },
   "outputs": [],
   "source": [
    "sequences_train = tok.texts_to_sequences(df_train['clean_col'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_train = sequence.pad_sequences(sequences_train,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WUXbJyfs6IH"
   },
   "outputs": [],
   "source": [
    "sequences_dev = tok.texts_to_sequences(df_test['clean_col'].astype(str))\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "sequences_matrix_dev = sequence.pad_sequences(sequences_dev,maxlen=max_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxPk6kDK-4Ft"
   },
   "outputs": [],
   "source": [
    "y_train=df_train[['average_ind','average_grp','average_oth']].to_numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNUat-VkdpQl"
   },
   "outputs": [],
   "source": [
    "y_test=df_test[['average_ind','average_grp','average_oth']].to_numpy().argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eL6UCtGYtHwj"
   },
   "outputs": [],
   "source": [
    "\n",
    "def custom_gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "get_custom_objects().update({'custom_gelu': Activation(custom_gelu)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6UgxNXypd3-u"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ZhGwoIgieYkG",
    "outputId": "47d0ff65-66c1-44b6-bf49-c7bea9ccc45f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151178,)\n",
      "(37795,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-CGTzr4tmeH"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "581UbiUntv1x"
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('/content/drive/My Drive/IR_project/glove.6B', 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUCAVslAtlOC"
   },
   "outputs": [],
   "source": [
    "word_index=tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-brG0wAts8w"
   },
   "outputs": [],
   "source": [
    "embedding_matrix_1 = np.zeros([max_words + 1, 400])\n",
    "for word, i in tok.word_index.items():\n",
    "    if word in model:\n",
    "      if i>max_words:\n",
    "        break\n",
    "      embedding_matrix_1[i] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zNjzoa2t6y4"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from keras.layers import CuDNNGRU,CuDNNLSTM,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
    "from sklearn.utils import class_weight\n",
    "class Attention(Layer):\n",
    "    def __init__(self,step_dim=20,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, Dropout\n",
    "\n",
    "#max_len=\n",
    "\n",
    "def BidLstm(maxlen, max_features, embed_size):\n",
    "    inp1 = Input(shape=(maxlen, ))\n",
    "    #inp2=Input(shape=(1,))\n",
    "    #x=Embedding(len(word_index)+1,embed_size)(inp1)\n",
    "    x1 = Embedding(max_words+1,embed_size,weights=[embedding_matrix_1],\n",
    "                  trainable=True)(inp1)\n",
    "    # x2 = Embedding(len(tok.word_index) + 1,embed_size_2,weights=[embedding_matrix_2],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x3 = Embedding(len(tok.word_index) + 1,embed_size_3,weights=[embedding_matrix_3],\n",
    "    #                trainable=True)(inp1)\n",
    "    # x1 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x1)\n",
    "    # x2 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x2)\n",
    "    # x3 = Bidirectional(LSTM(200, return_sequences=True, dropout=0.4,\n",
    "    #                        recurrent_dropout=0.4))(x3)   \n",
    "    # x1 = Attention(maxlen)(x1)\n",
    "    # x2 = Attention(maxlen)(x2)\n",
    "    # x3 = Attention(maxlen)(x3)\n",
    "    # x=  Concatenate()([x1,x2,x3])\n",
    "    x1 = Bidirectional(CuDNNLSTM(200, return_sequences=True))(x1)   \n",
    "    x1 = SeqSelfAttention(kernel_regularizer=keras.regularizers.l2(1e-4),\n",
    "                    bias_regularizer=keras.regularizers.l1(1e-4),\n",
    "                    attention_regularizer_weight=1e-4,\n",
    "                    name='Attention')(x1) \n",
    "    x2=  GlobalMaxPooling1D()(x1)\n",
    "    x3= GlobalAveragePooling1D()(x1)\n",
    "    x=  Concatenate()([x2,x3])\n",
    "    x = Dropout(0.1)(x)\n",
    "    #x = Attention(maxlen)(x)\n",
    "    # layer = Dense(600,name='FC1')(x)\n",
    "    # layer = Dense(300,activation='relu')(layer)\n",
    "    layer = Dense(128,activation='relu')(x)\n",
    " #   layer = BatchNormalization(name = 'BN1')(layer)\n",
    "    #layer = Activation('relu')(layer)\n",
    "    #layer = Dropout(0.4)(layer)\n",
    "    layer = Dense(64,name='FC2')(layer)\n",
    "#    layer = BatchNormalization(name = 'BN2')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.4)(layer)\n",
    "   # layer=  Concatenate()([layer,inp2])\n",
    "    # layer=Dense(256,activation='relu')(layer)\n",
    "    # layer=Dense(128,activation='relu')(layer)\n",
    "    layer = Dense(3,name='out_layer',activation='softmax')(layer)\n",
    "\n",
    "    model = Model(inputs=[inp1],outputs=layer)\n",
    "\n",
    "    return model\n",
    "model_bi=BidLstm(max_len,max_features=max_words,embed_size=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NViEAJS9t9lX"
   },
   "outputs": [],
   "source": [
    "model_bi.compile(optimizer='Adam',loss='sparse_categorical_crossentropy',metrics=['acc',km.f1_score()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 654
    },
    "colab_type": "code",
    "id": "tT_1ZJX7uAH9",
    "outputId": "1b02018c-0422-41e9-d734-bdf943268b76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 25)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 25, 400)      4000400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 25, 400)      963200      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Attention (SeqSelfAttention)    (None, 25, 400)      25665       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 400)          0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 400)          0           Attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 800)          0           global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 800)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          102528      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "FC2 (Dense)                     (None, 64)           8256        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64)           0           FC2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 64)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "out_layer (Dense)               (None, 3)            195         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,100,244\n",
      "Trainable params: 5,100,244\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_bi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iv3iu0fjuCZC"
   },
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)\n",
    "class_weights=dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmuQzEZUuEsQ"
   },
   "outputs": [],
   "source": [
    "cp_filepath='/content/drive/My Drive/offenseval/'+'checkpoints/lstm_model_2020b_big_attention.h5'\n",
    "cp_check_point=keras.callbacks.ModelCheckpoint(cp_filepath, monitor='val_f1_score', verbose=0, save_best_only=True, save_weights_only=False, mode='max', period=1)\n",
    "es = EarlyStopping(monitor='val_f1_score', mode='max', min_delta=0,patience=2,restore_best_weights=True)\n",
    "reduce_lr=keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "id": "_bsiPzmFuKtE",
    "outputId": "ac13162a-bb72-472b-9a30-4ff090cdbfa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151178 samples, validate on 37795 samples\n",
      "Epoch 1/10\n",
      "151178/151178 [==============================] - 9s 61us/step - loss: 0.5544 - acc: 0.7743 - f1_score: 0.0350 - val_loss: 0.4707 - val_acc: 0.8030 - val_f1_score: 0.0127\n",
      "Epoch 2/10\n",
      "151178/151178 [==============================] - 8s 55us/step - loss: 0.3631 - acc: 0.8554 - f1_score: 0.0204 - val_loss: 0.3186 - val_acc: 0.8695 - val_f1_score: 0.0316\n",
      "Epoch 3/10\n",
      "151178/151178 [==============================] - 8s 55us/step - loss: 0.3014 - acc: 0.8761 - f1_score: 0.0161 - val_loss: 0.3664 - val_acc: 0.8515 - val_f1_score: 0.0283\n",
      "Epoch 4/10\n",
      "151178/151178 [==============================] - 8s 55us/step - loss: 0.2659 - acc: 0.8864 - f1_score: 0.0132 - val_loss: 0.3229 - val_acc: 0.8696 - val_f1_score: 0.0359\n",
      "Epoch 5/10\n",
      "151178/151178 [==============================] - 8s 55us/step - loss: 0.2359 - acc: 0.8960 - f1_score: 0.0114 - val_loss: 0.3023 - val_acc: 0.8789 - val_f1_score: 0.0430\n",
      "Epoch 6/10\n",
      "151178/151178 [==============================] - 8s 55us/step - loss: 0.2105 - acc: 0.9040 - f1_score: 0.0100 - val_loss: 0.3323 - val_acc: 0.8675 - val_f1_score: 0.0388\n",
      "Epoch 7/10\n",
      "151178/151178 [==============================] - 8s 55us/step - loss: 0.1869 - acc: 0.9135 - f1_score: 0.0086 - val_loss: 0.3922 - val_acc: 0.8695 - val_f1_score: 0.0426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f5016d1d0>"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bi.fit([sequences_matrix_train],y_train,validation_data=([sequences_matrix_dev],y_test),epochs=10,batch_size=1024,class_weight=class_weights,callbacks=[es,cp_check_point])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sn1nq_mLuSg-"
   },
   "outputs": [],
   "source": [
    "y_preds_dl=model_bi.predict(sequences_matrix_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "UMPo73aNhpAd",
    "outputId": "798ef234-fc46-4f4a-f644-83294b09a614"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.82520401e-01, 1.09897470e-02, 6.48984080e-03],\n",
       "       [9.99997854e-01, 2.12838199e-06, 1.73419661e-08],\n",
       "       [9.99440491e-01, 4.25356207e-04, 1.34196773e-04],\n",
       "       ...,\n",
       "       [9.99999404e-01, 5.71469855e-07, 1.05636069e-08],\n",
       "       [9.96202528e-01, 3.68329603e-03, 1.14121096e-04],\n",
       "       [9.99391675e-01, 4.09951987e-04, 1.98411479e-04]], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "NNLgNPTRyA3R",
    "outputId": "78d0427e-2a27-416b-f208-a0db1d1cd2e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.89      0.93     30598\n",
      "           1       0.71      0.86      0.78      4867\n",
      "           2       0.44      0.78      0.56      2330\n",
      "\n",
      "    accuracy                           0.88     37795\n",
      "   macro avg       0.71      0.84      0.76     37795\n",
      "weighted avg       0.91      0.88      0.89     37795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#y_pred = model_bi.predict(X_dev, batch_size=30, verbose=1)\n",
    "\n",
    "print(classification_report(y_test, y_preds_dl.argmax(axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uldsIGhJyBVQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "offens_task3_bilstm_attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
